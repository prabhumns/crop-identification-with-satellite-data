{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import svm\n",
    "from statistics import mean\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as shf\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('E:/Time Series/pra.pkl', 'rb')\n",
    "data2 = pickle.load(file_object)\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOYABEENS 204639\n",
      "CORN 151407\n",
      "DEVELOPED 12214\n",
      "FOREST 14646\n",
      "WATER BODY 6501\n",
      "ALFALFA 781\n",
      "OATS 25\n"
     ]
    }
   ],
   "source": [
    "for key, value in data2.items():\n",
    "    print (key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {key:value[:6000] for key, value in data.items() if len(value) >= 6000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-28373172d708>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    for n in range(len(data[key]):\n",
    "        example = data[key][n]\n",
    "        for i in range(44):\n",
    "            for t in range(i+1, 45):\n",
    "                example.append(example[i]*example[t])\n",
    "        data[key][n] = np.array([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(45*23):\n",
    "    lin = [l[i] for value in data.values() for l in value]\n",
    "    mins = min(lin)\n",
    "    maxs = max(lin)\n",
    "    means = mean(lin)\n",
    "    for key, value in data.items():\n",
    "        for example in value:\n",
    "            example[i] = np.float64((example[i] - means)/(maxs-mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = [l for value in data.values() for l in value]\n",
    "mins = min(lin)\n",
    "maxs = max(lin)\n",
    "means = mean(lin)\n",
    "data = {key:[np.float64((example - means)*((maxs-mins)**-1)  for example in value] for key, value in data.items()}\n",
    "for key in data.keys():\n",
    "    for i in range()\n",
    "    example = np.float64((example - means)*((maxs-mins)**-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : SOYABEENS\n",
      "1 : CORN\n",
      "2 : DEVELOPED\n",
      "3 : FOREST\n",
      "4 : WATER BODY\n"
     ]
    }
   ],
   "source": [
    "training_examples = []\n",
    "training_labels = []\n",
    "test_examples = []\n",
    "test_labels = []\n",
    "cv_examples = []\n",
    "cv_labels = []\n",
    "i = 0;\n",
    "for key, value in data.items():\n",
    "    training_examples = training_examples + value[:5000]\n",
    "    cv_examples = cv_examples + value[5000:5500]\n",
    "    test_examples = test_examples + value[5500:]\n",
    "    training_labels = training_labels + [i for v in range(5000)]\n",
    "    cv_labels = cv_labels + [i for v in range(500)]\n",
    "    test_labels = test_labels + [i for v in range(500)]\n",
    "    print(i,':', key)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples, training_labels  = shf(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=3, \n",
    "              kernel='rbf', \n",
    "              degree= 3,\n",
    "              gamma='auto',\n",
    "              coef0=0.0, \n",
    "              shrinking=True, \n",
    "              probability=True, \n",
    "              tol=0.0000001, \n",
    "              cache_size=200, \n",
    "              class_weight=None, \n",
    "              verbose=True, \n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d15d99840fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77356"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = MLPClassifier(hidden_layer_sizes=(30,10,10),\n",
    "                    activation='tanh', \n",
    "                    solver='sgd',\n",
    "                    alpha=0.00001, \n",
    "                    batch_size= 1000, \n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=10000, \n",
    "                    shuffle=True, \n",
    "                    random_state=None, \n",
    "                    tol=0.000001, \n",
    "                    verbose=True, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False,\n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82956934\n",
      "Iteration 2, loss = 1.80699371\n",
      "Iteration 3, loss = 1.79034833\n",
      "Iteration 4, loss = 1.77679501\n",
      "Iteration 5, loss = 1.76480556\n",
      "Iteration 6, loss = 1.75346249\n",
      "Iteration 7, loss = 1.74220737\n",
      "Iteration 8, loss = 1.73052129\n",
      "Iteration 9, loss = 1.71862417\n",
      "Iteration 10, loss = 1.70636640\n",
      "Iteration 11, loss = 1.69402438\n",
      "Iteration 12, loss = 1.68181121\n",
      "Iteration 13, loss = 1.66977698\n",
      "Iteration 14, loss = 1.65834948\n",
      "Iteration 15, loss = 1.64742818\n",
      "Iteration 16, loss = 1.63712959\n",
      "Iteration 17, loss = 1.62750407\n",
      "Iteration 18, loss = 1.61832214\n",
      "Iteration 19, loss = 1.60965302\n",
      "Iteration 20, loss = 1.60142727\n",
      "Iteration 21, loss = 1.59367774\n",
      "Iteration 22, loss = 1.58630443\n",
      "Iteration 23, loss = 1.57929360\n",
      "Iteration 24, loss = 1.57263325\n",
      "Iteration 25, loss = 1.56621942\n",
      "Iteration 26, loss = 1.56010353\n",
      "Iteration 27, loss = 1.55422196\n",
      "Iteration 28, loss = 1.54857000\n",
      "Iteration 29, loss = 1.54311798\n",
      "Iteration 30, loss = 1.53788285\n",
      "Iteration 31, loss = 1.53283039\n",
      "Iteration 32, loss = 1.52791842\n",
      "Iteration 33, loss = 1.52321004\n",
      "Iteration 34, loss = 1.51866423\n",
      "Iteration 35, loss = 1.51430110\n",
      "Iteration 36, loss = 1.51007644\n",
      "Iteration 37, loss = 1.50600642\n",
      "Iteration 38, loss = 1.50207201\n",
      "Iteration 39, loss = 1.49830224\n",
      "Iteration 40, loss = 1.49464715\n",
      "Iteration 41, loss = 1.49111230\n",
      "Iteration 42, loss = 1.48768673\n",
      "Iteration 43, loss = 1.48440847\n",
      "Iteration 44, loss = 1.48121723\n",
      "Iteration 45, loss = 1.47812825\n",
      "Iteration 46, loss = 1.47511653\n",
      "Iteration 47, loss = 1.47222258\n",
      "Iteration 48, loss = 1.46942272\n",
      "Iteration 49, loss = 1.46669346\n",
      "Iteration 50, loss = 1.46404285\n",
      "Iteration 51, loss = 1.46147564\n",
      "Iteration 52, loss = 1.45899567\n",
      "Iteration 53, loss = 1.45658237\n",
      "Iteration 54, loss = 1.45421492\n",
      "Iteration 55, loss = 1.45194406\n",
      "Iteration 56, loss = 1.44971849\n",
      "Iteration 57, loss = 1.44757746\n",
      "Iteration 58, loss = 1.44547935\n",
      "Iteration 59, loss = 1.44343578\n",
      "Iteration 60, loss = 1.44146324\n",
      "Iteration 61, loss = 1.43952817\n",
      "Iteration 62, loss = 1.43765864\n",
      "Iteration 63, loss = 1.43582468\n",
      "Iteration 64, loss = 1.43404464\n",
      "Iteration 65, loss = 1.43231811\n",
      "Iteration 66, loss = 1.43063262\n",
      "Iteration 67, loss = 1.42898104\n",
      "Iteration 68, loss = 1.42738327\n",
      "Iteration 69, loss = 1.42581667\n",
      "Iteration 70, loss = 1.42429611\n",
      "Iteration 71, loss = 1.42280172\n",
      "Iteration 72, loss = 1.42135585\n",
      "Iteration 73, loss = 1.41994210\n",
      "Iteration 74, loss = 1.41855521\n",
      "Iteration 75, loss = 1.41720285\n",
      "Iteration 76, loss = 1.41588319\n",
      "Iteration 77, loss = 1.41459835\n",
      "Iteration 78, loss = 1.41333109\n",
      "Iteration 79, loss = 1.41210392\n",
      "Iteration 80, loss = 1.41089614\n",
      "Iteration 81, loss = 1.40972347\n",
      "Iteration 82, loss = 1.40856416\n",
      "Iteration 83, loss = 1.40743474\n",
      "Iteration 84, loss = 1.40633245\n",
      "Iteration 85, loss = 1.40524838\n",
      "Iteration 86, loss = 1.40419369\n",
      "Iteration 87, loss = 1.40315987\n",
      "Iteration 88, loss = 1.40214334\n",
      "Iteration 89, loss = 1.40114600\n",
      "Iteration 90, loss = 1.40017554\n",
      "Iteration 91, loss = 1.39921582\n",
      "Iteration 92, loss = 1.39828451\n",
      "Iteration 93, loss = 1.39737111\n",
      "Iteration 94, loss = 1.39646613\n",
      "Iteration 95, loss = 1.39558878\n",
      "Iteration 96, loss = 1.39472460\n",
      "Iteration 97, loss = 1.39387773\n",
      "Iteration 98, loss = 1.39304685\n",
      "Iteration 99, loss = 1.39223340\n",
      "Iteration 100, loss = 1.39143411\n",
      "Iteration 101, loss = 1.39064789\n",
      "Iteration 102, loss = 1.38988146\n",
      "Iteration 103, loss = 1.38912416\n",
      "Iteration 104, loss = 1.38838789\n",
      "Iteration 105, loss = 1.38765623\n",
      "Iteration 106, loss = 1.38694573\n",
      "Iteration 107, loss = 1.38624487\n",
      "Iteration 108, loss = 1.38555375\n",
      "Iteration 109, loss = 1.38487408\n",
      "Iteration 110, loss = 1.38421474\n",
      "Iteration 111, loss = 1.38356425\n",
      "Iteration 112, loss = 1.38291607\n",
      "Iteration 113, loss = 1.38228089\n",
      "Iteration 114, loss = 1.38166673\n",
      "Iteration 115, loss = 1.38105405\n",
      "Iteration 116, loss = 1.38045646\n",
      "Iteration 117, loss = 1.37986531\n",
      "Iteration 118, loss = 1.37928104\n",
      "Iteration 119, loss = 1.37871099\n",
      "Iteration 120, loss = 1.37814412\n",
      "Iteration 121, loss = 1.37758964\n",
      "Iteration 122, loss = 1.37704651\n",
      "Iteration 123, loss = 1.37651022\n",
      "Iteration 124, loss = 1.37597851\n",
      "Iteration 125, loss = 1.37545580\n",
      "Iteration 126, loss = 1.37494389\n",
      "Iteration 127, loss = 1.37443646\n",
      "Iteration 128, loss = 1.37393254\n",
      "Iteration 129, loss = 1.37344349\n",
      "Iteration 130, loss = 1.37295786\n",
      "Iteration 131, loss = 1.37247976\n",
      "Iteration 132, loss = 1.37200522\n",
      "Iteration 133, loss = 1.37154044\n",
      "Iteration 134, loss = 1.37108079\n",
      "Iteration 135, loss = 1.37062241\n",
      "Iteration 136, loss = 1.37017538\n",
      "Iteration 137, loss = 1.36973503\n",
      "Iteration 138, loss = 1.36929570\n",
      "Iteration 139, loss = 1.36886713\n",
      "Iteration 140, loss = 1.36844149\n",
      "Iteration 141, loss = 1.36801803\n",
      "Iteration 142, loss = 1.36760590\n",
      "Iteration 143, loss = 1.36719291\n",
      "Iteration 144, loss = 1.36678971\n",
      "Iteration 145, loss = 1.36638961\n",
      "Iteration 146, loss = 1.36599183\n",
      "Iteration 147, loss = 1.36560227\n",
      "Iteration 148, loss = 1.36521871\n",
      "Iteration 149, loss = 1.36483393\n",
      "Iteration 150, loss = 1.36445637\n",
      "Iteration 151, loss = 1.36408306\n",
      "Iteration 152, loss = 1.36371572\n",
      "Iteration 153, loss = 1.36335028\n",
      "Iteration 154, loss = 1.36298881\n",
      "Iteration 155, loss = 1.36262947\n",
      "Iteration 156, loss = 1.36227564\n",
      "Iteration 157, loss = 1.36192640\n",
      "Iteration 158, loss = 1.36158313\n",
      "Iteration 159, loss = 1.36123620\n",
      "Iteration 160, loss = 1.36089825\n",
      "Iteration 161, loss = 1.36056232\n",
      "Iteration 162, loss = 1.36022966\n",
      "Iteration 163, loss = 1.35989889\n",
      "Iteration 164, loss = 1.35957332\n",
      "Iteration 165, loss = 1.35925043\n",
      "Iteration 166, loss = 1.35892963\n",
      "Iteration 167, loss = 1.35861309\n",
      "Iteration 168, loss = 1.35829980\n",
      "Iteration 169, loss = 1.35798769\n",
      "Iteration 170, loss = 1.35768112\n",
      "Iteration 171, loss = 1.35737293\n",
      "Iteration 172, loss = 1.35707284\n",
      "Iteration 173, loss = 1.35676883\n",
      "Iteration 174, loss = 1.35647288\n",
      "Iteration 175, loss = 1.35617822\n",
      "Iteration 176, loss = 1.35588768\n",
      "Iteration 177, loss = 1.35559487\n",
      "Iteration 178, loss = 1.35530744\n",
      "Iteration 179, loss = 1.35502176\n",
      "Iteration 180, loss = 1.35473735\n",
      "Iteration 181, loss = 1.35446064\n",
      "Iteration 182, loss = 1.35417883\n",
      "Iteration 183, loss = 1.35390179\n",
      "Iteration 184, loss = 1.35363165\n",
      "Iteration 185, loss = 1.35335831\n",
      "Iteration 186, loss = 1.35308778\n",
      "Iteration 187, loss = 1.35282065\n",
      "Iteration 188, loss = 1.35255533\n",
      "Iteration 189, loss = 1.35229145\n",
      "Iteration 190, loss = 1.35203183\n",
      "Iteration 191, loss = 1.35177033\n",
      "Iteration 192, loss = 1.35151250\n",
      "Iteration 193, loss = 1.35125848\n",
      "Iteration 194, loss = 1.35100473\n",
      "Iteration 195, loss = 1.35075275\n",
      "Iteration 196, loss = 1.35050351\n",
      "Iteration 197, loss = 1.35025266\n",
      "Iteration 198, loss = 1.35000917\n",
      "Iteration 199, loss = 1.34976356\n",
      "Iteration 200, loss = 1.34952047\n",
      "Iteration 201, loss = 1.34927890\n",
      "Iteration 202, loss = 1.34903874\n",
      "Iteration 203, loss = 1.34880054\n",
      "Iteration 204, loss = 1.34856467\n",
      "Iteration 205, loss = 1.34832816\n",
      "Iteration 206, loss = 1.34809493\n",
      "Iteration 207, loss = 1.34786175\n",
      "Iteration 208, loss = 1.34763158\n",
      "Iteration 209, loss = 1.34740223\n",
      "Iteration 210, loss = 1.34717603\n",
      "Iteration 211, loss = 1.34694785\n",
      "Iteration 212, loss = 1.34672142\n",
      "Iteration 213, loss = 1.34649953\n",
      "Iteration 214, loss = 1.34627581\n",
      "Iteration 215, loss = 1.34605407\n",
      "Iteration 216, loss = 1.34583537\n",
      "Iteration 217, loss = 1.34561547\n",
      "Iteration 218, loss = 1.34539727\n",
      "Iteration 219, loss = 1.34518125\n",
      "Iteration 220, loss = 1.34496518\n",
      "Iteration 221, loss = 1.34475163\n",
      "Iteration 222, loss = 1.34453881\n",
      "Iteration 223, loss = 1.34432614\n",
      "Iteration 224, loss = 1.34411467\n",
      "Iteration 225, loss = 1.34390513\n",
      "Iteration 226, loss = 1.34369575\n",
      "Iteration 227, loss = 1.34348913\n",
      "Iteration 228, loss = 1.34328330\n",
      "Iteration 229, loss = 1.34307717\n",
      "Iteration 230, loss = 1.34287377\n",
      "Iteration 231, loss = 1.34267045\n",
      "Iteration 232, loss = 1.34246584\n",
      "Iteration 233, loss = 1.34226686\n",
      "Iteration 234, loss = 1.34206740\n",
      "Iteration 235, loss = 1.34186738\n",
      "Iteration 236, loss = 1.34166978\n",
      "Iteration 237, loss = 1.34147152\n",
      "Iteration 238, loss = 1.34127631\n",
      "Iteration 239, loss = 1.34108086\n",
      "Iteration 240, loss = 1.34088615\n",
      "Iteration 241, loss = 1.34069236\n",
      "Iteration 242, loss = 1.34050061\n",
      "Iteration 243, loss = 1.34030821\n",
      "Iteration 244, loss = 1.34011565\n",
      "Iteration 245, loss = 1.33992818\n",
      "Iteration 246, loss = 1.33973683\n",
      "Iteration 247, loss = 1.33954902\n",
      "Iteration 248, loss = 1.33936116\n",
      "Iteration 249, loss = 1.33917346\n",
      "Iteration 250, loss = 1.33898648\n",
      "Iteration 251, loss = 1.33880203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 1.33861725\n",
      "Iteration 253, loss = 1.33843432\n",
      "Iteration 254, loss = 1.33825003\n",
      "Iteration 255, loss = 1.33806621\n",
      "Iteration 256, loss = 1.33788385\n",
      "Iteration 257, loss = 1.33770455\n",
      "Iteration 258, loss = 1.33752237\n",
      "Iteration 259, loss = 1.33734265\n",
      "Iteration 260, loss = 1.33716405\n",
      "Iteration 261, loss = 1.33698425\n",
      "Iteration 262, loss = 1.33680517\n",
      "Iteration 263, loss = 1.33663009\n",
      "Iteration 264, loss = 1.33645253\n",
      "Iteration 265, loss = 1.33627357\n",
      "Iteration 266, loss = 1.33609952\n",
      "Iteration 267, loss = 1.33592548\n",
      "Iteration 268, loss = 1.33574797\n",
      "Iteration 269, loss = 1.33557502\n",
      "Iteration 270, loss = 1.33539980\n",
      "Iteration 271, loss = 1.33522910\n",
      "Iteration 272, loss = 1.33505350\n",
      "Iteration 273, loss = 1.33488248\n",
      "Iteration 274, loss = 1.33471143\n",
      "Iteration 275, loss = 1.33454113\n",
      "Iteration 276, loss = 1.33436855\n",
      "Iteration 277, loss = 1.33419808\n",
      "Iteration 278, loss = 1.33402918\n",
      "Iteration 279, loss = 1.33385831\n",
      "Iteration 280, loss = 1.33369013\n",
      "Iteration 281, loss = 1.33352000\n",
      "Iteration 282, loss = 1.33335241\n",
      "Iteration 283, loss = 1.33318527\n",
      "Iteration 284, loss = 1.33301732\n",
      "Iteration 285, loss = 1.33285031\n",
      "Iteration 286, loss = 1.33268361\n",
      "Iteration 287, loss = 1.33251747\n",
      "Iteration 288, loss = 1.33235311\n",
      "Iteration 289, loss = 1.33218565\n",
      "Iteration 290, loss = 1.33202418\n",
      "Iteration 291, loss = 1.33185950\n",
      "Iteration 292, loss = 1.33169354\n",
      "Iteration 293, loss = 1.33153100\n",
      "Iteration 294, loss = 1.33137036\n",
      "Iteration 295, loss = 1.33120774\n",
      "Iteration 296, loss = 1.33104464\n",
      "Iteration 297, loss = 1.33088497\n",
      "Iteration 298, loss = 1.33072409\n",
      "Iteration 299, loss = 1.33056351\n",
      "Iteration 300, loss = 1.33040593\n",
      "Iteration 301, loss = 1.33024743\n",
      "Iteration 302, loss = 1.33008970\n",
      "Iteration 303, loss = 1.32993202\n",
      "Iteration 304, loss = 1.32977564\n",
      "Iteration 305, loss = 1.32962029\n",
      "Iteration 306, loss = 1.32946542\n",
      "Iteration 307, loss = 1.32931193\n",
      "Iteration 308, loss = 1.32915825\n",
      "Iteration 309, loss = 1.32900480\n",
      "Iteration 310, loss = 1.32885223\n",
      "Iteration 311, loss = 1.32869951\n",
      "Iteration 312, loss = 1.32854905\n",
      "Iteration 313, loss = 1.32839879\n",
      "Iteration 314, loss = 1.32824731\n",
      "Iteration 315, loss = 1.32809826\n",
      "Iteration 316, loss = 1.32795069\n",
      "Iteration 317, loss = 1.32780117\n",
      "Iteration 318, loss = 1.32765455\n",
      "Iteration 319, loss = 1.32750839\n",
      "Iteration 320, loss = 1.32736035\n",
      "Iteration 321, loss = 1.32721645\n",
      "Iteration 322, loss = 1.32706983\n",
      "Iteration 323, loss = 1.32692517\n",
      "Iteration 324, loss = 1.32678118\n",
      "Iteration 325, loss = 1.32663695\n",
      "Iteration 326, loss = 1.32649212\n",
      "Iteration 327, loss = 1.32634997\n",
      "Iteration 328, loss = 1.32620862\n",
      "Iteration 329, loss = 1.32606426\n",
      "Iteration 330, loss = 1.32592502\n",
      "Iteration 331, loss = 1.32578353\n",
      "Iteration 332, loss = 1.32564345\n",
      "Iteration 333, loss = 1.32550173\n",
      "Iteration 334, loss = 1.32536344\n",
      "Iteration 335, loss = 1.32522478\n",
      "Iteration 336, loss = 1.32508377\n",
      "Iteration 337, loss = 1.32494711\n",
      "Iteration 338, loss = 1.32481045\n",
      "Iteration 339, loss = 1.32467275\n",
      "Iteration 340, loss = 1.32453357\n",
      "Iteration 341, loss = 1.32439718\n",
      "Iteration 342, loss = 1.32426165\n",
      "Iteration 343, loss = 1.32412612\n",
      "Iteration 344, loss = 1.32399099\n",
      "Iteration 345, loss = 1.32385439\n",
      "Iteration 346, loss = 1.32372071\n",
      "Iteration 347, loss = 1.32358556\n",
      "Iteration 348, loss = 1.32345030\n",
      "Iteration 349, loss = 1.32331719\n",
      "Iteration 350, loss = 1.32318287\n",
      "Iteration 351, loss = 1.32305019\n",
      "Iteration 352, loss = 1.32291806\n",
      "Iteration 353, loss = 1.32278541\n",
      "Iteration 354, loss = 1.32265215\n",
      "Iteration 355, loss = 1.32252084\n",
      "Iteration 356, loss = 1.32239146\n",
      "Iteration 357, loss = 1.32226029\n",
      "Iteration 358, loss = 1.32212705\n",
      "Iteration 359, loss = 1.32199745\n",
      "Iteration 360, loss = 1.32186727\n",
      "Iteration 361, loss = 1.32173839\n",
      "Iteration 362, loss = 1.32160952\n",
      "Iteration 363, loss = 1.32147893\n",
      "Iteration 364, loss = 1.32135085\n",
      "Iteration 365, loss = 1.32122353\n",
      "Iteration 366, loss = 1.32109275\n",
      "Iteration 367, loss = 1.32096700\n",
      "Iteration 368, loss = 1.32083847\n",
      "Iteration 369, loss = 1.32071127\n",
      "Iteration 370, loss = 1.32058358\n",
      "Iteration 371, loss = 1.32045907\n",
      "Iteration 372, loss = 1.32033190\n",
      "Iteration 373, loss = 1.32020634\n",
      "Iteration 374, loss = 1.32008169\n",
      "Iteration 375, loss = 1.31995510\n",
      "Iteration 376, loss = 1.31983081\n",
      "Iteration 377, loss = 1.31970616\n",
      "Iteration 378, loss = 1.31958126\n",
      "Iteration 379, loss = 1.31945889\n",
      "Iteration 380, loss = 1.31933272\n",
      "Iteration 381, loss = 1.31920946\n",
      "Iteration 382, loss = 1.31908687\n",
      "Iteration 383, loss = 1.31896408\n",
      "Iteration 384, loss = 1.31884168\n",
      "Iteration 385, loss = 1.31871858\n",
      "Iteration 386, loss = 1.31859710\n",
      "Iteration 387, loss = 1.31847406\n",
      "Iteration 388, loss = 1.31835356\n",
      "Iteration 389, loss = 1.31823162\n",
      "Iteration 390, loss = 1.31811061\n",
      "Iteration 391, loss = 1.31799008\n",
      "Iteration 392, loss = 1.31786813\n",
      "Iteration 393, loss = 1.31774868\n",
      "Iteration 394, loss = 1.31762763\n",
      "Iteration 395, loss = 1.31750696\n",
      "Iteration 396, loss = 1.31738859\n",
      "Iteration 397, loss = 1.31726814\n",
      "Iteration 398, loss = 1.31714849\n",
      "Iteration 399, loss = 1.31703080\n",
      "Iteration 400, loss = 1.31691229\n",
      "Iteration 401, loss = 1.31679501\n",
      "Iteration 402, loss = 1.31667533\n",
      "Iteration 403, loss = 1.31655611\n",
      "Iteration 404, loss = 1.31643824\n",
      "Iteration 405, loss = 1.31632113\n",
      "Iteration 406, loss = 1.31620341\n",
      "Iteration 407, loss = 1.31608717\n",
      "Iteration 408, loss = 1.31596887\n",
      "Iteration 409, loss = 1.31585212\n",
      "Iteration 410, loss = 1.31573973\n",
      "Iteration 411, loss = 1.31562105\n",
      "Iteration 412, loss = 1.31550409\n",
      "Iteration 413, loss = 1.31539103\n",
      "Iteration 414, loss = 1.31527253\n",
      "Iteration 415, loss = 1.31515874\n",
      "Iteration 416, loss = 1.31504214\n",
      "Iteration 417, loss = 1.31492914\n",
      "Iteration 418, loss = 1.31481279\n",
      "Iteration 419, loss = 1.31469864\n",
      "Iteration 420, loss = 1.31458469\n",
      "Iteration 421, loss = 1.31447030\n",
      "Iteration 422, loss = 1.31435662\n",
      "Iteration 423, loss = 1.31424253\n",
      "Iteration 424, loss = 1.31412998\n",
      "Iteration 425, loss = 1.31401805\n",
      "Iteration 426, loss = 1.31390371\n",
      "Iteration 427, loss = 1.31379032\n",
      "Iteration 428, loss = 1.31367872\n",
      "Iteration 429, loss = 1.31356615\n",
      "Iteration 430, loss = 1.31345406\n",
      "Iteration 431, loss = 1.31334153\n",
      "Iteration 432, loss = 1.31322870\n",
      "Iteration 433, loss = 1.31311848\n",
      "Iteration 434, loss = 1.31300583\n",
      "Iteration 435, loss = 1.31289405\n",
      "Iteration 436, loss = 1.31278324\n",
      "Iteration 437, loss = 1.31267526\n",
      "Iteration 438, loss = 1.31256392\n",
      "Iteration 439, loss = 1.31245074\n",
      "Iteration 440, loss = 1.31234088\n",
      "Iteration 441, loss = 1.31222859\n",
      "Iteration 442, loss = 1.31211911\n",
      "Iteration 443, loss = 1.31200816\n",
      "Iteration 444, loss = 1.31190043\n",
      "Iteration 445, loss = 1.31178878\n",
      "Iteration 446, loss = 1.31167898\n",
      "Iteration 447, loss = 1.31156936\n",
      "Iteration 448, loss = 1.31146032\n",
      "Iteration 449, loss = 1.31135086\n",
      "Iteration 450, loss = 1.31124132\n",
      "Iteration 451, loss = 1.31113245\n",
      "Iteration 452, loss = 1.31102278\n",
      "Iteration 453, loss = 1.31091522\n",
      "Iteration 454, loss = 1.31080579\n",
      "Iteration 455, loss = 1.31069822\n",
      "Iteration 456, loss = 1.31059030\n",
      "Iteration 457, loss = 1.31048110\n",
      "Iteration 458, loss = 1.31037234\n",
      "Iteration 459, loss = 1.31026545\n",
      "Iteration 460, loss = 1.31015695\n",
      "Iteration 461, loss = 1.31004994\n",
      "Iteration 462, loss = 1.30994062\n",
      "Iteration 463, loss = 1.30983434\n",
      "Iteration 464, loss = 1.30972668\n",
      "Iteration 465, loss = 1.30961966\n",
      "Iteration 466, loss = 1.30951124\n",
      "Iteration 467, loss = 1.30940448\n",
      "Iteration 468, loss = 1.30929870\n",
      "Iteration 469, loss = 1.30919194\n",
      "Iteration 470, loss = 1.30908409\n",
      "Iteration 471, loss = 1.30897872\n",
      "Iteration 472, loss = 1.30887123\n",
      "Iteration 473, loss = 1.30876568\n",
      "Iteration 474, loss = 1.30865929\n",
      "Iteration 475, loss = 1.30855318\n",
      "Iteration 476, loss = 1.30844765\n",
      "Iteration 477, loss = 1.30834003\n",
      "Iteration 478, loss = 1.30823651\n",
      "Iteration 479, loss = 1.30812764\n",
      "Iteration 480, loss = 1.30802322\n",
      "Iteration 481, loss = 1.30791880\n",
      "Iteration 482, loss = 1.30781252\n",
      "Iteration 483, loss = 1.30770643\n",
      "Iteration 484, loss = 1.30760121\n",
      "Iteration 485, loss = 1.30749869\n",
      "Iteration 486, loss = 1.30739077\n",
      "Iteration 487, loss = 1.30728596\n",
      "Iteration 488, loss = 1.30718139\n",
      "Iteration 489, loss = 1.30707495\n",
      "Iteration 490, loss = 1.30697020\n",
      "Iteration 491, loss = 1.30686980\n",
      "Iteration 492, loss = 1.30676135\n",
      "Iteration 493, loss = 1.30665796\n",
      "Iteration 494, loss = 1.30655160\n",
      "Iteration 495, loss = 1.30645045\n",
      "Iteration 496, loss = 1.30634408\n",
      "Iteration 497, loss = 1.30624070\n",
      "Iteration 498, loss = 1.30613605\n",
      "Iteration 499, loss = 1.30603217\n",
      "Iteration 500, loss = 1.30592867\n",
      "Iteration 501, loss = 1.30582452\n",
      "Iteration 502, loss = 1.30572152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 1.30561633\n",
      "Iteration 504, loss = 1.30551308\n",
      "Iteration 505, loss = 1.30541084\n",
      "Iteration 506, loss = 1.30530548\n",
      "Iteration 507, loss = 1.30520322\n",
      "Iteration 508, loss = 1.30510000\n",
      "Iteration 509, loss = 1.30499673\n",
      "Iteration 510, loss = 1.30489393\n",
      "Iteration 511, loss = 1.30479033\n",
      "Iteration 512, loss = 1.30468783\n",
      "Iteration 513, loss = 1.30458479\n",
      "Iteration 514, loss = 1.30448376\n",
      "Iteration 515, loss = 1.30438100\n",
      "Iteration 516, loss = 1.30427732\n",
      "Iteration 517, loss = 1.30417562\n",
      "Iteration 518, loss = 1.30407252\n",
      "Iteration 519, loss = 1.30397130\n",
      "Iteration 520, loss = 1.30387141\n",
      "Iteration 521, loss = 1.30376814\n",
      "Iteration 522, loss = 1.30366463\n",
      "Iteration 523, loss = 1.30356307\n",
      "Iteration 524, loss = 1.30346244\n",
      "Iteration 525, loss = 1.30336274\n",
      "Iteration 526, loss = 1.30326025\n",
      "Iteration 527, loss = 1.30315857\n",
      "Iteration 528, loss = 1.30305714\n",
      "Iteration 529, loss = 1.30295630\n",
      "Iteration 530, loss = 1.30285425\n",
      "Iteration 531, loss = 1.30275396\n",
      "Iteration 532, loss = 1.30265551\n",
      "Iteration 533, loss = 1.30255317\n",
      "Iteration 534, loss = 1.30245277\n",
      "Iteration 535, loss = 1.30235085\n",
      "Iteration 536, loss = 1.30225278\n",
      "Iteration 537, loss = 1.30215204\n",
      "Iteration 538, loss = 1.30205238\n",
      "Iteration 539, loss = 1.30195259\n",
      "Iteration 540, loss = 1.30185222\n",
      "Iteration 541, loss = 1.30175476\n",
      "Iteration 542, loss = 1.30165378\n",
      "Iteration 543, loss = 1.30155343\n",
      "Iteration 544, loss = 1.30145517\n",
      "Iteration 545, loss = 1.30135560\n",
      "Iteration 546, loss = 1.30125608\n",
      "Iteration 547, loss = 1.30115990\n",
      "Iteration 548, loss = 1.30105853\n",
      "Iteration 549, loss = 1.30096012\n",
      "Iteration 550, loss = 1.30086154\n",
      "Iteration 551, loss = 1.30076236\n",
      "Iteration 552, loss = 1.30066379\n",
      "Iteration 553, loss = 1.30056607\n",
      "Iteration 554, loss = 1.30046942\n",
      "Iteration 555, loss = 1.30036887\n",
      "Iteration 556, loss = 1.30027123\n",
      "Iteration 557, loss = 1.30017346\n",
      "Iteration 558, loss = 1.30007653\n",
      "Iteration 559, loss = 1.29997858\n",
      "Iteration 560, loss = 1.29988152\n",
      "Iteration 561, loss = 1.29978466\n",
      "Iteration 562, loss = 1.29968698\n",
      "Iteration 563, loss = 1.29958975\n",
      "Iteration 564, loss = 1.29949404\n",
      "Iteration 565, loss = 1.29939527\n",
      "Iteration 566, loss = 1.29930037\n",
      "Iteration 567, loss = 1.29920272\n",
      "Iteration 568, loss = 1.29910572\n",
      "Iteration 569, loss = 1.29901021\n",
      "Iteration 570, loss = 1.29891370\n",
      "Iteration 571, loss = 1.29881861\n",
      "Iteration 572, loss = 1.29872221\n",
      "Iteration 573, loss = 1.29862567\n",
      "Iteration 574, loss = 1.29852963\n",
      "Iteration 575, loss = 1.29843418\n",
      "Iteration 576, loss = 1.29833750\n",
      "Iteration 577, loss = 1.29824336\n",
      "Iteration 578, loss = 1.29814650\n",
      "Iteration 579, loss = 1.29805233\n",
      "Iteration 580, loss = 1.29795643\n",
      "Iteration 581, loss = 1.29786215\n",
      "Iteration 582, loss = 1.29776582\n",
      "Iteration 583, loss = 1.29767074\n",
      "Iteration 584, loss = 1.29757605\n",
      "Iteration 585, loss = 1.29748186\n",
      "Iteration 586, loss = 1.29738711\n",
      "Iteration 587, loss = 1.29729152\n",
      "Iteration 588, loss = 1.29719801\n",
      "Iteration 589, loss = 1.29710385\n",
      "Iteration 590, loss = 1.29701014\n",
      "Iteration 591, loss = 1.29691480\n",
      "Iteration 592, loss = 1.29682101\n",
      "Iteration 593, loss = 1.29672679\n",
      "Iteration 594, loss = 1.29663388\n",
      "Iteration 595, loss = 1.29653911\n",
      "Iteration 596, loss = 1.29644539\n",
      "Iteration 597, loss = 1.29635231\n",
      "Iteration 598, loss = 1.29625825\n",
      "Iteration 599, loss = 1.29616625\n",
      "Iteration 600, loss = 1.29607419\n",
      "Iteration 601, loss = 1.29597927\n",
      "Iteration 602, loss = 1.29588502\n",
      "Iteration 603, loss = 1.29579372\n",
      "Iteration 604, loss = 1.29570041\n",
      "Iteration 605, loss = 1.29561057\n",
      "Iteration 606, loss = 1.29551547\n",
      "Iteration 607, loss = 1.29542330\n",
      "Iteration 608, loss = 1.29533057\n",
      "Iteration 609, loss = 1.29523820\n",
      "Iteration 610, loss = 1.29514546\n",
      "Iteration 611, loss = 1.29505521\n",
      "Iteration 612, loss = 1.29496238\n",
      "Iteration 613, loss = 1.29487154\n",
      "Iteration 614, loss = 1.29477916\n",
      "Iteration 615, loss = 1.29468636\n",
      "Iteration 616, loss = 1.29459506\n",
      "Iteration 617, loss = 1.29450370\n",
      "Iteration 618, loss = 1.29441200\n",
      "Iteration 619, loss = 1.29432090\n",
      "Iteration 620, loss = 1.29422992\n",
      "Iteration 621, loss = 1.29413814\n",
      "Iteration 622, loss = 1.29404817\n",
      "Iteration 623, loss = 1.29395891\n",
      "Iteration 624, loss = 1.29386595\n",
      "Iteration 625, loss = 1.29377501\n",
      "Iteration 626, loss = 1.29368585\n",
      "Iteration 627, loss = 1.29359481\n",
      "Iteration 628, loss = 1.29350582\n",
      "Iteration 629, loss = 1.29341509\n",
      "Iteration 630, loss = 1.29332469\n",
      "Iteration 631, loss = 1.29323523\n",
      "Iteration 632, loss = 1.29314469\n",
      "Iteration 633, loss = 1.29305520\n",
      "Iteration 634, loss = 1.29296458\n",
      "Iteration 635, loss = 1.29287546\n",
      "Iteration 636, loss = 1.29278649\n",
      "Iteration 637, loss = 1.29269693\n",
      "Iteration 638, loss = 1.29260732\n",
      "Iteration 639, loss = 1.29251745\n",
      "Iteration 640, loss = 1.29243125\n",
      "Iteration 641, loss = 1.29234153\n",
      "Iteration 642, loss = 1.29224980\n",
      "Iteration 643, loss = 1.29216146\n",
      "Iteration 644, loss = 1.29207225\n",
      "Iteration 645, loss = 1.29198379\n",
      "Iteration 646, loss = 1.29189464\n",
      "Iteration 647, loss = 1.29180545\n",
      "Iteration 648, loss = 1.29171673\n",
      "Iteration 649, loss = 1.29162904\n",
      "Iteration 650, loss = 1.29154246\n",
      "Iteration 651, loss = 1.29145213\n",
      "Iteration 652, loss = 1.29136518\n",
      "Iteration 653, loss = 1.29127570\n",
      "Iteration 654, loss = 1.29118855\n",
      "Iteration 655, loss = 1.29110084\n",
      "Iteration 656, loss = 1.29101202\n",
      "Iteration 657, loss = 1.29092433\n",
      "Iteration 658, loss = 1.29083759\n",
      "Iteration 659, loss = 1.29075054\n",
      "Iteration 660, loss = 1.29066387\n",
      "Iteration 661, loss = 1.29057635\n",
      "Iteration 662, loss = 1.29048809\n",
      "Iteration 663, loss = 1.29040266\n",
      "Iteration 664, loss = 1.29031636\n",
      "Iteration 665, loss = 1.29022945\n",
      "Iteration 666, loss = 1.29014189\n",
      "Iteration 667, loss = 1.29005672\n",
      "Iteration 668, loss = 1.28997005\n",
      "Iteration 669, loss = 1.28988364\n",
      "Iteration 670, loss = 1.28979856\n",
      "Iteration 671, loss = 1.28971164\n",
      "Iteration 672, loss = 1.28962650\n",
      "Iteration 673, loss = 1.28954076\n",
      "Iteration 674, loss = 1.28945477\n",
      "Iteration 675, loss = 1.28936937\n",
      "Iteration 676, loss = 1.28928406\n",
      "Iteration 677, loss = 1.28919959\n",
      "Iteration 678, loss = 1.28911363\n",
      "Iteration 679, loss = 1.28902746\n",
      "Iteration 680, loss = 1.28894177\n",
      "Iteration 681, loss = 1.28885677\n",
      "Iteration 682, loss = 1.28877373\n",
      "Iteration 683, loss = 1.28868717\n",
      "Iteration 684, loss = 1.28860229\n",
      "Iteration 685, loss = 1.28851910\n",
      "Iteration 686, loss = 1.28843293\n",
      "Iteration 687, loss = 1.28834924\n",
      "Iteration 688, loss = 1.28826298\n",
      "Iteration 689, loss = 1.28818106\n",
      "Iteration 690, loss = 1.28809659\n",
      "Iteration 691, loss = 1.28801260\n",
      "Iteration 692, loss = 1.28792802\n",
      "Iteration 693, loss = 1.28784389\n",
      "Iteration 694, loss = 1.28776192\n",
      "Iteration 695, loss = 1.28767583\n",
      "Iteration 696, loss = 1.28759370\n",
      "Iteration 697, loss = 1.28751157\n",
      "Iteration 698, loss = 1.28742689\n",
      "Iteration 699, loss = 1.28734523\n",
      "Iteration 700, loss = 1.28726146\n",
      "Iteration 701, loss = 1.28717902\n",
      "Iteration 702, loss = 1.28709503\n",
      "Iteration 703, loss = 1.28701223\n",
      "Iteration 704, loss = 1.28693058\n",
      "Iteration 705, loss = 1.28684766\n",
      "Iteration 706, loss = 1.28676554\n",
      "Iteration 707, loss = 1.28668301\n",
      "Iteration 708, loss = 1.28660074\n",
      "Iteration 709, loss = 1.28651902\n",
      "Iteration 710, loss = 1.28643606\n",
      "Iteration 711, loss = 1.28635511\n",
      "Iteration 712, loss = 1.28627268\n",
      "Iteration 713, loss = 1.28619144\n",
      "Iteration 714, loss = 1.28610923\n",
      "Iteration 715, loss = 1.28602822\n",
      "Iteration 716, loss = 1.28594629\n",
      "Iteration 717, loss = 1.28586624\n",
      "Iteration 718, loss = 1.28578345\n",
      "Iteration 719, loss = 1.28570321\n",
      "Iteration 720, loss = 1.28562126\n",
      "Iteration 721, loss = 1.28554051\n",
      "Iteration 722, loss = 1.28545918\n",
      "Iteration 723, loss = 1.28538087\n",
      "Iteration 724, loss = 1.28529830\n",
      "Iteration 725, loss = 1.28521801\n",
      "Iteration 726, loss = 1.28513717\n",
      "Iteration 727, loss = 1.28505681\n",
      "Iteration 728, loss = 1.28497606\n",
      "Iteration 729, loss = 1.28489547\n",
      "Iteration 730, loss = 1.28481559\n",
      "Iteration 731, loss = 1.28473541\n",
      "Iteration 732, loss = 1.28465563\n",
      "Iteration 733, loss = 1.28457547\n",
      "Iteration 734, loss = 1.28449614\n",
      "Iteration 735, loss = 1.28441475\n",
      "Iteration 736, loss = 1.28433500\n",
      "Iteration 737, loss = 1.28425721\n",
      "Iteration 738, loss = 1.28417613\n",
      "Iteration 739, loss = 1.28409589\n",
      "Iteration 740, loss = 1.28401708\n",
      "Iteration 741, loss = 1.28393846\n",
      "Iteration 742, loss = 1.28385780\n",
      "Iteration 743, loss = 1.28377736\n",
      "Iteration 744, loss = 1.28369892\n",
      "Iteration 745, loss = 1.28361976\n",
      "Iteration 746, loss = 1.28354174\n",
      "Iteration 747, loss = 1.28346040\n",
      "Iteration 748, loss = 1.28338383\n",
      "Iteration 749, loss = 1.28330396\n",
      "Iteration 750, loss = 1.28322573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 751, loss = 1.28314732\n",
      "Iteration 752, loss = 1.28306822\n",
      "Iteration 753, loss = 1.28298844\n",
      "Iteration 754, loss = 1.28291075\n",
      "Iteration 755, loss = 1.28283255\n",
      "Iteration 756, loss = 1.28275434\n",
      "Iteration 757, loss = 1.28267616\n",
      "Iteration 758, loss = 1.28259736\n",
      "Iteration 759, loss = 1.28252016\n",
      "Iteration 760, loss = 1.28244117\n",
      "Iteration 761, loss = 1.28236481\n",
      "Iteration 762, loss = 1.28228587\n",
      "Iteration 763, loss = 1.28220937\n",
      "Iteration 764, loss = 1.28213016\n",
      "Iteration 765, loss = 1.28205509\n",
      "Iteration 766, loss = 1.28197555\n",
      "Iteration 767, loss = 1.28189746\n",
      "Iteration 768, loss = 1.28182017\n",
      "Iteration 769, loss = 1.28174353\n",
      "Iteration 770, loss = 1.28166639\n",
      "Iteration 771, loss = 1.28158808\n",
      "Iteration 772, loss = 1.28151174\n",
      "Iteration 773, loss = 1.28143424\n",
      "Iteration 774, loss = 1.28135709\n",
      "Iteration 775, loss = 1.28128008\n",
      "Iteration 776, loss = 1.28120416\n",
      "Iteration 777, loss = 1.28112720\n",
      "Iteration 778, loss = 1.28105205\n",
      "Iteration 779, loss = 1.28097451\n",
      "Iteration 780, loss = 1.28090003\n",
      "Iteration 781, loss = 1.28082121\n",
      "Iteration 782, loss = 1.28074550\n",
      "Iteration 783, loss = 1.28066882\n",
      "Iteration 784, loss = 1.28059329\n",
      "Iteration 785, loss = 1.28051724\n",
      "Iteration 786, loss = 1.28044159\n",
      "Iteration 787, loss = 1.28036482\n",
      "Iteration 788, loss = 1.28028918\n",
      "Iteration 789, loss = 1.28021419\n",
      "Iteration 790, loss = 1.28013860\n",
      "Iteration 791, loss = 1.28006286\n",
      "Iteration 792, loss = 1.27998905\n",
      "Iteration 793, loss = 1.27991122\n",
      "Iteration 794, loss = 1.27983581\n",
      "Iteration 795, loss = 1.27976061\n",
      "Iteration 796, loss = 1.27968709\n",
      "Iteration 797, loss = 1.27961119\n",
      "Iteration 798, loss = 1.27953694\n",
      "Iteration 799, loss = 1.27946060\n",
      "Iteration 800, loss = 1.27938538\n",
      "Iteration 801, loss = 1.27931022\n",
      "Iteration 802, loss = 1.27923579\n",
      "Iteration 803, loss = 1.27916004\n",
      "Iteration 804, loss = 1.27908539\n",
      "Iteration 805, loss = 1.27901182\n",
      "Iteration 806, loss = 1.27893636\n",
      "Iteration 807, loss = 1.27886356\n",
      "Iteration 808, loss = 1.27878754\n",
      "Iteration 809, loss = 1.27871561\n",
      "Iteration 810, loss = 1.27863963\n",
      "Iteration 811, loss = 1.27856567\n",
      "Iteration 812, loss = 1.27849219\n",
      "Iteration 813, loss = 1.27841855\n",
      "Iteration 814, loss = 1.27834469\n",
      "Iteration 815, loss = 1.27827014\n",
      "Iteration 816, loss = 1.27819867\n",
      "Iteration 817, loss = 1.27812346\n",
      "Iteration 818, loss = 1.27805245\n",
      "Iteration 819, loss = 1.27797697\n",
      "Iteration 820, loss = 1.27790334\n",
      "Iteration 821, loss = 1.27783011\n",
      "Iteration 822, loss = 1.27775585\n",
      "Iteration 823, loss = 1.27768392\n",
      "Iteration 824, loss = 1.27761143\n",
      "Iteration 825, loss = 1.27753772\n",
      "Iteration 826, loss = 1.27746435\n",
      "Iteration 827, loss = 1.27739287\n",
      "Iteration 828, loss = 1.27731796\n",
      "Iteration 829, loss = 1.27724578\n",
      "Iteration 830, loss = 1.27717375\n",
      "Iteration 831, loss = 1.27710002\n",
      "Iteration 832, loss = 1.27702980\n",
      "Iteration 833, loss = 1.27695726\n",
      "Iteration 834, loss = 1.27688346\n",
      "Iteration 835, loss = 1.27681216\n",
      "Iteration 836, loss = 1.27673885\n",
      "Iteration 837, loss = 1.27666608\n",
      "Iteration 838, loss = 1.27659582\n",
      "Iteration 839, loss = 1.27652261\n",
      "Iteration 840, loss = 1.27644991\n",
      "Iteration 841, loss = 1.27637852\n",
      "Iteration 842, loss = 1.27630701\n",
      "Iteration 843, loss = 1.27623469\n",
      "Iteration 844, loss = 1.27616319\n",
      "Iteration 845, loss = 1.27609170\n",
      "Iteration 846, loss = 1.27602036\n",
      "Iteration 847, loss = 1.27594783\n",
      "Iteration 848, loss = 1.27587656\n",
      "Iteration 849, loss = 1.27580540\n",
      "Iteration 850, loss = 1.27573316\n",
      "Iteration 851, loss = 1.27566265\n",
      "Iteration 852, loss = 1.27559235\n",
      "Iteration 853, loss = 1.27552025\n",
      "Iteration 854, loss = 1.27544905\n",
      "Iteration 855, loss = 1.27537794\n",
      "Iteration 856, loss = 1.27530769\n",
      "Iteration 857, loss = 1.27523573\n",
      "Iteration 858, loss = 1.27516544\n",
      "Iteration 859, loss = 1.27509413\n",
      "Iteration 860, loss = 1.27502445\n",
      "Iteration 861, loss = 1.27495419\n",
      "Iteration 862, loss = 1.27488258\n",
      "Iteration 863, loss = 1.27481506\n",
      "Iteration 864, loss = 1.27474179\n",
      "Iteration 865, loss = 1.27467221\n",
      "Iteration 866, loss = 1.27460061\n",
      "Iteration 867, loss = 1.27453108\n",
      "Iteration 868, loss = 1.27446133\n",
      "Iteration 869, loss = 1.27439205\n",
      "Iteration 870, loss = 1.27432027\n",
      "Iteration 871, loss = 1.27425128\n",
      "Iteration 872, loss = 1.27418184\n",
      "Iteration 873, loss = 1.27411100\n",
      "Iteration 874, loss = 1.27404382\n",
      "Iteration 875, loss = 1.27397153\n",
      "Iteration 876, loss = 1.27390243\n",
      "Iteration 877, loss = 1.27383249\n",
      "Iteration 878, loss = 1.27376368\n",
      "Iteration 879, loss = 1.27369367\n",
      "Iteration 880, loss = 1.27362338\n",
      "Iteration 881, loss = 1.27355343\n",
      "Iteration 882, loss = 1.27348531\n",
      "Iteration 883, loss = 1.27341664\n",
      "Iteration 884, loss = 1.27334690\n",
      "Iteration 885, loss = 1.27327799\n",
      "Iteration 886, loss = 1.27320923\n",
      "Iteration 887, loss = 1.27314025\n",
      "Iteration 888, loss = 1.27307247\n",
      "Iteration 889, loss = 1.27300291\n",
      "Iteration 890, loss = 1.27293490\n",
      "Iteration 891, loss = 1.27286537\n",
      "Iteration 892, loss = 1.27279889\n",
      "Iteration 893, loss = 1.27272875\n",
      "Iteration 894, loss = 1.27266054\n",
      "Iteration 895, loss = 1.27259243\n",
      "Iteration 896, loss = 1.27252451\n",
      "Iteration 897, loss = 1.27245588\n",
      "Iteration 898, loss = 1.27238848\n",
      "Iteration 899, loss = 1.27231973\n",
      "Iteration 900, loss = 1.27225106\n",
      "Iteration 901, loss = 1.27218415\n",
      "Iteration 902, loss = 1.27211493\n",
      "Iteration 903, loss = 1.27204763\n",
      "Iteration 904, loss = 1.27197966\n",
      "Iteration 905, loss = 1.27191151\n",
      "Iteration 906, loss = 1.27184469\n",
      "Iteration 907, loss = 1.27177764\n",
      "Iteration 908, loss = 1.27170877\n",
      "Iteration 909, loss = 1.27164530\n",
      "Iteration 910, loss = 1.27157508\n",
      "Iteration 911, loss = 1.27150661\n",
      "Iteration 912, loss = 1.27144053\n",
      "Iteration 913, loss = 1.27137262\n",
      "Iteration 914, loss = 1.27130585\n",
      "Iteration 915, loss = 1.27123838\n",
      "Iteration 916, loss = 1.27117187\n",
      "Iteration 917, loss = 1.27110526\n",
      "Iteration 918, loss = 1.27103793\n",
      "Iteration 919, loss = 1.27097140\n",
      "Iteration 920, loss = 1.27090482\n",
      "Iteration 921, loss = 1.27083931\n",
      "Iteration 922, loss = 1.27077013\n",
      "Iteration 923, loss = 1.27070484\n",
      "Iteration 924, loss = 1.27063939\n",
      "Iteration 925, loss = 1.27057122\n",
      "Iteration 926, loss = 1.27050447\n",
      "Iteration 927, loss = 1.27043891\n",
      "Iteration 928, loss = 1.27037235\n",
      "Iteration 929, loss = 1.27030674\n",
      "Iteration 930, loss = 1.27024113\n",
      "Iteration 931, loss = 1.27017403\n",
      "Iteration 932, loss = 1.27010795\n",
      "Iteration 933, loss = 1.27004313\n",
      "Iteration 934, loss = 1.26997699\n",
      "Iteration 935, loss = 1.26991016\n",
      "Iteration 936, loss = 1.26984540\n",
      "Iteration 937, loss = 1.26977969\n",
      "Iteration 938, loss = 1.26971415\n",
      "Iteration 939, loss = 1.26964832\n",
      "Iteration 940, loss = 1.26958403\n",
      "Iteration 941, loss = 1.26951875\n",
      "Iteration 942, loss = 1.26945306\n",
      "Iteration 943, loss = 1.26938898\n",
      "Iteration 944, loss = 1.26932284\n",
      "Iteration 945, loss = 1.26925814\n",
      "Iteration 946, loss = 1.26919271\n",
      "Iteration 947, loss = 1.26913068\n",
      "Iteration 948, loss = 1.26906247\n",
      "Iteration 949, loss = 1.26900155\n",
      "Iteration 950, loss = 1.26893385\n",
      "Iteration 951, loss = 1.26887130\n",
      "Iteration 952, loss = 1.26880497\n",
      "Iteration 953, loss = 1.26873951\n",
      "Iteration 954, loss = 1.26867580\n",
      "Iteration 955, loss = 1.26861149\n",
      "Iteration 956, loss = 1.26854765\n",
      "Iteration 957, loss = 1.26848378\n",
      "Iteration 958, loss = 1.26841869\n",
      "Iteration 959, loss = 1.26835575\n",
      "Iteration 960, loss = 1.26829124\n",
      "Iteration 961, loss = 1.26822763\n",
      "Iteration 962, loss = 1.26816266\n",
      "Iteration 963, loss = 1.26810039\n",
      "Iteration 964, loss = 1.26803638\n",
      "Iteration 965, loss = 1.26797234\n",
      "Iteration 966, loss = 1.26790750\n",
      "Iteration 967, loss = 1.26784429\n",
      "Iteration 968, loss = 1.26778110\n",
      "Iteration 969, loss = 1.26771806\n",
      "Iteration 970, loss = 1.26765443\n",
      "Iteration 971, loss = 1.26759216\n",
      "Iteration 972, loss = 1.26752838\n",
      "Iteration 973, loss = 1.26746545\n",
      "Iteration 974, loss = 1.26740399\n",
      "Iteration 975, loss = 1.26733840\n",
      "Iteration 976, loss = 1.26727766\n",
      "Iteration 977, loss = 1.26721567\n",
      "Iteration 978, loss = 1.26715127\n",
      "Iteration 979, loss = 1.26708839\n",
      "Iteration 980, loss = 1.26702676\n",
      "Iteration 981, loss = 1.26696649\n",
      "Iteration 982, loss = 1.26690186\n",
      "Iteration 983, loss = 1.26684300\n",
      "Iteration 984, loss = 1.26677899\n",
      "Iteration 985, loss = 1.26671600\n",
      "Iteration 986, loss = 1.26665424\n",
      "Iteration 987, loss = 1.26659245\n",
      "Iteration 988, loss = 1.26652969\n",
      "Iteration 989, loss = 1.26647065\n",
      "Iteration 990, loss = 1.26640732\n",
      "Iteration 991, loss = 1.26634453\n",
      "Iteration 992, loss = 1.26628259\n",
      "Iteration 993, loss = 1.26622055\n",
      "Iteration 994, loss = 1.26615895\n",
      "Iteration 995, loss = 1.26609780\n",
      "Iteration 996, loss = 1.26603724\n",
      "Iteration 997, loss = 1.26597592\n",
      "Iteration 998, loss = 1.26591349\n",
      "Iteration 999, loss = 1.26585430\n",
      "Iteration 1000, loss = 1.26579286\n",
      "Iteration 1001, loss = 1.26573038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1002, loss = 1.26566908\n",
      "Iteration 1003, loss = 1.26560768\n",
      "Iteration 1004, loss = 1.26554695\n",
      "Iteration 1005, loss = 1.26548637\n",
      "Iteration 1006, loss = 1.26542543\n",
      "Iteration 1007, loss = 1.26536601\n",
      "Iteration 1008, loss = 1.26530378\n",
      "Iteration 1009, loss = 1.26524376\n",
      "Iteration 1010, loss = 1.26518293\n",
      "Iteration 1011, loss = 1.26512183\n",
      "Iteration 1012, loss = 1.26506136\n",
      "Iteration 1013, loss = 1.26500123\n",
      "Iteration 1014, loss = 1.26494170\n",
      "Iteration 1015, loss = 1.26488199\n",
      "Iteration 1016, loss = 1.26482132\n",
      "Iteration 1017, loss = 1.26476160\n",
      "Iteration 1018, loss = 1.26470131\n",
      "Iteration 1019, loss = 1.26464196\n",
      "Iteration 1020, loss = 1.26458203\n",
      "Iteration 1021, loss = 1.26452354\n",
      "Iteration 1022, loss = 1.26446325\n",
      "Iteration 1023, loss = 1.26440237\n",
      "Iteration 1024, loss = 1.26434446\n",
      "Iteration 1025, loss = 1.26428518\n",
      "Iteration 1026, loss = 1.26422508\n",
      "Iteration 1027, loss = 1.26416595\n",
      "Iteration 1028, loss = 1.26410742\n",
      "Iteration 1029, loss = 1.26404829\n",
      "Iteration 1030, loss = 1.26398941\n",
      "Iteration 1031, loss = 1.26393059\n",
      "Iteration 1032, loss = 1.26387221\n",
      "Iteration 1033, loss = 1.26381317\n",
      "Iteration 1034, loss = 1.26375318\n",
      "Iteration 1035, loss = 1.26369806\n",
      "Iteration 1036, loss = 1.26363586\n",
      "Iteration 1037, loss = 1.26358183\n",
      "Iteration 1038, loss = 1.26351992\n",
      "Iteration 1039, loss = 1.26346060\n",
      "Iteration 1040, loss = 1.26340230\n",
      "Iteration 1041, loss = 1.26334503\n",
      "Iteration 1042, loss = 1.26328584\n",
      "Iteration 1043, loss = 1.26322714\n",
      "Iteration 1044, loss = 1.26317099\n",
      "Iteration 1045, loss = 1.26311117\n",
      "Iteration 1046, loss = 1.26305441\n",
      "Iteration 1047, loss = 1.26299519\n",
      "Iteration 1048, loss = 1.26293815\n",
      "Iteration 1049, loss = 1.26288113\n",
      "Iteration 1050, loss = 1.26282266\n",
      "Iteration 1051, loss = 1.26276556\n",
      "Iteration 1052, loss = 1.26270664\n",
      "Iteration 1053, loss = 1.26265005\n",
      "Iteration 1054, loss = 1.26259338\n",
      "Iteration 1055, loss = 1.26253370\n",
      "Iteration 1056, loss = 1.26247747\n",
      "Iteration 1057, loss = 1.26241950\n",
      "Iteration 1058, loss = 1.26236312\n",
      "Iteration 1059, loss = 1.26230465\n",
      "Iteration 1060, loss = 1.26224943\n",
      "Iteration 1061, loss = 1.26219197\n",
      "Iteration 1062, loss = 1.26213458\n",
      "Iteration 1063, loss = 1.26207757\n",
      "Iteration 1064, loss = 1.26202223\n",
      "Iteration 1065, loss = 1.26196577\n",
      "Iteration 1066, loss = 1.26190889\n",
      "Iteration 1067, loss = 1.26185167\n",
      "Iteration 1068, loss = 1.26179447\n",
      "Iteration 1069, loss = 1.26173963\n",
      "Iteration 1070, loss = 1.26168300\n",
      "Iteration 1071, loss = 1.26162610\n",
      "Iteration 1072, loss = 1.26156946\n",
      "Iteration 1073, loss = 1.26151397\n",
      "Iteration 1074, loss = 1.26145834\n",
      "Iteration 1075, loss = 1.26140246\n",
      "Iteration 1076, loss = 1.26134510\n",
      "Iteration 1077, loss = 1.26128939\n",
      "Iteration 1078, loss = 1.26123454\n",
      "Iteration 1079, loss = 1.26117822\n",
      "Iteration 1080, loss = 1.26112242\n",
      "Iteration 1081, loss = 1.26106751\n",
      "Iteration 1082, loss = 1.26101095\n",
      "Iteration 1083, loss = 1.26095566\n",
      "Iteration 1084, loss = 1.26090020\n",
      "Iteration 1085, loss = 1.26084418\n",
      "Iteration 1086, loss = 1.26079101\n",
      "Iteration 1087, loss = 1.26073303\n",
      "Iteration 1088, loss = 1.26067863\n",
      "Iteration 1089, loss = 1.26062349\n",
      "Iteration 1090, loss = 1.26056884\n",
      "Iteration 1091, loss = 1.26051451\n",
      "Iteration 1092, loss = 1.26045840\n",
      "Iteration 1093, loss = 1.26040299\n",
      "Iteration 1094, loss = 1.26034844\n",
      "Iteration 1095, loss = 1.26029379\n",
      "Iteration 1096, loss = 1.26024004\n",
      "Iteration 1097, loss = 1.26018508\n",
      "Iteration 1098, loss = 1.26013022\n",
      "Iteration 1099, loss = 1.26007640\n",
      "Iteration 1100, loss = 1.26002215\n",
      "Iteration 1101, loss = 1.25996671\n",
      "Iteration 1102, loss = 1.25991297\n",
      "Iteration 1103, loss = 1.25985897\n",
      "Iteration 1104, loss = 1.25980470\n",
      "Iteration 1105, loss = 1.25975069\n",
      "Iteration 1106, loss = 1.25969621\n",
      "Iteration 1107, loss = 1.25964244\n",
      "Iteration 1108, loss = 1.25958936\n",
      "Iteration 1109, loss = 1.25953468\n",
      "Iteration 1110, loss = 1.25948050\n",
      "Iteration 1111, loss = 1.25942705\n",
      "Iteration 1112, loss = 1.25937491\n",
      "Iteration 1113, loss = 1.25931997\n",
      "Iteration 1114, loss = 1.25926766\n",
      "Iteration 1115, loss = 1.25921404\n",
      "Iteration 1116, loss = 1.25915871\n",
      "Iteration 1117, loss = 1.25910640\n",
      "Iteration 1118, loss = 1.25905322\n",
      "Iteration 1119, loss = 1.25899946\n",
      "Iteration 1120, loss = 1.25894574\n",
      "Iteration 1121, loss = 1.25889179\n",
      "Iteration 1122, loss = 1.25884006\n",
      "Iteration 1123, loss = 1.25878606\n",
      "Iteration 1124, loss = 1.25873279\n",
      "Iteration 1125, loss = 1.25867895\n",
      "Iteration 1126, loss = 1.25862665\n",
      "Iteration 1127, loss = 1.25857318\n",
      "Iteration 1128, loss = 1.25852055\n",
      "Iteration 1129, loss = 1.25846819\n",
      "Iteration 1130, loss = 1.25841570\n",
      "Iteration 1131, loss = 1.25836223\n",
      "Iteration 1132, loss = 1.25831009\n",
      "Iteration 1133, loss = 1.25825742\n",
      "Iteration 1134, loss = 1.25820554\n",
      "Iteration 1135, loss = 1.25815186\n",
      "Iteration 1136, loss = 1.25810019\n",
      "Iteration 1137, loss = 1.25804751\n",
      "Iteration 1138, loss = 1.25799557\n",
      "Iteration 1139, loss = 1.25794320\n",
      "Iteration 1140, loss = 1.25789091\n",
      "Iteration 1141, loss = 1.25783878\n",
      "Iteration 1142, loss = 1.25778611\n",
      "Iteration 1143, loss = 1.25773446\n",
      "Iteration 1144, loss = 1.25768208\n",
      "Iteration 1145, loss = 1.25762958\n",
      "Iteration 1146, loss = 1.25757900\n",
      "Iteration 1147, loss = 1.25752597\n",
      "Iteration 1148, loss = 1.25747435\n",
      "Iteration 1149, loss = 1.25742221\n",
      "Iteration 1150, loss = 1.25737091\n",
      "Iteration 1151, loss = 1.25731919\n",
      "Iteration 1152, loss = 1.25726787\n",
      "Iteration 1153, loss = 1.25721534\n",
      "Iteration 1154, loss = 1.25716438\n",
      "Iteration 1155, loss = 1.25711393\n",
      "Iteration 1156, loss = 1.25706193\n",
      "Iteration 1157, loss = 1.25701072\n",
      "Iteration 1158, loss = 1.25695854\n",
      "Iteration 1159, loss = 1.25690979\n",
      "Iteration 1160, loss = 1.25685633\n",
      "Iteration 1161, loss = 1.25680594\n",
      "Iteration 1162, loss = 1.25675486\n",
      "Iteration 1163, loss = 1.25670416\n",
      "Iteration 1164, loss = 1.25665464\n",
      "Iteration 1165, loss = 1.25660302\n",
      "Iteration 1166, loss = 1.25655148\n",
      "Iteration 1167, loss = 1.25650105\n",
      "Iteration 1168, loss = 1.25644969\n",
      "Iteration 1169, loss = 1.25639984\n",
      "Iteration 1170, loss = 1.25634937\n",
      "Iteration 1171, loss = 1.25629789\n",
      "Iteration 1172, loss = 1.25624763\n",
      "Iteration 1173, loss = 1.25619677\n",
      "Iteration 1174, loss = 1.25614758\n",
      "Iteration 1175, loss = 1.25609601\n",
      "Iteration 1176, loss = 1.25604708\n",
      "Iteration 1177, loss = 1.25599518\n",
      "Iteration 1178, loss = 1.25594477\n",
      "Iteration 1179, loss = 1.25589547\n",
      "Iteration 1180, loss = 1.25584416\n",
      "Iteration 1181, loss = 1.25579322\n",
      "Iteration 1182, loss = 1.25574351\n",
      "Iteration 1183, loss = 1.25569364\n",
      "Iteration 1184, loss = 1.25564385\n",
      "Iteration 1185, loss = 1.25559366\n",
      "Iteration 1186, loss = 1.25554479\n",
      "Iteration 1187, loss = 1.25549393\n",
      "Iteration 1188, loss = 1.25544415\n",
      "Iteration 1189, loss = 1.25539404\n",
      "Iteration 1190, loss = 1.25534466\n",
      "Iteration 1191, loss = 1.25529594\n",
      "Iteration 1192, loss = 1.25524452\n",
      "Iteration 1193, loss = 1.25519620\n",
      "Iteration 1194, loss = 1.25514619\n",
      "Iteration 1195, loss = 1.25509614\n",
      "Iteration 1196, loss = 1.25504718\n",
      "Iteration 1197, loss = 1.25499746\n",
      "Iteration 1198, loss = 1.25494794\n",
      "Iteration 1199, loss = 1.25489863\n",
      "Iteration 1200, loss = 1.25484971\n",
      "Iteration 1201, loss = 1.25480029\n",
      "Iteration 1202, loss = 1.25475076\n",
      "Iteration 1203, loss = 1.25470256\n",
      "Iteration 1204, loss = 1.25465236\n",
      "Iteration 1205, loss = 1.25460328\n",
      "Iteration 1206, loss = 1.25455477\n",
      "Iteration 1207, loss = 1.25450656\n",
      "Iteration 1208, loss = 1.25445658\n",
      "Iteration 1209, loss = 1.25440761\n",
      "Iteration 1210, loss = 1.25435864\n",
      "Iteration 1211, loss = 1.25430915\n",
      "Iteration 1212, loss = 1.25426089\n",
      "Iteration 1213, loss = 1.25421325\n",
      "Iteration 1214, loss = 1.25416386\n",
      "Iteration 1215, loss = 1.25411571\n",
      "Iteration 1216, loss = 1.25406765\n",
      "Iteration 1217, loss = 1.25401822\n",
      "Iteration 1218, loss = 1.25397095\n",
      "Iteration 1219, loss = 1.25392140\n",
      "Iteration 1220, loss = 1.25387371\n",
      "Iteration 1221, loss = 1.25382463\n",
      "Iteration 1222, loss = 1.25377697\n",
      "Iteration 1223, loss = 1.25373165\n",
      "Iteration 1224, loss = 1.25368107\n",
      "Iteration 1225, loss = 1.25363273\n",
      "Iteration 1226, loss = 1.25358503\n",
      "Iteration 1227, loss = 1.25353602\n",
      "Iteration 1228, loss = 1.25348864\n",
      "Iteration 1229, loss = 1.25344097\n",
      "Iteration 1230, loss = 1.25339347\n",
      "Iteration 1231, loss = 1.25334457\n",
      "Iteration 1232, loss = 1.25329740\n",
      "Iteration 1233, loss = 1.25324966\n",
      "Iteration 1234, loss = 1.25320662\n",
      "Iteration 1235, loss = 1.25315448\n",
      "Iteration 1236, loss = 1.25310795\n",
      "Iteration 1237, loss = 1.25306033\n",
      "Iteration 1238, loss = 1.25301197\n",
      "Iteration 1239, loss = 1.25296425\n",
      "Iteration 1240, loss = 1.25291667\n",
      "Iteration 1241, loss = 1.25287065\n",
      "Iteration 1242, loss = 1.25282164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1243, loss = 1.25277456\n",
      "Iteration 1244, loss = 1.25272746\n",
      "Iteration 1245, loss = 1.25267994\n",
      "Iteration 1246, loss = 1.25263376\n",
      "Iteration 1247, loss = 1.25258574\n",
      "Iteration 1248, loss = 1.25253857\n",
      "Iteration 1249, loss = 1.25249064\n",
      "Iteration 1250, loss = 1.25244491\n",
      "Iteration 1251, loss = 1.25239708\n",
      "Iteration 1252, loss = 1.25235106\n",
      "Iteration 1253, loss = 1.25230333\n",
      "Iteration 1254, loss = 1.25225875\n",
      "Iteration 1255, loss = 1.25220842\n",
      "Iteration 1256, loss = 1.25216176\n",
      "Iteration 1257, loss = 1.25211517\n",
      "Iteration 1258, loss = 1.25206779\n",
      "Iteration 1259, loss = 1.25202089\n",
      "Iteration 1260, loss = 1.25197525\n",
      "Iteration 1261, loss = 1.25192753\n",
      "Iteration 1262, loss = 1.25188084\n",
      "Iteration 1263, loss = 1.25183479\n",
      "Iteration 1264, loss = 1.25178929\n",
      "Iteration 1265, loss = 1.25174102\n",
      "Iteration 1266, loss = 1.25169460\n",
      "Iteration 1267, loss = 1.25164841\n",
      "Iteration 1268, loss = 1.25160158\n",
      "Iteration 1269, loss = 1.25155634\n",
      "Iteration 1270, loss = 1.25150831\n",
      "Iteration 1271, loss = 1.25146198\n",
      "Iteration 1272, loss = 1.25141719\n",
      "Iteration 1273, loss = 1.25137118\n",
      "Iteration 1274, loss = 1.25132454\n",
      "Iteration 1275, loss = 1.25127761\n",
      "Iteration 1276, loss = 1.25123157\n",
      "Iteration 1277, loss = 1.25118510\n",
      "Iteration 1278, loss = 1.25114072\n",
      "Iteration 1279, loss = 1.25109331\n",
      "Iteration 1280, loss = 1.25105128\n",
      "Iteration 1281, loss = 1.25100028\n",
      "Iteration 1282, loss = 1.25095503\n",
      "Iteration 1283, loss = 1.25090915\n",
      "Iteration 1284, loss = 1.25086387\n",
      "Iteration 1285, loss = 1.25081733\n",
      "Iteration 1286, loss = 1.25077108\n",
      "Iteration 1287, loss = 1.25072595\n",
      "Iteration 1288, loss = 1.25068024\n",
      "Iteration 1289, loss = 1.25063436\n",
      "Iteration 1290, loss = 1.25059024\n",
      "Iteration 1291, loss = 1.25054402\n",
      "Iteration 1292, loss = 1.25049879\n",
      "Iteration 1293, loss = 1.25045278\n",
      "Iteration 1294, loss = 1.25040801\n",
      "Iteration 1295, loss = 1.25036276\n",
      "Iteration 1296, loss = 1.25031658\n",
      "Iteration 1297, loss = 1.25027513\n",
      "Iteration 1298, loss = 1.25022638\n",
      "Iteration 1299, loss = 1.25018154\n",
      "Iteration 1300, loss = 1.25013582\n",
      "Iteration 1301, loss = 1.25009068\n",
      "Iteration 1302, loss = 1.25004576\n",
      "Iteration 1303, loss = 1.25000149\n",
      "Iteration 1304, loss = 1.24995592\n",
      "Iteration 1305, loss = 1.24991020\n",
      "Iteration 1306, loss = 1.24986677\n",
      "Iteration 1307, loss = 1.24982096\n",
      "Iteration 1308, loss = 1.24977608\n",
      "Iteration 1309, loss = 1.24973340\n",
      "Iteration 1310, loss = 1.24968666\n",
      "Iteration 1311, loss = 1.24964257\n",
      "Iteration 1312, loss = 1.24959799\n",
      "Iteration 1313, loss = 1.24955348\n",
      "Iteration 1314, loss = 1.24950789\n",
      "Iteration 1315, loss = 1.24946330\n",
      "Iteration 1316, loss = 1.24941879\n",
      "Iteration 1317, loss = 1.24937528\n",
      "Iteration 1318, loss = 1.24933116\n",
      "Iteration 1319, loss = 1.24928551\n",
      "Iteration 1320, loss = 1.24924357\n",
      "Iteration 1321, loss = 1.24919620\n",
      "Iteration 1322, loss = 1.24915274\n",
      "Iteration 1323, loss = 1.24910874\n",
      "Iteration 1324, loss = 1.24906297\n",
      "Iteration 1325, loss = 1.24901879\n",
      "Iteration 1326, loss = 1.24897565\n",
      "Iteration 1327, loss = 1.24893138\n",
      "Iteration 1328, loss = 1.24888677\n",
      "Iteration 1329, loss = 1.24884210\n",
      "Iteration 1330, loss = 1.24880101\n",
      "Iteration 1331, loss = 1.24875408\n",
      "Iteration 1332, loss = 1.24871120\n",
      "Iteration 1333, loss = 1.24866668\n",
      "Iteration 1334, loss = 1.24862186\n",
      "Iteration 1335, loss = 1.24857854\n",
      "Iteration 1336, loss = 1.24853517\n",
      "Iteration 1337, loss = 1.24849087\n",
      "Iteration 1338, loss = 1.24844843\n",
      "Iteration 1339, loss = 1.24840373\n",
      "Iteration 1340, loss = 1.24835912\n",
      "Iteration 1341, loss = 1.24831583\n",
      "Iteration 1342, loss = 1.24827431\n",
      "Iteration 1343, loss = 1.24822782\n",
      "Iteration 1344, loss = 1.24818586\n",
      "Iteration 1345, loss = 1.24814178\n",
      "Iteration 1346, loss = 1.24809688\n",
      "Iteration 1347, loss = 1.24805334\n",
      "Iteration 1348, loss = 1.24800968\n",
      "Iteration 1349, loss = 1.24796619\n",
      "Iteration 1350, loss = 1.24792376\n",
      "Iteration 1351, loss = 1.24787927\n",
      "Iteration 1352, loss = 1.24783632\n",
      "Iteration 1353, loss = 1.24779361\n",
      "Iteration 1354, loss = 1.24774954\n",
      "Iteration 1355, loss = 1.24770752\n",
      "Iteration 1356, loss = 1.24766420\n",
      "Iteration 1357, loss = 1.24762137\n",
      "Iteration 1358, loss = 1.24757990\n",
      "Iteration 1359, loss = 1.24753498\n",
      "Iteration 1360, loss = 1.24749189\n",
      "Iteration 1361, loss = 1.24744851\n",
      "Iteration 1362, loss = 1.24740580\n",
      "Iteration 1363, loss = 1.24736450\n",
      "Iteration 1364, loss = 1.24732029\n",
      "Iteration 1365, loss = 1.24727897\n",
      "Iteration 1366, loss = 1.24723425\n",
      "Iteration 1367, loss = 1.24719229\n",
      "Iteration 1368, loss = 1.24714891\n",
      "Iteration 1369, loss = 1.24710744\n",
      "Iteration 1370, loss = 1.24706368\n",
      "Iteration 1371, loss = 1.24702231\n",
      "Iteration 1372, loss = 1.24697795\n",
      "Iteration 1373, loss = 1.24693790\n",
      "Iteration 1374, loss = 1.24689425\n",
      "Iteration 1375, loss = 1.24685148\n",
      "Iteration 1376, loss = 1.24681042\n",
      "Iteration 1377, loss = 1.24676676\n",
      "Iteration 1378, loss = 1.24672323\n",
      "Iteration 1379, loss = 1.24668183\n",
      "Iteration 1380, loss = 1.24664138\n",
      "Iteration 1381, loss = 1.24659607\n",
      "Iteration 1382, loss = 1.24655442\n",
      "Iteration 1383, loss = 1.24651205\n",
      "Iteration 1384, loss = 1.24647172\n",
      "Iteration 1385, loss = 1.24642728\n",
      "Iteration 1386, loss = 1.24638559\n",
      "Iteration 1387, loss = 1.24634369\n",
      "Iteration 1388, loss = 1.24630004\n",
      "Iteration 1389, loss = 1.24625745\n",
      "Iteration 1390, loss = 1.24621701\n",
      "Iteration 1391, loss = 1.24617438\n",
      "Iteration 1392, loss = 1.24613159\n",
      "Iteration 1393, loss = 1.24609178\n",
      "Iteration 1394, loss = 1.24604752\n",
      "Iteration 1395, loss = 1.24600663\n",
      "Iteration 1396, loss = 1.24596306\n",
      "Iteration 1397, loss = 1.24592285\n",
      "Iteration 1398, loss = 1.24588026\n",
      "Iteration 1399, loss = 1.24583802\n",
      "Iteration 1400, loss = 1.24579486\n",
      "Iteration 1401, loss = 1.24575419\n",
      "Iteration 1402, loss = 1.24571187\n",
      "Iteration 1403, loss = 1.24567090\n",
      "Iteration 1404, loss = 1.24562905\n",
      "Iteration 1405, loss = 1.24559004\n",
      "Iteration 1406, loss = 1.24554510\n",
      "Iteration 1407, loss = 1.24550402\n",
      "Iteration 1408, loss = 1.24546201\n",
      "Iteration 1409, loss = 1.24542037\n",
      "Iteration 1410, loss = 1.24537919\n",
      "Iteration 1411, loss = 1.24533804\n",
      "Iteration 1412, loss = 1.24529583\n",
      "Iteration 1413, loss = 1.24525462\n",
      "Iteration 1414, loss = 1.24521278\n",
      "Iteration 1415, loss = 1.24517171\n",
      "Iteration 1416, loss = 1.24513016\n",
      "Iteration 1417, loss = 1.24508858\n",
      "Iteration 1418, loss = 1.24504811\n",
      "Iteration 1419, loss = 1.24500689\n",
      "Iteration 1420, loss = 1.24496411\n",
      "Iteration 1421, loss = 1.24492401\n",
      "Iteration 1422, loss = 1.24488162\n",
      "Iteration 1423, loss = 1.24484091\n",
      "Iteration 1424, loss = 1.24479881\n",
      "Iteration 1425, loss = 1.24475794\n",
      "Iteration 1426, loss = 1.24471749\n",
      "Iteration 1427, loss = 1.24467739\n",
      "Iteration 1428, loss = 1.24463930\n",
      "Iteration 1429, loss = 1.24459496\n",
      "Iteration 1430, loss = 1.24455351\n",
      "Iteration 1431, loss = 1.24451231\n",
      "Iteration 1432, loss = 1.24447441\n",
      "Iteration 1433, loss = 1.24443079\n",
      "Iteration 1434, loss = 1.24439043\n",
      "Iteration 1435, loss = 1.24434955\n",
      "Iteration 1436, loss = 1.24430946\n",
      "Iteration 1437, loss = 1.24426805\n",
      "Iteration 1438, loss = 1.24422777\n",
      "Iteration 1439, loss = 1.24418671\n",
      "Iteration 1440, loss = 1.24414585\n",
      "Iteration 1441, loss = 1.24410540\n",
      "Iteration 1442, loss = 1.24406700\n",
      "Iteration 1443, loss = 1.24402479\n",
      "Iteration 1444, loss = 1.24398551\n",
      "Iteration 1445, loss = 1.24394382\n",
      "Iteration 1446, loss = 1.24390535\n",
      "Iteration 1447, loss = 1.24386410\n",
      "Iteration 1448, loss = 1.24382426\n",
      "Iteration 1449, loss = 1.24378217\n",
      "Iteration 1450, loss = 1.24374158\n",
      "Iteration 1451, loss = 1.24370346\n",
      "Iteration 1452, loss = 1.24366161\n",
      "Iteration 1453, loss = 1.24362215\n",
      "Iteration 1454, loss = 1.24358006\n",
      "Iteration 1455, loss = 1.24354068\n",
      "Iteration 1456, loss = 1.24350028\n",
      "Iteration 1457, loss = 1.24346026\n",
      "Iteration 1458, loss = 1.24342045\n",
      "Iteration 1459, loss = 1.24338038\n",
      "Iteration 1460, loss = 1.24334105\n",
      "Iteration 1461, loss = 1.24330112\n",
      "Iteration 1462, loss = 1.24326015\n",
      "Iteration 1463, loss = 1.24322031\n",
      "Iteration 1464, loss = 1.24318098\n",
      "Iteration 1465, loss = 1.24314117\n",
      "Iteration 1466, loss = 1.24310238\n",
      "Iteration 1467, loss = 1.24306325\n",
      "Iteration 1468, loss = 1.24302131\n",
      "Iteration 1469, loss = 1.24298132\n",
      "Iteration 1470, loss = 1.24294175\n",
      "Iteration 1471, loss = 1.24290255\n",
      "Iteration 1472, loss = 1.24286428\n",
      "Iteration 1473, loss = 1.24282320\n",
      "Iteration 1474, loss = 1.24278337\n",
      "Iteration 1475, loss = 1.24274391\n",
      "Iteration 1476, loss = 1.24270410\n",
      "Iteration 1477, loss = 1.24266624\n",
      "Iteration 1478, loss = 1.24262465\n",
      "Iteration 1479, loss = 1.24258544\n",
      "Iteration 1480, loss = 1.24254768\n",
      "Iteration 1481, loss = 1.24250684\n",
      "Iteration 1482, loss = 1.24246745\n",
      "Iteration 1483, loss = 1.24242785\n",
      "Iteration 1484, loss = 1.24238900\n",
      "Iteration 1485, loss = 1.24235003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1486, loss = 1.24230865\n",
      "Iteration 1487, loss = 1.24227013\n",
      "Iteration 1488, loss = 1.24223127\n",
      "Iteration 1489, loss = 1.24219385\n",
      "Iteration 1490, loss = 1.24215294\n",
      "Iteration 1491, loss = 1.24211420\n",
      "Iteration 1492, loss = 1.24207535\n",
      "Iteration 1493, loss = 1.24203768\n",
      "Iteration 1494, loss = 1.24199975\n",
      "Iteration 1495, loss = 1.24195877\n",
      "Iteration 1496, loss = 1.24191968\n",
      "Iteration 1497, loss = 1.24188148\n",
      "Iteration 1498, loss = 1.24184133\n",
      "Iteration 1499, loss = 1.24180260\n",
      "Iteration 1500, loss = 1.24176294\n",
      "Iteration 1501, loss = 1.24172420\n",
      "Iteration 1502, loss = 1.24168510\n",
      "Iteration 1503, loss = 1.24164716\n",
      "Iteration 1504, loss = 1.24160711\n",
      "Iteration 1505, loss = 1.24156826\n",
      "Iteration 1506, loss = 1.24153262\n",
      "Iteration 1507, loss = 1.24149092\n",
      "Iteration 1508, loss = 1.24145360\n",
      "Iteration 1509, loss = 1.24141341\n",
      "Iteration 1510, loss = 1.24137576\n",
      "Iteration 1511, loss = 1.24133678\n",
      "Iteration 1512, loss = 1.24129958\n",
      "Iteration 1513, loss = 1.24125934\n",
      "Iteration 1514, loss = 1.24122144\n",
      "Iteration 1515, loss = 1.24118225\n",
      "Iteration 1516, loss = 1.24114393\n",
      "Iteration 1517, loss = 1.24110614\n",
      "Iteration 1518, loss = 1.24106826\n",
      "Iteration 1519, loss = 1.24102886\n",
      "Iteration 1520, loss = 1.24099078\n",
      "Iteration 1521, loss = 1.24095391\n",
      "Iteration 1522, loss = 1.24091423\n",
      "Iteration 1523, loss = 1.24087503\n",
      "Iteration 1524, loss = 1.24083743\n",
      "Iteration 1525, loss = 1.24079846\n",
      "Iteration 1526, loss = 1.24075985\n",
      "Iteration 1527, loss = 1.24072329\n",
      "Iteration 1528, loss = 1.24068645\n",
      "Iteration 1529, loss = 1.24064522\n",
      "Iteration 1530, loss = 1.24060818\n",
      "Iteration 1531, loss = 1.24056965\n",
      "Iteration 1532, loss = 1.24053172\n",
      "Iteration 1533, loss = 1.24049555\n",
      "Iteration 1534, loss = 1.24045573\n",
      "Iteration 1535, loss = 1.24041863\n",
      "Iteration 1536, loss = 1.24037949\n",
      "Iteration 1537, loss = 1.24034214\n",
      "Iteration 1538, loss = 1.24030404\n",
      "Iteration 1539, loss = 1.24026836\n",
      "Iteration 1540, loss = 1.24022898\n",
      "Iteration 1541, loss = 1.24019112\n",
      "Iteration 1542, loss = 1.24015223\n",
      "Iteration 1543, loss = 1.24011450\n",
      "Iteration 1544, loss = 1.24007667\n",
      "Iteration 1545, loss = 1.24004070\n",
      "Iteration 1546, loss = 1.24000218\n",
      "Iteration 1547, loss = 1.23996372\n",
      "Iteration 1548, loss = 1.23992644\n",
      "Iteration 1549, loss = 1.23988848\n",
      "Iteration 1550, loss = 1.23985102\n",
      "Iteration 1551, loss = 1.23981336\n",
      "Iteration 1552, loss = 1.23977608\n",
      "Iteration 1553, loss = 1.23973874\n",
      "Iteration 1554, loss = 1.23970133\n",
      "Iteration 1555, loss = 1.23966294\n",
      "Iteration 1556, loss = 1.23962678\n",
      "Iteration 1557, loss = 1.23959017\n",
      "Iteration 1558, loss = 1.23955193\n",
      "Iteration 1559, loss = 1.23951449\n",
      "Iteration 1560, loss = 1.23947605\n",
      "Iteration 1561, loss = 1.23943906\n",
      "Iteration 1562, loss = 1.23940213\n",
      "Iteration 1563, loss = 1.23936452\n",
      "Iteration 1564, loss = 1.23932824\n",
      "Iteration 1565, loss = 1.23929069\n",
      "Iteration 1566, loss = 1.23925252\n",
      "Iteration 1567, loss = 1.23921632\n",
      "Iteration 1568, loss = 1.23917936\n",
      "Iteration 1569, loss = 1.23914381\n",
      "Iteration 1570, loss = 1.23910503\n",
      "Iteration 1571, loss = 1.23906721\n",
      "Iteration 1572, loss = 1.23903133\n",
      "Iteration 1573, loss = 1.23899425\n",
      "Iteration 1574, loss = 1.23895700\n",
      "Iteration 1575, loss = 1.23891937\n",
      "Iteration 1576, loss = 1.23888276\n",
      "Iteration 1577, loss = 1.23885122\n",
      "Iteration 1578, loss = 1.23880940\n",
      "Iteration 1579, loss = 1.23877251\n",
      "Iteration 1580, loss = 1.23873708\n",
      "Iteration 1581, loss = 1.23869991\n",
      "Iteration 1582, loss = 1.23866220\n",
      "Iteration 1583, loss = 1.23862586\n",
      "Iteration 1584, loss = 1.23858836\n",
      "Iteration 1585, loss = 1.23855162\n",
      "Iteration 1586, loss = 1.23851568\n",
      "Iteration 1587, loss = 1.23847971\n",
      "Iteration 1588, loss = 1.23844368\n",
      "Iteration 1589, loss = 1.23840763\n",
      "Iteration 1590, loss = 1.23837021\n",
      "Iteration 1591, loss = 1.23833195\n",
      "Iteration 1592, loss = 1.23829871\n",
      "Iteration 1593, loss = 1.23825902\n",
      "Iteration 1594, loss = 1.23822286\n",
      "Iteration 1595, loss = 1.23818716\n",
      "Iteration 1596, loss = 1.23815050\n",
      "Iteration 1597, loss = 1.23811412\n",
      "Iteration 1598, loss = 1.23807734\n",
      "Iteration 1599, loss = 1.23804296\n",
      "Iteration 1600, loss = 1.23800488\n",
      "Iteration 1601, loss = 1.23796908\n",
      "Iteration 1602, loss = 1.23793361\n",
      "Iteration 1603, loss = 1.23789751\n",
      "Iteration 1604, loss = 1.23786191\n",
      "Iteration 1605, loss = 1.23782471\n",
      "Iteration 1606, loss = 1.23778927\n",
      "Iteration 1607, loss = 1.23775234\n",
      "Iteration 1608, loss = 1.23771652\n",
      "Iteration 1609, loss = 1.23768202\n",
      "Iteration 1610, loss = 1.23764453\n",
      "Iteration 1611, loss = 1.23760902\n",
      "Iteration 1612, loss = 1.23757306\n",
      "Iteration 1613, loss = 1.23753762\n",
      "Iteration 1614, loss = 1.23750065\n",
      "Iteration 1615, loss = 1.23746557\n",
      "Iteration 1616, loss = 1.23742859\n",
      "Iteration 1617, loss = 1.23739284\n",
      "Iteration 1618, loss = 1.23735836\n",
      "Iteration 1619, loss = 1.23732345\n",
      "Iteration 1620, loss = 1.23728519\n",
      "Iteration 1621, loss = 1.23725152\n",
      "Iteration 1622, loss = 1.23721410\n",
      "Iteration 1623, loss = 1.23717816\n",
      "Iteration 1624, loss = 1.23714298\n",
      "Iteration 1625, loss = 1.23710655\n",
      "Iteration 1626, loss = 1.23707254\n",
      "Iteration 1627, loss = 1.23703485\n",
      "Iteration 1628, loss = 1.23699948\n",
      "Iteration 1629, loss = 1.23696415\n",
      "Iteration 1630, loss = 1.23692893\n",
      "Iteration 1631, loss = 1.23689445\n",
      "Iteration 1632, loss = 1.23685805\n",
      "Iteration 1633, loss = 1.23682219\n",
      "Iteration 1634, loss = 1.23678807\n",
      "Iteration 1635, loss = 1.23675111\n",
      "Iteration 1636, loss = 1.23671751\n",
      "Iteration 1637, loss = 1.23668063\n",
      "Iteration 1638, loss = 1.23664613\n",
      "Iteration 1639, loss = 1.23661097\n",
      "Iteration 1640, loss = 1.23657610\n",
      "Iteration 1641, loss = 1.23654059\n",
      "Iteration 1642, loss = 1.23650578\n",
      "Iteration 1643, loss = 1.23647008\n",
      "Iteration 1644, loss = 1.23643628\n",
      "Iteration 1645, loss = 1.23639961\n",
      "Iteration 1646, loss = 1.23636511\n",
      "Iteration 1647, loss = 1.23633086\n",
      "Iteration 1648, loss = 1.23629482\n",
      "Iteration 1649, loss = 1.23625950\n",
      "Iteration 1650, loss = 1.23622380\n",
      "Iteration 1651, loss = 1.23618922\n",
      "Iteration 1652, loss = 1.23615498\n",
      "Iteration 1653, loss = 1.23611886\n",
      "Iteration 1654, loss = 1.23608475\n",
      "Iteration 1655, loss = 1.23605314\n",
      "Iteration 1656, loss = 1.23601800\n",
      "Iteration 1657, loss = 1.23598039\n",
      "Iteration 1658, loss = 1.23594497\n",
      "Iteration 1659, loss = 1.23591107\n",
      "Iteration 1660, loss = 1.23587662\n",
      "Iteration 1661, loss = 1.23584089\n",
      "Iteration 1662, loss = 1.23580622\n",
      "Iteration 1663, loss = 1.23577167\n",
      "Iteration 1664, loss = 1.23573628\n",
      "Iteration 1665, loss = 1.23570193\n",
      "Iteration 1666, loss = 1.23566779\n",
      "Iteration 1667, loss = 1.23563248\n",
      "Iteration 1668, loss = 1.23559801\n",
      "Iteration 1669, loss = 1.23556539\n",
      "Iteration 1670, loss = 1.23552987\n",
      "Iteration 1671, loss = 1.23549755\n",
      "Iteration 1672, loss = 1.23546010\n",
      "Iteration 1673, loss = 1.23542564\n",
      "Iteration 1674, loss = 1.23539066\n",
      "Iteration 1675, loss = 1.23535828\n",
      "Iteration 1676, loss = 1.23532189\n",
      "Iteration 1677, loss = 1.23528760\n",
      "Iteration 1678, loss = 1.23525368\n",
      "Iteration 1679, loss = 1.23521981\n",
      "Iteration 1680, loss = 1.23518494\n",
      "Iteration 1681, loss = 1.23515186\n",
      "Iteration 1682, loss = 1.23511694\n",
      "Iteration 1683, loss = 1.23508219\n",
      "Iteration 1684, loss = 1.23504825\n",
      "Iteration 1685, loss = 1.23501331\n",
      "Iteration 1686, loss = 1.23497971\n",
      "Iteration 1687, loss = 1.23494578\n",
      "Iteration 1688, loss = 1.23491169\n",
      "Iteration 1689, loss = 1.23487646\n",
      "Iteration 1690, loss = 1.23484258\n",
      "Iteration 1691, loss = 1.23480868\n",
      "Iteration 1692, loss = 1.23477520\n",
      "Iteration 1693, loss = 1.23474103\n",
      "Iteration 1694, loss = 1.23470741\n",
      "Iteration 1695, loss = 1.23467291\n",
      "Iteration 1696, loss = 1.23464106\n",
      "Iteration 1697, loss = 1.23460547\n",
      "Iteration 1698, loss = 1.23457270\n",
      "Iteration 1699, loss = 1.23453781\n",
      "Iteration 1700, loss = 1.23450289\n",
      "Iteration 1701, loss = 1.23446976\n",
      "Iteration 1702, loss = 1.23443568\n",
      "Iteration 1703, loss = 1.23440166\n",
      "Iteration 1704, loss = 1.23436939\n",
      "Iteration 1705, loss = 1.23433557\n",
      "Iteration 1706, loss = 1.23430070\n",
      "Iteration 1707, loss = 1.23426689\n",
      "Iteration 1708, loss = 1.23423347\n",
      "Iteration 1709, loss = 1.23419921\n",
      "Iteration 1710, loss = 1.23416682\n",
      "Iteration 1711, loss = 1.23413197\n",
      "Iteration 1712, loss = 1.23409986\n",
      "Iteration 1713, loss = 1.23406555\n",
      "Iteration 1714, loss = 1.23403156\n",
      "Iteration 1715, loss = 1.23400055\n",
      "Iteration 1716, loss = 1.23396523\n",
      "Iteration 1717, loss = 1.23393084\n",
      "Iteration 1718, loss = 1.23389758\n",
      "Iteration 1719, loss = 1.23386520\n",
      "Iteration 1720, loss = 1.23383372\n",
      "Iteration 1721, loss = 1.23379896\n",
      "Iteration 1722, loss = 1.23376494\n",
      "Iteration 1723, loss = 1.23373168\n",
      "Iteration 1724, loss = 1.23369832\n",
      "Iteration 1725, loss = 1.23366588\n",
      "Iteration 1726, loss = 1.23363200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1727, loss = 1.23359985\n",
      "Iteration 1728, loss = 1.23356589\n",
      "Iteration 1729, loss = 1.23353238\n",
      "Iteration 1730, loss = 1.23349891\n",
      "Iteration 1731, loss = 1.23346550\n",
      "Iteration 1732, loss = 1.23343268\n",
      "Iteration 1733, loss = 1.23339988\n",
      "Iteration 1734, loss = 1.23336660\n",
      "Iteration 1735, loss = 1.23333470\n",
      "Iteration 1736, loss = 1.23329997\n",
      "Iteration 1737, loss = 1.23326756\n",
      "Iteration 1738, loss = 1.23323526\n",
      "Iteration 1739, loss = 1.23320290\n",
      "Iteration 1740, loss = 1.23316955\n",
      "Iteration 1741, loss = 1.23313813\n",
      "Iteration 1742, loss = 1.23310357\n",
      "Iteration 1743, loss = 1.23307083\n",
      "Iteration 1744, loss = 1.23303821\n",
      "Iteration 1745, loss = 1.23300635\n",
      "Iteration 1746, loss = 1.23297218\n",
      "Iteration 1747, loss = 1.23294031\n",
      "Iteration 1748, loss = 1.23290673\n",
      "Iteration 1749, loss = 1.23287495\n",
      "Iteration 1750, loss = 1.23284180\n",
      "Iteration 1751, loss = 1.23281007\n",
      "Iteration 1752, loss = 1.23277618\n",
      "Iteration 1753, loss = 1.23274500\n",
      "Iteration 1754, loss = 1.23271132\n",
      "Iteration 1755, loss = 1.23267777\n",
      "Iteration 1756, loss = 1.23264786\n",
      "Iteration 1757, loss = 1.23261393\n",
      "Iteration 1758, loss = 1.23258087\n",
      "Iteration 1759, loss = 1.23254881\n",
      "Iteration 1760, loss = 1.23251696\n",
      "Iteration 1761, loss = 1.23248422\n",
      "Iteration 1762, loss = 1.23245075\n",
      "Iteration 1763, loss = 1.23241856\n",
      "Iteration 1764, loss = 1.23238630\n",
      "Iteration 1765, loss = 1.23235574\n",
      "Iteration 1766, loss = 1.23232131\n",
      "Iteration 1767, loss = 1.23228886\n",
      "Iteration 1768, loss = 1.23225539\n",
      "Iteration 1769, loss = 1.23222438\n",
      "Iteration 1770, loss = 1.23219174\n",
      "Iteration 1771, loss = 1.23215862\n",
      "Iteration 1772, loss = 1.23212691\n",
      "Iteration 1773, loss = 1.23209425\n",
      "Iteration 1774, loss = 1.23206240\n",
      "Iteration 1775, loss = 1.23203086\n",
      "Iteration 1776, loss = 1.23199773\n",
      "Iteration 1777, loss = 1.23196570\n",
      "Iteration 1778, loss = 1.23193472\n",
      "Iteration 1779, loss = 1.23190232\n",
      "Iteration 1780, loss = 1.23187161\n",
      "Iteration 1781, loss = 1.23183759\n",
      "Iteration 1782, loss = 1.23180590\n",
      "Iteration 1783, loss = 1.23177271\n",
      "Iteration 1784, loss = 1.23174342\n",
      "Iteration 1785, loss = 1.23170991\n",
      "Iteration 1786, loss = 1.23167744\n",
      "Iteration 1787, loss = 1.23164649\n",
      "Iteration 1788, loss = 1.23161579\n",
      "Iteration 1789, loss = 1.23158284\n",
      "Iteration 1790, loss = 1.23154963\n",
      "Iteration 1791, loss = 1.23151781\n",
      "Iteration 1792, loss = 1.23148637\n",
      "Iteration 1793, loss = 1.23145634\n",
      "Iteration 1794, loss = 1.23142416\n",
      "Iteration 1795, loss = 1.23139195\n",
      "Iteration 1796, loss = 1.23136062\n",
      "Iteration 1797, loss = 1.23132787\n",
      "Iteration 1798, loss = 1.23129580\n",
      "Iteration 1799, loss = 1.23126361\n",
      "Iteration 1800, loss = 1.23123409\n",
      "Iteration 1801, loss = 1.23120228\n",
      "Iteration 1802, loss = 1.23117022\n",
      "Iteration 1803, loss = 1.23113935\n",
      "Iteration 1804, loss = 1.23110637\n",
      "Iteration 1805, loss = 1.23107523\n",
      "Iteration 1806, loss = 1.23104258\n",
      "Iteration 1807, loss = 1.23101444\n",
      "Iteration 1808, loss = 1.23098040\n",
      "Iteration 1809, loss = 1.23094959\n",
      "Iteration 1810, loss = 1.23091731\n",
      "Iteration 1811, loss = 1.23088807\n",
      "Iteration 1812, loss = 1.23085694\n",
      "Iteration 1813, loss = 1.23082228\n",
      "Iteration 1814, loss = 1.23079142\n",
      "Iteration 1815, loss = 1.23076102\n",
      "Iteration 1816, loss = 1.23072926\n",
      "Iteration 1817, loss = 1.23069743\n",
      "Iteration 1818, loss = 1.23066741\n",
      "Iteration 1819, loss = 1.23063667\n",
      "Iteration 1820, loss = 1.23060267\n",
      "Iteration 1821, loss = 1.23057066\n",
      "Iteration 1822, loss = 1.23054246\n",
      "Iteration 1823, loss = 1.23051002\n",
      "Iteration 1824, loss = 1.23047852\n",
      "Iteration 1825, loss = 1.23044763\n",
      "Iteration 1826, loss = 1.23041873\n",
      "Iteration 1827, loss = 1.23038488\n",
      "Iteration 1828, loss = 1.23035335\n",
      "Iteration 1829, loss = 1.23032240\n",
      "Iteration 1830, loss = 1.23029095\n",
      "Iteration 1831, loss = 1.23027074\n",
      "Iteration 1832, loss = 1.23022880\n",
      "Iteration 1833, loss = 1.23019849\n",
      "Iteration 1834, loss = 1.23016807\n",
      "Iteration 1835, loss = 1.23013653\n",
      "Iteration 1836, loss = 1.23010596\n",
      "Iteration 1837, loss = 1.23007593\n",
      "Iteration 1838, loss = 1.23004515\n",
      "Iteration 1839, loss = 1.23001294\n",
      "Iteration 1840, loss = 1.22998102\n",
      "Iteration 1841, loss = 1.22995076\n",
      "Iteration 1842, loss = 1.22992077\n",
      "Iteration 1843, loss = 1.22988879\n",
      "Iteration 1844, loss = 1.22985757\n",
      "Iteration 1845, loss = 1.22982714\n",
      "Iteration 1846, loss = 1.22979620\n",
      "Iteration 1847, loss = 1.22976630\n",
      "Iteration 1848, loss = 1.22973622\n",
      "Iteration 1849, loss = 1.22970512\n",
      "Iteration 1850, loss = 1.22967572\n",
      "Iteration 1851, loss = 1.22964365\n",
      "Iteration 1852, loss = 1.22961389\n",
      "Iteration 1853, loss = 1.22958592\n",
      "Iteration 1854, loss = 1.22955281\n",
      "Iteration 1855, loss = 1.22952185\n",
      "Iteration 1856, loss = 1.22949127\n",
      "Iteration 1857, loss = 1.22945956\n",
      "Iteration 1858, loss = 1.22942912\n",
      "Iteration 1859, loss = 1.22939893\n",
      "Iteration 1860, loss = 1.22936873\n",
      "Iteration 1861, loss = 1.22934012\n",
      "Iteration 1862, loss = 1.22930917\n",
      "Iteration 1863, loss = 1.22927739\n",
      "Iteration 1864, loss = 1.22924658\n",
      "Iteration 1865, loss = 1.22921713\n",
      "Iteration 1866, loss = 1.22918598\n",
      "Iteration 1867, loss = 1.22915662\n",
      "Iteration 1868, loss = 1.22912565\n",
      "Iteration 1869, loss = 1.22909683\n",
      "Iteration 1870, loss = 1.22906434\n",
      "Iteration 1871, loss = 1.22903443\n",
      "Iteration 1872, loss = 1.22900580\n",
      "Iteration 1873, loss = 1.22897471\n",
      "Iteration 1874, loss = 1.22894441\n",
      "Iteration 1875, loss = 1.22891406\n",
      "Iteration 1876, loss = 1.22888336\n",
      "Iteration 1877, loss = 1.22885478\n",
      "Iteration 1878, loss = 1.22882377\n",
      "Iteration 1879, loss = 1.22879351\n",
      "Iteration 1880, loss = 1.22876424\n",
      "Iteration 1881, loss = 1.22873346\n",
      "Iteration 1882, loss = 1.22870377\n",
      "Iteration 1883, loss = 1.22867264\n",
      "Iteration 1884, loss = 1.22864346\n",
      "Iteration 1885, loss = 1.22861376\n",
      "Iteration 1886, loss = 1.22858269\n",
      "Iteration 1887, loss = 1.22855434\n",
      "Iteration 1888, loss = 1.22852350\n",
      "Iteration 1889, loss = 1.22849436\n",
      "Iteration 1890, loss = 1.22846370\n",
      "Iteration 1891, loss = 1.22843359\n",
      "Iteration 1892, loss = 1.22840377\n",
      "Iteration 1893, loss = 1.22837368\n",
      "Iteration 1894, loss = 1.22834438\n",
      "Iteration 1895, loss = 1.22831468\n",
      "Iteration 1896, loss = 1.22828444\n",
      "Iteration 1897, loss = 1.22825421\n",
      "Iteration 1898, loss = 1.22822410\n",
      "Iteration 1899, loss = 1.22819535\n",
      "Iteration 1900, loss = 1.22816630\n",
      "Iteration 1901, loss = 1.22813478\n",
      "Iteration 1902, loss = 1.22810502\n",
      "Iteration 1903, loss = 1.22807564\n",
      "Iteration 1904, loss = 1.22804588\n",
      "Iteration 1905, loss = 1.22801639\n",
      "Iteration 1906, loss = 1.22798610\n",
      "Iteration 1907, loss = 1.22795691\n",
      "Iteration 1908, loss = 1.22792846\n",
      "Iteration 1909, loss = 1.22790150\n",
      "Iteration 1910, loss = 1.22786768\n",
      "Iteration 1911, loss = 1.22783774\n",
      "Iteration 1912, loss = 1.22781048\n",
      "Iteration 1913, loss = 1.22777858\n",
      "Iteration 1914, loss = 1.22774931\n",
      "Iteration 1915, loss = 1.22771958\n",
      "Iteration 1916, loss = 1.22768992\n",
      "Iteration 1917, loss = 1.22766115\n",
      "Iteration 1918, loss = 1.22763478\n",
      "Iteration 1919, loss = 1.22760188\n",
      "Iteration 1920, loss = 1.22757350\n",
      "Iteration 1921, loss = 1.22754321\n",
      "Iteration 1922, loss = 1.22751541\n",
      "Iteration 1923, loss = 1.22748532\n",
      "Iteration 1924, loss = 1.22745643\n",
      "Iteration 1925, loss = 1.22742581\n",
      "Iteration 1926, loss = 1.22739645\n",
      "Iteration 1927, loss = 1.22736673\n",
      "Iteration 1928, loss = 1.22733832\n",
      "Iteration 1929, loss = 1.22730803\n",
      "Iteration 1930, loss = 1.22727925\n",
      "Iteration 1931, loss = 1.22725018\n",
      "Iteration 1932, loss = 1.22722090\n",
      "Iteration 1933, loss = 1.22719370\n",
      "Iteration 1934, loss = 1.22716185\n",
      "Iteration 1935, loss = 1.22713325\n",
      "Iteration 1936, loss = 1.22710395\n",
      "Iteration 1937, loss = 1.22707602\n",
      "Iteration 1938, loss = 1.22704513\n",
      "Iteration 1939, loss = 1.22701602\n",
      "Iteration 1940, loss = 1.22698839\n",
      "Iteration 1941, loss = 1.22695794\n",
      "Iteration 1942, loss = 1.22692900\n",
      "Iteration 1943, loss = 1.22690154\n",
      "Iteration 1944, loss = 1.22687271\n",
      "Iteration 1945, loss = 1.22684375\n",
      "Iteration 1946, loss = 1.22681313\n",
      "Iteration 1947, loss = 1.22678304\n",
      "Iteration 1948, loss = 1.22675524\n",
      "Iteration 1949, loss = 1.22672644\n",
      "Iteration 1950, loss = 1.22669617\n",
      "Iteration 1951, loss = 1.22666862\n",
      "Iteration 1952, loss = 1.22663860\n",
      "Iteration 1953, loss = 1.22660960\n",
      "Iteration 1954, loss = 1.22658065\n",
      "Iteration 1955, loss = 1.22655194\n",
      "Iteration 1956, loss = 1.22652296\n",
      "Iteration 1957, loss = 1.22649494\n",
      "Iteration 1958, loss = 1.22646726\n",
      "Iteration 1959, loss = 1.22643727\n",
      "Iteration 1960, loss = 1.22640949\n",
      "Iteration 1961, loss = 1.22638047\n",
      "Iteration 1962, loss = 1.22635053\n",
      "Iteration 1963, loss = 1.22632146\n",
      "Iteration 1964, loss = 1.22629425\n",
      "Iteration 1965, loss = 1.22626435\n",
      "Iteration 1966, loss = 1.22623598\n",
      "Iteration 1967, loss = 1.22620696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1968, loss = 1.22617808\n",
      "Iteration 1969, loss = 1.22615007\n",
      "Iteration 1970, loss = 1.22612096\n",
      "Iteration 1971, loss = 1.22609252\n",
      "Iteration 1972, loss = 1.22606371\n",
      "Iteration 1973, loss = 1.22603717\n",
      "Iteration 1974, loss = 1.22600788\n",
      "Iteration 1975, loss = 1.22597753\n",
      "Iteration 1976, loss = 1.22595113\n",
      "Iteration 1977, loss = 1.22592109\n",
      "Iteration 1978, loss = 1.22589178\n",
      "Iteration 1979, loss = 1.22586414\n",
      "Iteration 1980, loss = 1.22583539\n",
      "Iteration 1981, loss = 1.22580709\n",
      "Iteration 1982, loss = 1.22577822\n",
      "Iteration 1983, loss = 1.22575194\n",
      "Iteration 1984, loss = 1.22572150\n",
      "Iteration 1985, loss = 1.22569576\n",
      "Iteration 1986, loss = 1.22566511\n",
      "Iteration 1987, loss = 1.22563709\n",
      "Iteration 1988, loss = 1.22560987\n",
      "Iteration 1989, loss = 1.22558028\n",
      "Iteration 1990, loss = 1.22555339\n",
      "Iteration 1991, loss = 1.22552353\n",
      "Iteration 1992, loss = 1.22549512\n",
      "Iteration 1993, loss = 1.22546665\n",
      "Iteration 1994, loss = 1.22543811\n",
      "Iteration 1995, loss = 1.22541033\n",
      "Iteration 1996, loss = 1.22538245\n",
      "Iteration 1997, loss = 1.22535488\n",
      "Iteration 1998, loss = 1.22532679\n",
      "Iteration 1999, loss = 1.22529872\n",
      "Iteration 2000, loss = 1.22526892\n",
      "Iteration 2001, loss = 1.22524064\n",
      "Iteration 2002, loss = 1.22521257\n",
      "Iteration 2003, loss = 1.22518554\n",
      "Iteration 2004, loss = 1.22515667\n",
      "Iteration 2005, loss = 1.22512939\n",
      "Iteration 2006, loss = 1.22510013\n",
      "Iteration 2007, loss = 1.22507223\n",
      "Iteration 2008, loss = 1.22504378\n",
      "Iteration 2009, loss = 1.22501879\n",
      "Iteration 2010, loss = 1.22498911\n",
      "Iteration 2011, loss = 1.22496124\n",
      "Iteration 2012, loss = 1.22493311\n",
      "Iteration 2013, loss = 1.22490361\n",
      "Iteration 2014, loss = 1.22487703\n",
      "Iteration 2015, loss = 1.22484713\n",
      "Iteration 2016, loss = 1.22482155\n",
      "Iteration 2017, loss = 1.22479533\n",
      "Iteration 2018, loss = 1.22476354\n",
      "Iteration 2019, loss = 1.22473555\n",
      "Iteration 2020, loss = 1.22470812\n",
      "Iteration 2021, loss = 1.22468091\n",
      "Iteration 2022, loss = 1.22465352\n",
      "Iteration 2023, loss = 1.22462437\n",
      "Iteration 2024, loss = 1.22459734\n",
      "Iteration 2025, loss = 1.22456893\n",
      "Iteration 2026, loss = 1.22454164\n",
      "Iteration 2027, loss = 1.22451457\n",
      "Iteration 2028, loss = 1.22448605\n",
      "Iteration 2029, loss = 1.22445794\n",
      "Iteration 2030, loss = 1.22443173\n",
      "Iteration 2031, loss = 1.22440286\n",
      "Iteration 2032, loss = 1.22437508\n",
      "Iteration 2033, loss = 1.22434662\n",
      "Iteration 2034, loss = 1.22431914\n",
      "Iteration 2035, loss = 1.22429186\n",
      "Iteration 2036, loss = 1.22426459\n",
      "Iteration 2037, loss = 1.22423728\n",
      "Iteration 2038, loss = 1.22420925\n",
      "Iteration 2039, loss = 1.22418193\n",
      "Iteration 2040, loss = 1.22415471\n",
      "Iteration 2041, loss = 1.22412671\n",
      "Iteration 2042, loss = 1.22409999\n",
      "Iteration 2043, loss = 1.22407169\n",
      "Iteration 2044, loss = 1.22404584\n",
      "Iteration 2045, loss = 1.22402152\n",
      "Iteration 2046, loss = 1.22398951\n",
      "Iteration 2047, loss = 1.22396517\n",
      "Iteration 2048, loss = 1.22393591\n",
      "Iteration 2049, loss = 1.22390831\n",
      "Iteration 2050, loss = 1.22388155\n",
      "Iteration 2051, loss = 1.22385379\n",
      "Iteration 2052, loss = 1.22382655\n",
      "Iteration 2053, loss = 1.22379845\n",
      "Iteration 2054, loss = 1.22377082\n",
      "Iteration 2055, loss = 1.22374329\n",
      "Iteration 2056, loss = 1.22371574\n",
      "Iteration 2057, loss = 1.22368903\n",
      "Iteration 2058, loss = 1.22366205\n",
      "Iteration 2059, loss = 1.22363474\n",
      "Iteration 2060, loss = 1.22360685\n",
      "Iteration 2061, loss = 1.22358125\n",
      "Iteration 2062, loss = 1.22355339\n",
      "Iteration 2063, loss = 1.22352565\n",
      "Iteration 2064, loss = 1.22349897\n",
      "Iteration 2065, loss = 1.22347173\n",
      "Iteration 2066, loss = 1.22344383\n",
      "Iteration 2067, loss = 1.22341685\n",
      "Iteration 2068, loss = 1.22339157\n",
      "Iteration 2069, loss = 1.22336287\n",
      "Iteration 2070, loss = 1.22333503\n",
      "Iteration 2071, loss = 1.22330712\n",
      "Iteration 2072, loss = 1.22328103\n",
      "Iteration 2073, loss = 1.22325444\n",
      "Iteration 2074, loss = 1.22322799\n",
      "Iteration 2075, loss = 1.22320062\n",
      "Iteration 2076, loss = 1.22317324\n",
      "Iteration 2077, loss = 1.22314480\n",
      "Iteration 2078, loss = 1.22311824\n",
      "Iteration 2079, loss = 1.22309104\n",
      "Iteration 2080, loss = 1.22306575\n",
      "Iteration 2081, loss = 1.22303973\n",
      "Iteration 2082, loss = 1.22301181\n",
      "Iteration 2083, loss = 1.22298315\n",
      "Iteration 2084, loss = 1.22295684\n",
      "Iteration 2085, loss = 1.22292937\n",
      "Iteration 2086, loss = 1.22290285\n",
      "Iteration 2087, loss = 1.22287689\n",
      "Iteration 2088, loss = 1.22284922\n",
      "Iteration 2089, loss = 1.22282109\n",
      "Iteration 2090, loss = 1.22279584\n",
      "Iteration 2091, loss = 1.22276786\n",
      "Iteration 2092, loss = 1.22274085\n",
      "Iteration 2093, loss = 1.22271619\n",
      "Iteration 2094, loss = 1.22269043\n",
      "Iteration 2095, loss = 1.22266072\n",
      "Iteration 2096, loss = 1.22263522\n",
      "Iteration 2097, loss = 1.22260684\n",
      "Iteration 2098, loss = 1.22258342\n",
      "Iteration 2099, loss = 1.22255440\n",
      "Iteration 2100, loss = 1.22252723\n",
      "Iteration 2101, loss = 1.22250033\n",
      "Iteration 2102, loss = 1.22247570\n",
      "Iteration 2103, loss = 1.22244706\n",
      "Iteration 2104, loss = 1.22242089\n",
      "Iteration 2105, loss = 1.22239598\n",
      "Iteration 2106, loss = 1.22236827\n",
      "Iteration 2107, loss = 1.22234146\n",
      "Iteration 2108, loss = 1.22231437\n",
      "Iteration 2109, loss = 1.22228682\n",
      "Iteration 2110, loss = 1.22226175\n",
      "Iteration 2111, loss = 1.22223365\n",
      "Iteration 2112, loss = 1.22221051\n",
      "Iteration 2113, loss = 1.22218117\n",
      "Iteration 2114, loss = 1.22215669\n",
      "Iteration 2115, loss = 1.22212922\n",
      "Iteration 2116, loss = 1.22210210\n",
      "Iteration 2117, loss = 1.22208039\n",
      "Iteration 2118, loss = 1.22204846\n",
      "Iteration 2119, loss = 1.22202418\n",
      "Iteration 2120, loss = 1.22199816\n",
      "Iteration 2121, loss = 1.22197069\n",
      "Iteration 2122, loss = 1.22194496\n",
      "Iteration 2123, loss = 1.22191741\n",
      "Iteration 2124, loss = 1.22189279\n",
      "Iteration 2125, loss = 1.22186589\n",
      "Iteration 2126, loss = 1.22183787\n",
      "Iteration 2127, loss = 1.22181327\n",
      "Iteration 2128, loss = 1.22178543\n",
      "Iteration 2129, loss = 1.22175994\n",
      "Iteration 2130, loss = 1.22173179\n",
      "Iteration 2131, loss = 1.22170640\n",
      "Iteration 2132, loss = 1.22167891\n",
      "Iteration 2133, loss = 1.22165357\n",
      "Iteration 2134, loss = 1.22162720\n",
      "Iteration 2135, loss = 1.22160177\n",
      "Iteration 2136, loss = 1.22157498\n",
      "Iteration 2137, loss = 1.22154916\n",
      "Iteration 2138, loss = 1.22152200\n",
      "Iteration 2139, loss = 1.22149628\n",
      "Iteration 2140, loss = 1.22147023\n",
      "Iteration 2141, loss = 1.22144262\n",
      "Iteration 2142, loss = 1.22141857\n",
      "Iteration 2143, loss = 1.22139166\n",
      "Iteration 2144, loss = 1.22136926\n",
      "Iteration 2145, loss = 1.22133939\n",
      "Iteration 2146, loss = 1.22131494\n",
      "Iteration 2147, loss = 1.22128865\n",
      "Iteration 2148, loss = 1.22126269\n",
      "Iteration 2149, loss = 1.22123539\n",
      "Iteration 2150, loss = 1.22120898\n",
      "Iteration 2151, loss = 1.22118550\n",
      "Iteration 2152, loss = 1.22115684\n",
      "Iteration 2153, loss = 1.22113006\n",
      "Iteration 2154, loss = 1.22110562\n",
      "Iteration 2155, loss = 1.22107943\n",
      "Iteration 2156, loss = 1.22105256\n",
      "Iteration 2157, loss = 1.22102647\n",
      "Iteration 2158, loss = 1.22100171\n",
      "Iteration 2159, loss = 1.22097531\n",
      "Iteration 2160, loss = 1.22095025\n",
      "Iteration 2161, loss = 1.22092399\n",
      "Iteration 2162, loss = 1.22089876\n",
      "Iteration 2163, loss = 1.22087128\n",
      "Iteration 2164, loss = 1.22084568\n",
      "Iteration 2165, loss = 1.22082139\n",
      "Iteration 2166, loss = 1.22079326\n",
      "Iteration 2167, loss = 1.22076804\n",
      "Iteration 2168, loss = 1.22074230\n",
      "Iteration 2169, loss = 1.22071604\n",
      "Iteration 2170, loss = 1.22069156\n",
      "Iteration 2171, loss = 1.22066750\n",
      "Iteration 2172, loss = 1.22063889\n",
      "Iteration 2173, loss = 1.22061337\n",
      "Iteration 2174, loss = 1.22058693\n",
      "Iteration 2175, loss = 1.22056074\n",
      "Iteration 2176, loss = 1.22053580\n",
      "Iteration 2177, loss = 1.22051065\n",
      "Iteration 2178, loss = 1.22048537\n",
      "Iteration 2179, loss = 1.22045794\n",
      "Iteration 2180, loss = 1.22043384\n",
      "Iteration 2181, loss = 1.22040702\n",
      "Iteration 2182, loss = 1.22038152\n",
      "Iteration 2183, loss = 1.22035742\n",
      "Iteration 2184, loss = 1.22033134\n",
      "Iteration 2185, loss = 1.22030570\n",
      "Iteration 2186, loss = 1.22028241\n",
      "Iteration 2187, loss = 1.22025409\n",
      "Iteration 2188, loss = 1.22023047\n",
      "Iteration 2189, loss = 1.22020407\n",
      "Iteration 2190, loss = 1.22017831\n",
      "Iteration 2191, loss = 1.22015131\n",
      "Iteration 2192, loss = 1.22012695\n",
      "Iteration 2193, loss = 1.22010077\n",
      "Iteration 2194, loss = 1.22007407\n",
      "Iteration 2195, loss = 1.22005053\n",
      "Iteration 2196, loss = 1.22002436\n",
      "Iteration 2197, loss = 1.21999890\n",
      "Iteration 2198, loss = 1.21997241\n",
      "Iteration 2199, loss = 1.21994714\n",
      "Iteration 2200, loss = 1.21992196\n",
      "Iteration 2201, loss = 1.21989554\n",
      "Iteration 2202, loss = 1.21986932\n",
      "Iteration 2203, loss = 1.21984764\n",
      "Iteration 2204, loss = 1.21981932\n",
      "Iteration 2205, loss = 1.21979432\n",
      "Iteration 2206, loss = 1.21976869\n",
      "Iteration 2207, loss = 1.21974314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2208, loss = 1.21971968\n",
      "Iteration 2209, loss = 1.21969200\n",
      "Iteration 2210, loss = 1.21966672\n",
      "Iteration 2211, loss = 1.21964191\n",
      "Iteration 2212, loss = 1.21961627\n",
      "Iteration 2213, loss = 1.21959147\n",
      "Iteration 2214, loss = 1.21956663\n",
      "Iteration 2215, loss = 1.21954220\n",
      "Iteration 2216, loss = 1.21951583\n",
      "Iteration 2217, loss = 1.21949028\n",
      "Iteration 2218, loss = 1.21946410\n",
      "Iteration 2219, loss = 1.21943992\n",
      "Iteration 2220, loss = 1.21941481\n",
      "Iteration 2221, loss = 1.21938835\n",
      "Iteration 2222, loss = 1.21936291\n",
      "Iteration 2223, loss = 1.21933960\n",
      "Iteration 2224, loss = 1.21931322\n",
      "Iteration 2225, loss = 1.21928704\n",
      "Iteration 2226, loss = 1.21926446\n",
      "Iteration 2227, loss = 1.21923802\n",
      "Iteration 2228, loss = 1.21921568\n",
      "Iteration 2229, loss = 1.21918797\n",
      "Iteration 2230, loss = 1.21916243\n",
      "Iteration 2231, loss = 1.21913771\n",
      "Iteration 2232, loss = 1.21911367\n",
      "Iteration 2233, loss = 1.21908956\n",
      "Iteration 2234, loss = 1.21906181\n",
      "Iteration 2235, loss = 1.21903875\n",
      "Iteration 2236, loss = 1.21901157\n",
      "Iteration 2237, loss = 1.21898785\n",
      "Iteration 2238, loss = 1.21896139\n",
      "Iteration 2239, loss = 1.21893687\n",
      "Iteration 2240, loss = 1.21891225\n",
      "Iteration 2241, loss = 1.21888800\n",
      "Iteration 2242, loss = 1.21886381\n",
      "Iteration 2243, loss = 1.21883710\n",
      "Iteration 2244, loss = 1.21881189\n",
      "Iteration 2245, loss = 1.21878749\n",
      "Iteration 2246, loss = 1.21876217\n",
      "Iteration 2247, loss = 1.21873728\n",
      "Iteration 2248, loss = 1.21871245\n",
      "Iteration 2249, loss = 1.21868861\n",
      "Iteration 2250, loss = 1.21866263\n",
      "Iteration 2251, loss = 1.21863803\n",
      "Iteration 2252, loss = 1.21861377\n",
      "Iteration 2253, loss = 1.21858773\n",
      "Iteration 2254, loss = 1.21856234\n",
      "Iteration 2255, loss = 1.21853778\n",
      "Iteration 2256, loss = 1.21851295\n",
      "Iteration 2257, loss = 1.21848796\n",
      "Iteration 2258, loss = 1.21846414\n",
      "Iteration 2259, loss = 1.21844054\n",
      "Iteration 2260, loss = 1.21841448\n",
      "Iteration 2261, loss = 1.21839088\n",
      "Iteration 2262, loss = 1.21836482\n",
      "Iteration 2263, loss = 1.21833927\n",
      "Iteration 2264, loss = 1.21831576\n",
      "Iteration 2265, loss = 1.21829011\n",
      "Iteration 2266, loss = 1.21826617\n",
      "Iteration 2267, loss = 1.21824184\n",
      "Iteration 2268, loss = 1.21822190\n",
      "Iteration 2269, loss = 1.21819120\n",
      "Iteration 2270, loss = 1.21816732\n",
      "Iteration 2271, loss = 1.21814127\n",
      "Iteration 2272, loss = 1.21811798\n",
      "Iteration 2273, loss = 1.21809355\n",
      "Iteration 2274, loss = 1.21806893\n",
      "Iteration 2275, loss = 1.21804492\n",
      "Iteration 2276, loss = 1.21801902\n",
      "Iteration 2277, loss = 1.21799483\n",
      "Iteration 2278, loss = 1.21797139\n",
      "Iteration 2279, loss = 1.21794515\n",
      "Iteration 2280, loss = 1.21792296\n",
      "Iteration 2281, loss = 1.21789715\n",
      "Iteration 2282, loss = 1.21787422\n",
      "Iteration 2283, loss = 1.21785111\n",
      "Iteration 2284, loss = 1.21782473\n",
      "Iteration 2285, loss = 1.21780507\n",
      "Iteration 2286, loss = 1.21777503\n",
      "Iteration 2287, loss = 1.21775137\n",
      "Iteration 2288, loss = 1.21772673\n",
      "Iteration 2289, loss = 1.21770142\n",
      "Iteration 2290, loss = 1.21767720\n",
      "Iteration 2291, loss = 1.21765291\n",
      "Iteration 2292, loss = 1.21763585\n",
      "Iteration 2293, loss = 1.21760445\n",
      "Iteration 2294, loss = 1.21758074\n",
      "Iteration 2295, loss = 1.21755464\n",
      "Iteration 2296, loss = 1.21752996\n",
      "Iteration 2297, loss = 1.21750592\n",
      "Iteration 2298, loss = 1.21748226\n",
      "Iteration 2299, loss = 1.21745805\n",
      "Iteration 2300, loss = 1.21743311\n",
      "Iteration 2301, loss = 1.21740831\n",
      "Iteration 2302, loss = 1.21738448\n",
      "Iteration 2303, loss = 1.21736022\n",
      "Iteration 2304, loss = 1.21733594\n",
      "Iteration 2305, loss = 1.21731087\n",
      "Iteration 2306, loss = 1.21728757\n",
      "Iteration 2307, loss = 1.21726460\n",
      "Iteration 2308, loss = 1.21724028\n",
      "Iteration 2309, loss = 1.21721415\n",
      "Iteration 2310, loss = 1.21719295\n",
      "Iteration 2311, loss = 1.21716628\n",
      "Iteration 2312, loss = 1.21714233\n",
      "Iteration 2313, loss = 1.21711860\n",
      "Iteration 2314, loss = 1.21709554\n",
      "Iteration 2315, loss = 1.21707029\n",
      "Iteration 2316, loss = 1.21704480\n",
      "Iteration 2317, loss = 1.21702154\n",
      "Iteration 2318, loss = 1.21699871\n",
      "Iteration 2319, loss = 1.21697584\n",
      "Iteration 2320, loss = 1.21694942\n",
      "Iteration 2321, loss = 1.21692532\n",
      "Iteration 2322, loss = 1.21690024\n",
      "Iteration 2323, loss = 1.21687759\n",
      "Iteration 2324, loss = 1.21685210\n",
      "Iteration 2325, loss = 1.21682871\n",
      "Iteration 2326, loss = 1.21680498\n",
      "Iteration 2327, loss = 1.21678075\n",
      "Iteration 2328, loss = 1.21675958\n",
      "Iteration 2329, loss = 1.21673324\n",
      "Iteration 2330, loss = 1.21670870\n",
      "Iteration 2331, loss = 1.21668457\n",
      "Iteration 2332, loss = 1.21666116\n",
      "Iteration 2333, loss = 1.21663766\n",
      "Iteration 2334, loss = 1.21661292\n",
      "Iteration 2335, loss = 1.21658993\n",
      "Iteration 2336, loss = 1.21656672\n",
      "Iteration 2337, loss = 1.21654270\n",
      "Iteration 2338, loss = 1.21651854\n",
      "Iteration 2339, loss = 1.21649558\n",
      "Iteration 2340, loss = 1.21647132\n",
      "Iteration 2341, loss = 1.21644840\n",
      "Iteration 2342, loss = 1.21642229\n",
      "Iteration 2343, loss = 1.21639938\n",
      "Iteration 2344, loss = 1.21637581\n",
      "Iteration 2345, loss = 1.21635461\n",
      "Iteration 2346, loss = 1.21632642\n",
      "Iteration 2347, loss = 1.21630522\n",
      "Iteration 2348, loss = 1.21627860\n",
      "Iteration 2349, loss = 1.21625476\n",
      "Iteration 2350, loss = 1.21623301\n",
      "Iteration 2351, loss = 1.21620762\n",
      "Iteration 2352, loss = 1.21618938\n",
      "Iteration 2353, loss = 1.21616171\n",
      "Iteration 2354, loss = 1.21613587\n",
      "Iteration 2355, loss = 1.21611220\n",
      "Iteration 2356, loss = 1.21609186\n",
      "Iteration 2357, loss = 1.21606574\n",
      "Iteration 2358, loss = 1.21604170\n",
      "Iteration 2359, loss = 1.21601828\n",
      "Iteration 2360, loss = 1.21599644\n",
      "Iteration 2361, loss = 1.21597181\n",
      "Iteration 2362, loss = 1.21594746\n",
      "Iteration 2363, loss = 1.21592410\n",
      "Iteration 2364, loss = 1.21589996\n",
      "Iteration 2365, loss = 1.21587767\n",
      "Iteration 2366, loss = 1.21585321\n",
      "Iteration 2367, loss = 1.21582977\n",
      "Iteration 2368, loss = 1.21580742\n",
      "Iteration 2369, loss = 1.21578431\n",
      "Iteration 2370, loss = 1.21575942\n",
      "Iteration 2371, loss = 1.21573570\n",
      "Iteration 2372, loss = 1.21571184\n",
      "Iteration 2373, loss = 1.21568842\n",
      "Iteration 2374, loss = 1.21566456\n",
      "Iteration 2375, loss = 1.21564135\n",
      "Iteration 2376, loss = 1.21561916\n",
      "Iteration 2377, loss = 1.21559586\n",
      "Iteration 2378, loss = 1.21557137\n",
      "Iteration 2379, loss = 1.21554781\n",
      "Iteration 2380, loss = 1.21552494\n",
      "Iteration 2381, loss = 1.21550342\n",
      "Iteration 2382, loss = 1.21547770\n",
      "Iteration 2383, loss = 1.21545429\n",
      "Iteration 2384, loss = 1.21543177\n",
      "Iteration 2385, loss = 1.21540799\n",
      "Iteration 2386, loss = 1.21538489\n",
      "Iteration 2387, loss = 1.21536088\n",
      "Iteration 2388, loss = 1.21533932\n",
      "Iteration 2389, loss = 1.21531454\n",
      "Iteration 2390, loss = 1.21529171\n",
      "Iteration 2391, loss = 1.21526749\n",
      "Iteration 2392, loss = 1.21524595\n",
      "Iteration 2393, loss = 1.21522085\n",
      "Iteration 2394, loss = 1.21519799\n",
      "Iteration 2395, loss = 1.21517477\n",
      "Iteration 2396, loss = 1.21515168\n",
      "Iteration 2397, loss = 1.21512850\n",
      "Iteration 2398, loss = 1.21510527\n",
      "Iteration 2399, loss = 1.21508120\n",
      "Iteration 2400, loss = 1.21505821\n",
      "Iteration 2401, loss = 1.21503521\n",
      "Iteration 2402, loss = 1.21501340\n",
      "Iteration 2403, loss = 1.21498763\n",
      "Iteration 2404, loss = 1.21496523\n",
      "Iteration 2405, loss = 1.21494253\n",
      "Iteration 2406, loss = 1.21491904\n",
      "Iteration 2407, loss = 1.21489694\n",
      "Iteration 2408, loss = 1.21487260\n",
      "Iteration 2409, loss = 1.21485008\n",
      "Iteration 2410, loss = 1.21483094\n",
      "Iteration 2411, loss = 1.21480406\n",
      "Iteration 2412, loss = 1.21478170\n",
      "Iteration 2413, loss = 1.21475739\n",
      "Iteration 2414, loss = 1.21473517\n",
      "Iteration 2415, loss = 1.21471168\n",
      "Iteration 2416, loss = 1.21469006\n",
      "Iteration 2417, loss = 1.21466464\n",
      "Iteration 2418, loss = 1.21464141\n",
      "Iteration 2419, loss = 1.21461992\n",
      "Iteration 2420, loss = 1.21459701\n",
      "Iteration 2421, loss = 1.21457821\n",
      "Iteration 2422, loss = 1.21454975\n",
      "Iteration 2423, loss = 1.21452649\n",
      "Iteration 2424, loss = 1.21450411\n",
      "Iteration 2425, loss = 1.21448371\n",
      "Iteration 2426, loss = 1.21445931\n",
      "Iteration 2427, loss = 1.21443723\n",
      "Iteration 2428, loss = 1.21441253\n",
      "Iteration 2429, loss = 1.21438984\n",
      "Iteration 2430, loss = 1.21436696\n",
      "Iteration 2431, loss = 1.21434550\n",
      "Iteration 2432, loss = 1.21432074\n",
      "Iteration 2433, loss = 1.21429789\n",
      "Iteration 2434, loss = 1.21427630\n",
      "Iteration 2435, loss = 1.21425295\n",
      "Iteration 2436, loss = 1.21422976\n",
      "Iteration 2437, loss = 1.21420706\n",
      "Iteration 2438, loss = 1.21418629\n",
      "Iteration 2439, loss = 1.21416090\n",
      "Iteration 2440, loss = 1.21413797\n",
      "Iteration 2441, loss = 1.21411549\n",
      "Iteration 2442, loss = 1.21409270\n",
      "Iteration 2443, loss = 1.21407051\n",
      "Iteration 2444, loss = 1.21404832\n",
      "Iteration 2445, loss = 1.21402322\n",
      "Iteration 2446, loss = 1.21400339\n",
      "Iteration 2447, loss = 1.21397897\n",
      "Iteration 2448, loss = 1.21395623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2449, loss = 1.21393253\n",
      "Iteration 2450, loss = 1.21391002\n",
      "Iteration 2451, loss = 1.21388835\n",
      "Iteration 2452, loss = 1.21386513\n",
      "Iteration 2453, loss = 1.21384314\n",
      "Iteration 2454, loss = 1.21382213\n",
      "Iteration 2455, loss = 1.21379653\n",
      "Iteration 2456, loss = 1.21377580\n",
      "Iteration 2457, loss = 1.21375216\n",
      "Iteration 2458, loss = 1.21373047\n",
      "Iteration 2459, loss = 1.21370873\n",
      "Iteration 2460, loss = 1.21368482\n",
      "Iteration 2461, loss = 1.21366373\n",
      "Iteration 2462, loss = 1.21364081\n",
      "Iteration 2463, loss = 1.21361740\n",
      "Iteration 2464, loss = 1.21359543\n",
      "Iteration 2465, loss = 1.21357141\n",
      "Iteration 2466, loss = 1.21354954\n",
      "Iteration 2467, loss = 1.21352846\n",
      "Iteration 2468, loss = 1.21350422\n",
      "Iteration 2469, loss = 1.21348012\n",
      "Iteration 2470, loss = 1.21345890\n",
      "Iteration 2471, loss = 1.21343715\n",
      "Iteration 2472, loss = 1.21341291\n",
      "Iteration 2473, loss = 1.21339116\n",
      "Iteration 2474, loss = 1.21336837\n",
      "Iteration 2475, loss = 1.21334663\n",
      "Iteration 2476, loss = 1.21332366\n",
      "Iteration 2477, loss = 1.21330080\n",
      "Iteration 2478, loss = 1.21327844\n",
      "Iteration 2479, loss = 1.21325709\n",
      "Iteration 2480, loss = 1.21323552\n",
      "Iteration 2481, loss = 1.21321425\n",
      "Iteration 2482, loss = 1.21318959\n",
      "Iteration 2483, loss = 1.21316742\n",
      "Iteration 2484, loss = 1.21314419\n",
      "Iteration 2485, loss = 1.21312189\n",
      "Iteration 2486, loss = 1.21310011\n",
      "Iteration 2487, loss = 1.21307856\n",
      "Iteration 2488, loss = 1.21305539\n",
      "Iteration 2489, loss = 1.21303345\n",
      "Iteration 2490, loss = 1.21301030\n",
      "Iteration 2491, loss = 1.21299037\n",
      "Iteration 2492, loss = 1.21296669\n",
      "Iteration 2493, loss = 1.21294411\n",
      "Iteration 2494, loss = 1.21292237\n",
      "Iteration 2495, loss = 1.21290074\n",
      "Iteration 2496, loss = 1.21287741\n",
      "Iteration 2497, loss = 1.21285556\n",
      "Iteration 2498, loss = 1.21283404\n",
      "Iteration 2499, loss = 1.21281178\n",
      "Iteration 2500, loss = 1.21278915\n",
      "Iteration 2501, loss = 1.21276743\n",
      "Iteration 2502, loss = 1.21274515\n",
      "Iteration 2503, loss = 1.21272919\n",
      "Iteration 2504, loss = 1.21270053\n",
      "Iteration 2505, loss = 1.21267847\n",
      "Iteration 2506, loss = 1.21265604\n",
      "Iteration 2507, loss = 1.21263517\n",
      "Iteration 2508, loss = 1.21261289\n",
      "Iteration 2509, loss = 1.21259819\n",
      "Iteration 2510, loss = 1.21257163\n",
      "Iteration 2511, loss = 1.21254764\n",
      "Iteration 2512, loss = 1.21252323\n",
      "Iteration 2513, loss = 1.21250310\n",
      "Iteration 2514, loss = 1.21248014\n",
      "Iteration 2515, loss = 1.21245737\n",
      "Iteration 2516, loss = 1.21243683\n",
      "Iteration 2517, loss = 1.21241443\n",
      "Iteration 2518, loss = 1.21239135\n",
      "Iteration 2519, loss = 1.21236912\n",
      "Iteration 2520, loss = 1.21234665\n",
      "Iteration 2521, loss = 1.21232568\n",
      "Iteration 2522, loss = 1.21230397\n",
      "Iteration 2523, loss = 1.21228263\n",
      "Iteration 2524, loss = 1.21226348\n",
      "Iteration 2525, loss = 1.21223736\n",
      "Iteration 2526, loss = 1.21221840\n",
      "Iteration 2527, loss = 1.21219374\n",
      "Iteration 2528, loss = 1.21217228\n",
      "Iteration 2529, loss = 1.21214969\n",
      "Iteration 2530, loss = 1.21213028\n",
      "Iteration 2531, loss = 1.21210881\n",
      "Iteration 2532, loss = 1.21208500\n",
      "Iteration 2533, loss = 1.21206455\n",
      "Iteration 2534, loss = 1.21204086\n",
      "Iteration 2535, loss = 1.21202107\n",
      "Iteration 2536, loss = 1.21199596\n",
      "Iteration 2537, loss = 1.21197513\n",
      "Iteration 2538, loss = 1.21195488\n",
      "Iteration 2539, loss = 1.21193158\n",
      "Iteration 2540, loss = 1.21190989\n",
      "Iteration 2541, loss = 1.21188956\n",
      "Iteration 2542, loss = 1.21186681\n",
      "Iteration 2543, loss = 1.21184399\n",
      "Iteration 2544, loss = 1.21182289\n",
      "Iteration 2545, loss = 1.21180317\n",
      "Iteration 2546, loss = 1.21177991\n",
      "Iteration 2547, loss = 1.21175904\n",
      "Iteration 2548, loss = 1.21173555\n",
      "Iteration 2549, loss = 1.21171449\n",
      "Iteration 2550, loss = 1.21169089\n",
      "Iteration 2551, loss = 1.21167072\n",
      "Iteration 2552, loss = 1.21164966\n",
      "Iteration 2553, loss = 1.21163009\n",
      "Iteration 2554, loss = 1.21160503\n",
      "Iteration 2555, loss = 1.21158421\n",
      "Iteration 2556, loss = 1.21156172\n",
      "Iteration 2557, loss = 1.21154035\n",
      "Iteration 2558, loss = 1.21151921\n",
      "Iteration 2559, loss = 1.21149809\n",
      "Iteration 2560, loss = 1.21147688\n",
      "Iteration 2561, loss = 1.21145438\n",
      "Iteration 2562, loss = 1.21143189\n",
      "Iteration 2563, loss = 1.21141077\n",
      "Iteration 2564, loss = 1.21139202\n",
      "Iteration 2565, loss = 1.21136618\n",
      "Iteration 2566, loss = 1.21134482\n",
      "Iteration 2567, loss = 1.21132286\n",
      "Iteration 2568, loss = 1.21130196\n",
      "Iteration 2569, loss = 1.21128037\n",
      "Iteration 2570, loss = 1.21125955\n",
      "Iteration 2571, loss = 1.21123906\n",
      "Iteration 2572, loss = 1.21121779\n",
      "Iteration 2573, loss = 1.21119612\n",
      "Iteration 2574, loss = 1.21117304\n",
      "Iteration 2575, loss = 1.21115333\n",
      "Iteration 2576, loss = 1.21113002\n",
      "Iteration 2577, loss = 1.21110938\n",
      "Iteration 2578, loss = 1.21108957\n",
      "Iteration 2579, loss = 1.21106598\n",
      "Iteration 2580, loss = 1.21104663\n",
      "Iteration 2581, loss = 1.21102134\n",
      "Iteration 2582, loss = 1.21100308\n",
      "Iteration 2583, loss = 1.21097938\n",
      "Iteration 2584, loss = 1.21095779\n",
      "Iteration 2585, loss = 1.21093761\n",
      "Iteration 2586, loss = 1.21091515\n",
      "Iteration 2587, loss = 1.21089347\n",
      "Iteration 2588, loss = 1.21087275\n",
      "Iteration 2589, loss = 1.21085131\n",
      "Iteration 2590, loss = 1.21083162\n",
      "Iteration 2591, loss = 1.21080773\n",
      "Iteration 2592, loss = 1.21078864\n",
      "Iteration 2593, loss = 1.21076510\n",
      "Iteration 2594, loss = 1.21074493\n",
      "Iteration 2595, loss = 1.21072287\n",
      "Iteration 2596, loss = 1.21070179\n",
      "Iteration 2597, loss = 1.21068092\n",
      "Iteration 2598, loss = 1.21065990\n",
      "Iteration 2599, loss = 1.21063918\n",
      "Iteration 2600, loss = 1.21061676\n",
      "Iteration 2601, loss = 1.21059541\n",
      "Iteration 2602, loss = 1.21057411\n",
      "Iteration 2603, loss = 1.21055368\n",
      "Iteration 2604, loss = 1.21053076\n",
      "Iteration 2605, loss = 1.21050940\n",
      "Iteration 2606, loss = 1.21048758\n",
      "Iteration 2607, loss = 1.21046670\n",
      "Iteration 2608, loss = 1.21044643\n",
      "Iteration 2609, loss = 1.21042555\n",
      "Iteration 2610, loss = 1.21040517\n",
      "Iteration 2611, loss = 1.21038260\n",
      "Iteration 2612, loss = 1.21036264\n",
      "Iteration 2613, loss = 1.21033959\n",
      "Iteration 2614, loss = 1.21032183\n",
      "Iteration 2615, loss = 1.21029773\n",
      "Iteration 2616, loss = 1.21027968\n",
      "Iteration 2617, loss = 1.21025647\n",
      "Iteration 2618, loss = 1.21023465\n",
      "Iteration 2619, loss = 1.21021300\n",
      "Iteration 2620, loss = 1.21019425\n",
      "Iteration 2621, loss = 1.21017153\n",
      "Iteration 2622, loss = 1.21014954\n",
      "Iteration 2623, loss = 1.21013295\n",
      "Iteration 2624, loss = 1.21010859\n",
      "Iteration 2625, loss = 1.21008590\n",
      "Iteration 2626, loss = 1.21006597\n",
      "Iteration 2627, loss = 1.21004563\n",
      "Iteration 2628, loss = 1.21002399\n",
      "Iteration 2629, loss = 1.21000399\n",
      "Iteration 2630, loss = 1.20998161\n",
      "Iteration 2631, loss = 1.20996319\n",
      "Iteration 2632, loss = 1.20994171\n",
      "Iteration 2633, loss = 1.20991936\n",
      "Iteration 2634, loss = 1.20990036\n",
      "Iteration 2635, loss = 1.20987782\n",
      "Iteration 2636, loss = 1.20985578\n",
      "Iteration 2637, loss = 1.20983420\n",
      "Iteration 2638, loss = 1.20981772\n",
      "Iteration 2639, loss = 1.20979284\n",
      "Iteration 2640, loss = 1.20977342\n",
      "Iteration 2641, loss = 1.20975038\n",
      "Iteration 2642, loss = 1.20972978\n",
      "Iteration 2643, loss = 1.20971021\n",
      "Iteration 2644, loss = 1.20968821\n",
      "Iteration 2645, loss = 1.20966742\n",
      "Iteration 2646, loss = 1.20964584\n",
      "Iteration 2647, loss = 1.20962540\n",
      "Iteration 2648, loss = 1.20960655\n",
      "Iteration 2649, loss = 1.20958373\n",
      "Iteration 2650, loss = 1.20956313\n",
      "Iteration 2651, loss = 1.20954179\n",
      "Iteration 2652, loss = 1.20952180\n",
      "Iteration 2653, loss = 1.20950057\n",
      "Iteration 2654, loss = 1.20948179\n",
      "Iteration 2655, loss = 1.20945883\n",
      "Iteration 2656, loss = 1.20943758\n",
      "Iteration 2657, loss = 1.20941747\n",
      "Iteration 2658, loss = 1.20939812\n",
      "Iteration 2659, loss = 1.20937697\n",
      "Iteration 2660, loss = 1.20935489\n",
      "Iteration 2661, loss = 1.20933387\n",
      "Iteration 2662, loss = 1.20931330\n",
      "Iteration 2663, loss = 1.20929292\n",
      "Iteration 2664, loss = 1.20927234\n",
      "Iteration 2665, loss = 1.20925068\n",
      "Iteration 2666, loss = 1.20923318\n",
      "Iteration 2667, loss = 1.20921109\n",
      "Iteration 2668, loss = 1.20918856\n",
      "Iteration 2669, loss = 1.20916836\n",
      "Iteration 2670, loss = 1.20914837\n",
      "Iteration 2671, loss = 1.20912798\n",
      "Iteration 2672, loss = 1.20910632\n",
      "Iteration 2673, loss = 1.20908717\n",
      "Iteration 2674, loss = 1.20906603\n",
      "Iteration 2675, loss = 1.20904543\n",
      "Iteration 2676, loss = 1.20902569\n",
      "Iteration 2677, loss = 1.20900346\n",
      "Iteration 2678, loss = 1.20898421\n",
      "Iteration 2679, loss = 1.20896163\n",
      "Iteration 2680, loss = 1.20894239\n",
      "Iteration 2681, loss = 1.20892167\n",
      "Iteration 2682, loss = 1.20890457\n",
      "Iteration 2683, loss = 1.20888020\n",
      "Iteration 2684, loss = 1.20886406\n",
      "Iteration 2685, loss = 1.20883930\n",
      "Iteration 2686, loss = 1.20881856\n",
      "Iteration 2687, loss = 1.20880090\n",
      "Iteration 2688, loss = 1.20878024\n",
      "Iteration 2689, loss = 1.20875701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2690, loss = 1.20874029\n",
      "Iteration 2691, loss = 1.20871655\n",
      "Iteration 2692, loss = 1.20869853\n",
      "Iteration 2693, loss = 1.20867695\n",
      "Iteration 2694, loss = 1.20865660\n",
      "Iteration 2695, loss = 1.20863444\n",
      "Iteration 2696, loss = 1.20861403\n",
      "Iteration 2697, loss = 1.20859509\n",
      "Iteration 2698, loss = 1.20857448\n",
      "Iteration 2699, loss = 1.20855241\n",
      "Iteration 2700, loss = 1.20853161\n",
      "Iteration 2701, loss = 1.20851100\n",
      "Iteration 2702, loss = 1.20849155\n",
      "Iteration 2703, loss = 1.20847103\n",
      "Iteration 2704, loss = 1.20845431\n",
      "Iteration 2705, loss = 1.20843069\n",
      "Iteration 2706, loss = 1.20841082\n",
      "Iteration 2707, loss = 1.20839027\n",
      "Iteration 2708, loss = 1.20837059\n",
      "Iteration 2709, loss = 1.20834912\n",
      "Iteration 2710, loss = 1.20832900\n",
      "Iteration 2711, loss = 1.20830930\n",
      "Iteration 2712, loss = 1.20828738\n",
      "Iteration 2713, loss = 1.20826824\n",
      "Iteration 2714, loss = 1.20824930\n",
      "Iteration 2715, loss = 1.20822821\n",
      "Iteration 2716, loss = 1.20820575\n",
      "Iteration 2717, loss = 1.20818573\n",
      "Iteration 2718, loss = 1.20816556\n",
      "Iteration 2719, loss = 1.20814514\n",
      "Iteration 2720, loss = 1.20812624\n",
      "Iteration 2721, loss = 1.20810478\n",
      "Iteration 2722, loss = 1.20808572\n",
      "Iteration 2723, loss = 1.20806546\n",
      "Iteration 2724, loss = 1.20804598\n",
      "Iteration 2725, loss = 1.20802580\n",
      "Iteration 2726, loss = 1.20800791\n",
      "Iteration 2727, loss = 1.20798567\n",
      "Iteration 2728, loss = 1.20796457\n",
      "Iteration 2729, loss = 1.20794428\n",
      "Iteration 2730, loss = 1.20792472\n",
      "Iteration 2731, loss = 1.20790387\n",
      "Iteration 2732, loss = 1.20788456\n",
      "Iteration 2733, loss = 1.20786583\n",
      "Iteration 2734, loss = 1.20784474\n",
      "Iteration 2735, loss = 1.20782551\n",
      "Iteration 2736, loss = 1.20780374\n",
      "Iteration 2737, loss = 1.20778518\n",
      "Iteration 2738, loss = 1.20776320\n",
      "Iteration 2739, loss = 1.20774452\n",
      "Iteration 2740, loss = 1.20772615\n",
      "Iteration 2741, loss = 1.20770357\n",
      "Iteration 2742, loss = 1.20768179\n",
      "Iteration 2743, loss = 1.20766422\n",
      "Iteration 2744, loss = 1.20764322\n",
      "Iteration 2745, loss = 1.20762293\n",
      "Iteration 2746, loss = 1.20760355\n",
      "Iteration 2747, loss = 1.20758342\n",
      "Iteration 2748, loss = 1.20756348\n",
      "Iteration 2749, loss = 1.20754286\n",
      "Iteration 2750, loss = 1.20752197\n",
      "Iteration 2751, loss = 1.20750268\n",
      "Iteration 2752, loss = 1.20748261\n",
      "Iteration 2753, loss = 1.20746411\n",
      "Iteration 2754, loss = 1.20744484\n",
      "Iteration 2755, loss = 1.20742318\n",
      "Iteration 2756, loss = 1.20740307\n",
      "Iteration 2757, loss = 1.20738489\n",
      "Iteration 2758, loss = 1.20736631\n",
      "Iteration 2759, loss = 1.20734283\n",
      "Iteration 2760, loss = 1.20732512\n",
      "Iteration 2761, loss = 1.20730787\n",
      "Iteration 2762, loss = 1.20728351\n",
      "Iteration 2763, loss = 1.20726753\n",
      "Iteration 2764, loss = 1.20724524\n",
      "Iteration 2765, loss = 1.20722538\n",
      "Iteration 2766, loss = 1.20720589\n",
      "Iteration 2767, loss = 1.20718574\n",
      "Iteration 2768, loss = 1.20716633\n",
      "Iteration 2769, loss = 1.20714472\n",
      "Iteration 2770, loss = 1.20712600\n",
      "Iteration 2771, loss = 1.20710794\n",
      "Iteration 2772, loss = 1.20708570\n",
      "Iteration 2773, loss = 1.20706575\n",
      "Iteration 2774, loss = 1.20704753\n",
      "Iteration 2775, loss = 1.20702797\n",
      "Iteration 2776, loss = 1.20700716\n",
      "Iteration 2777, loss = 1.20698677\n",
      "Iteration 2778, loss = 1.20697058\n",
      "Iteration 2779, loss = 1.20695059\n",
      "Iteration 2780, loss = 1.20692873\n",
      "Iteration 2781, loss = 1.20690941\n",
      "Iteration 2782, loss = 1.20688873\n",
      "Iteration 2783, loss = 1.20687083\n",
      "Iteration 2784, loss = 1.20685014\n",
      "Iteration 2785, loss = 1.20682921\n",
      "Iteration 2786, loss = 1.20681025\n",
      "Iteration 2787, loss = 1.20679005\n",
      "Iteration 2788, loss = 1.20677003\n",
      "Iteration 2789, loss = 1.20675083\n",
      "Iteration 2790, loss = 1.20673047\n",
      "Iteration 2791, loss = 1.20671507\n",
      "Iteration 2792, loss = 1.20669450\n",
      "Iteration 2793, loss = 1.20667227\n",
      "Iteration 2794, loss = 1.20665184\n",
      "Iteration 2795, loss = 1.20663744\n",
      "Iteration 2796, loss = 1.20661277\n",
      "Iteration 2797, loss = 1.20659470\n",
      "Iteration 2798, loss = 1.20657651\n",
      "Iteration 2799, loss = 1.20655518\n",
      "Iteration 2800, loss = 1.20653535\n",
      "Iteration 2801, loss = 1.20651523\n",
      "Iteration 2802, loss = 1.20649521\n",
      "Iteration 2803, loss = 1.20647619\n",
      "Iteration 2804, loss = 1.20645860\n",
      "Iteration 2805, loss = 1.20643710\n",
      "Iteration 2806, loss = 1.20641757\n",
      "Iteration 2807, loss = 1.20639792\n",
      "Iteration 2808, loss = 1.20637979\n",
      "Iteration 2809, loss = 1.20636083\n",
      "Iteration 2810, loss = 1.20633959\n",
      "Iteration 2811, loss = 1.20632000\n",
      "Iteration 2812, loss = 1.20630015\n",
      "Iteration 2813, loss = 1.20628496\n",
      "Iteration 2814, loss = 1.20626157\n",
      "Iteration 2815, loss = 1.20624126\n",
      "Iteration 2816, loss = 1.20622316\n",
      "Iteration 2817, loss = 1.20620288\n",
      "Iteration 2818, loss = 1.20618446\n",
      "Iteration 2819, loss = 1.20616553\n",
      "Iteration 2820, loss = 1.20614718\n",
      "Iteration 2821, loss = 1.20612510\n",
      "Iteration 2822, loss = 1.20610554\n",
      "Iteration 2823, loss = 1.20608938\n",
      "Iteration 2824, loss = 1.20606694\n",
      "Iteration 2825, loss = 1.20604923\n",
      "Iteration 2826, loss = 1.20603002\n",
      "Iteration 2827, loss = 1.20601210\n",
      "Iteration 2828, loss = 1.20599117\n",
      "Iteration 2829, loss = 1.20597281\n",
      "Iteration 2830, loss = 1.20595215\n",
      "Iteration 2831, loss = 1.20593330\n",
      "Iteration 2832, loss = 1.20591502\n",
      "Iteration 2833, loss = 1.20589665\n",
      "Iteration 2834, loss = 1.20587416\n",
      "Iteration 2835, loss = 1.20585748\n",
      "Iteration 2836, loss = 1.20583586\n",
      "Iteration 2837, loss = 1.20581745\n",
      "Iteration 2838, loss = 1.20579703\n",
      "Iteration 2839, loss = 1.20577869\n",
      "Iteration 2840, loss = 1.20575839\n",
      "Iteration 2841, loss = 1.20573917\n",
      "Iteration 2842, loss = 1.20572062\n",
      "Iteration 2843, loss = 1.20570008\n",
      "Iteration 2844, loss = 1.20568063\n",
      "Iteration 2845, loss = 1.20566103\n",
      "Iteration 2846, loss = 1.20564244\n",
      "Iteration 2847, loss = 1.20562241\n",
      "Iteration 2848, loss = 1.20560800\n",
      "Iteration 2849, loss = 1.20558409\n",
      "Iteration 2850, loss = 1.20556518\n",
      "Iteration 2851, loss = 1.20554664\n",
      "Iteration 2852, loss = 1.20552685\n",
      "Iteration 2853, loss = 1.20550988\n",
      "Iteration 2854, loss = 1.20548890\n",
      "Iteration 2855, loss = 1.20547177\n",
      "Iteration 2856, loss = 1.20544982\n",
      "Iteration 2857, loss = 1.20543148\n",
      "Iteration 2858, loss = 1.20541321\n",
      "Iteration 2859, loss = 1.20539371\n",
      "Iteration 2860, loss = 1.20537324\n",
      "Iteration 2861, loss = 1.20535539\n",
      "Iteration 2862, loss = 1.20533699\n",
      "Iteration 2863, loss = 1.20531796\n",
      "Iteration 2864, loss = 1.20529710\n",
      "Iteration 2865, loss = 1.20527846\n",
      "Iteration 2866, loss = 1.20526365\n",
      "Iteration 2867, loss = 1.20523940\n",
      "Iteration 2868, loss = 1.20522107\n",
      "Iteration 2869, loss = 1.20520138\n",
      "Iteration 2870, loss = 1.20518343\n",
      "Iteration 2871, loss = 1.20516344\n",
      "Iteration 2872, loss = 1.20514448\n",
      "Iteration 2873, loss = 1.20512705\n",
      "Iteration 2874, loss = 1.20510697\n",
      "Iteration 2875, loss = 1.20508734\n",
      "Iteration 2876, loss = 1.20506829\n",
      "Iteration 2877, loss = 1.20505417\n",
      "Iteration 2878, loss = 1.20503221\n",
      "Iteration 2879, loss = 1.20501113\n",
      "Iteration 2880, loss = 1.20499257\n",
      "Iteration 2881, loss = 1.20497483\n",
      "Iteration 2882, loss = 1.20495586\n",
      "Iteration 2883, loss = 1.20493652\n",
      "Iteration 2884, loss = 1.20491933\n",
      "Iteration 2885, loss = 1.20489905\n",
      "Iteration 2886, loss = 1.20487973\n",
      "Iteration 2887, loss = 1.20486095\n",
      "Iteration 2888, loss = 1.20484141\n",
      "Iteration 2889, loss = 1.20482359\n",
      "Iteration 2890, loss = 1.20480575\n",
      "Iteration 2891, loss = 1.20478655\n",
      "Iteration 2892, loss = 1.20476689\n",
      "Iteration 2893, loss = 1.20474885\n",
      "Iteration 2894, loss = 1.20473214\n",
      "Iteration 2895, loss = 1.20471036\n",
      "Iteration 2896, loss = 1.20469009\n",
      "Iteration 2897, loss = 1.20467206\n",
      "Iteration 2898, loss = 1.20465262\n",
      "Iteration 2899, loss = 1.20463638\n",
      "Iteration 2900, loss = 1.20461689\n",
      "Iteration 2901, loss = 1.20459618\n",
      "Iteration 2902, loss = 1.20457835\n",
      "Iteration 2903, loss = 1.20456169\n",
      "Iteration 2904, loss = 1.20454033\n",
      "Iteration 2905, loss = 1.20452491\n",
      "Iteration 2906, loss = 1.20450496\n",
      "Iteration 2907, loss = 1.20448553\n",
      "Iteration 2908, loss = 1.20446544\n",
      "Iteration 2909, loss = 1.20444796\n",
      "Iteration 2910, loss = 1.20442801\n",
      "Iteration 2911, loss = 1.20440877\n",
      "Iteration 2912, loss = 1.20439115\n",
      "Iteration 2913, loss = 1.20437274\n",
      "Iteration 2914, loss = 1.20435422\n",
      "Iteration 2915, loss = 1.20433451\n",
      "Iteration 2916, loss = 1.20431622\n",
      "Iteration 2917, loss = 1.20430159\n",
      "Iteration 2918, loss = 1.20427947\n",
      "Iteration 2919, loss = 1.20425983\n",
      "Iteration 2920, loss = 1.20424068\n",
      "Iteration 2921, loss = 1.20422326\n",
      "Iteration 2922, loss = 1.20420374\n",
      "Iteration 2923, loss = 1.20418619\n",
      "Iteration 2924, loss = 1.20416834\n",
      "Iteration 2925, loss = 1.20414950\n",
      "Iteration 2926, loss = 1.20413425\n",
      "Iteration 2927, loss = 1.20411140\n",
      "Iteration 2928, loss = 1.20409235\n",
      "Iteration 2929, loss = 1.20407195\n",
      "Iteration 2930, loss = 1.20405896\n",
      "Iteration 2931, loss = 1.20403655\n",
      "Iteration 2932, loss = 1.20401639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2933, loss = 1.20399999\n",
      "Iteration 2934, loss = 1.20398421\n",
      "Iteration 2935, loss = 1.20396056\n",
      "Iteration 2936, loss = 1.20394268\n",
      "Iteration 2937, loss = 1.20392334\n",
      "Iteration 2938, loss = 1.20390543\n",
      "Iteration 2939, loss = 1.20388671\n",
      "Iteration 2940, loss = 1.20386815\n",
      "Iteration 2941, loss = 1.20384945\n",
      "Iteration 2942, loss = 1.20383041\n",
      "Iteration 2943, loss = 1.20381189\n",
      "Iteration 2944, loss = 1.20379467\n",
      "Iteration 2945, loss = 1.20377483\n",
      "Iteration 2946, loss = 1.20375702\n",
      "Iteration 2947, loss = 1.20373912\n",
      "Iteration 2948, loss = 1.20371938\n",
      "Iteration 2949, loss = 1.20370122\n",
      "Iteration 2950, loss = 1.20368287\n",
      "Iteration 2951, loss = 1.20366560\n",
      "Iteration 2952, loss = 1.20364642\n",
      "Iteration 2953, loss = 1.20362964\n",
      "Iteration 2954, loss = 1.20360820\n",
      "Iteration 2955, loss = 1.20358970\n",
      "Iteration 2956, loss = 1.20357418\n",
      "Iteration 2957, loss = 1.20355292\n",
      "Iteration 2958, loss = 1.20353681\n",
      "Iteration 2959, loss = 1.20351752\n",
      "Iteration 2960, loss = 1.20349994\n",
      "Iteration 2961, loss = 1.20348005\n",
      "Iteration 2962, loss = 1.20346252\n",
      "Iteration 2963, loss = 1.20344435\n",
      "Iteration 2964, loss = 1.20342603\n",
      "Iteration 2965, loss = 1.20340808\n",
      "Iteration 2966, loss = 1.20338843\n",
      "Iteration 2967, loss = 1.20337099\n",
      "Iteration 2968, loss = 1.20335297\n",
      "Iteration 2969, loss = 1.20333263\n",
      "Iteration 2970, loss = 1.20331464\n",
      "Iteration 2971, loss = 1.20329580\n",
      "Iteration 2972, loss = 1.20327807\n",
      "Iteration 2973, loss = 1.20326035\n",
      "Iteration 2974, loss = 1.20324326\n",
      "Iteration 2975, loss = 1.20322462\n",
      "Iteration 2976, loss = 1.20320483\n",
      "Iteration 2977, loss = 1.20318759\n",
      "Iteration 2978, loss = 1.20317061\n",
      "Iteration 2979, loss = 1.20315188\n",
      "Iteration 2980, loss = 1.20313448\n",
      "Iteration 2981, loss = 1.20311307\n",
      "Iteration 2982, loss = 1.20309792\n",
      "Iteration 2983, loss = 1.20307701\n",
      "Iteration 2984, loss = 1.20305926\n",
      "Iteration 2985, loss = 1.20304061\n",
      "Iteration 2986, loss = 1.20302224\n",
      "Iteration 2987, loss = 1.20300437\n",
      "Iteration 2988, loss = 1.20298608\n",
      "Iteration 2989, loss = 1.20296941\n",
      "Iteration 2990, loss = 1.20294902\n",
      "Iteration 2991, loss = 1.20293226\n",
      "Iteration 2992, loss = 1.20291667\n",
      "Iteration 2993, loss = 1.20289677\n",
      "Iteration 2994, loss = 1.20287867\n",
      "Iteration 2995, loss = 1.20286016\n",
      "Iteration 2996, loss = 1.20284118\n",
      "Iteration 2997, loss = 1.20282536\n",
      "Iteration 2998, loss = 1.20280516\n",
      "Iteration 2999, loss = 1.20279217\n",
      "Iteration 3000, loss = 1.20276901\n",
      "Iteration 3001, loss = 1.20275114\n",
      "Iteration 3002, loss = 1.20273335\n",
      "Iteration 3003, loss = 1.20271565\n",
      "Iteration 3004, loss = 1.20269753\n",
      "Iteration 3005, loss = 1.20268062\n",
      "Iteration 3006, loss = 1.20266068\n",
      "Iteration 3007, loss = 1.20264325\n",
      "Iteration 3008, loss = 1.20262690\n",
      "Iteration 3009, loss = 1.20260543\n",
      "Iteration 3010, loss = 1.20259121\n",
      "Iteration 3011, loss = 1.20257035\n",
      "Iteration 3012, loss = 1.20255838\n",
      "Iteration 3013, loss = 1.20253432\n",
      "Iteration 3014, loss = 1.20251550\n",
      "Iteration 3015, loss = 1.20249926\n",
      "Iteration 3016, loss = 1.20248557\n",
      "Iteration 3017, loss = 1.20246382\n",
      "Iteration 3018, loss = 1.20244333\n",
      "Iteration 3019, loss = 1.20242645\n",
      "Iteration 3020, loss = 1.20240772\n",
      "Iteration 3021, loss = 1.20239035\n",
      "Iteration 3022, loss = 1.20237134\n",
      "Iteration 3023, loss = 1.20235254\n",
      "Iteration 3024, loss = 1.20233425\n",
      "Iteration 3025, loss = 1.20231724\n",
      "Iteration 3026, loss = 1.20229892\n",
      "Iteration 3027, loss = 1.20228805\n",
      "Iteration 3028, loss = 1.20226436\n",
      "Iteration 3029, loss = 1.20224538\n",
      "Iteration 3030, loss = 1.20223012\n",
      "Iteration 3031, loss = 1.20221006\n",
      "Iteration 3032, loss = 1.20219207\n",
      "Iteration 3033, loss = 1.20217423\n",
      "Iteration 3034, loss = 1.20215580\n",
      "Iteration 3035, loss = 1.20213747\n",
      "Iteration 3036, loss = 1.20212146\n",
      "Iteration 3037, loss = 1.20210190\n",
      "Iteration 3038, loss = 1.20208447\n",
      "Iteration 3039, loss = 1.20206653\n",
      "Iteration 3040, loss = 1.20204874\n",
      "Iteration 3041, loss = 1.20203066\n",
      "Iteration 3042, loss = 1.20201497\n",
      "Iteration 3043, loss = 1.20199450\n",
      "Iteration 3044, loss = 1.20198099\n",
      "Iteration 3045, loss = 1.20195885\n",
      "Iteration 3046, loss = 1.20194277\n",
      "Iteration 3047, loss = 1.20192447\n",
      "Iteration 3048, loss = 1.20190575\n",
      "Iteration 3049, loss = 1.20188737\n",
      "Iteration 3050, loss = 1.20187653\n",
      "Iteration 3051, loss = 1.20185296\n",
      "Iteration 3052, loss = 1.20183535\n",
      "Iteration 3053, loss = 1.20181600\n",
      "Iteration 3054, loss = 1.20180216\n",
      "Iteration 3055, loss = 1.20178296\n",
      "Iteration 3056, loss = 1.20176329\n",
      "Iteration 3057, loss = 1.20174517\n",
      "Iteration 3058, loss = 1.20173067\n",
      "Iteration 3059, loss = 1.20171119\n",
      "Iteration 3060, loss = 1.20169201\n",
      "Iteration 3061, loss = 1.20167448\n",
      "Iteration 3062, loss = 1.20165893\n",
      "Iteration 3063, loss = 1.20164265\n",
      "Iteration 3064, loss = 1.20162166\n",
      "Iteration 3065, loss = 1.20160668\n",
      "Iteration 3066, loss = 1.20158797\n",
      "Iteration 3067, loss = 1.20156860\n",
      "Iteration 3068, loss = 1.20155132\n",
      "Iteration 3069, loss = 1.20153331\n",
      "Iteration 3070, loss = 1.20151490\n",
      "Iteration 3071, loss = 1.20149725\n",
      "Iteration 3072, loss = 1.20147916\n",
      "Iteration 3073, loss = 1.20146249\n",
      "Iteration 3074, loss = 1.20144492\n",
      "Iteration 3075, loss = 1.20142709\n",
      "Iteration 3076, loss = 1.20140809\n",
      "Iteration 3077, loss = 1.20139111\n",
      "Iteration 3078, loss = 1.20137404\n",
      "Iteration 3079, loss = 1.20135476\n",
      "Iteration 3080, loss = 1.20133688\n",
      "Iteration 3081, loss = 1.20131956\n",
      "Iteration 3082, loss = 1.20130399\n",
      "Iteration 3083, loss = 1.20128633\n",
      "Iteration 3084, loss = 1.20127093\n",
      "Iteration 3085, loss = 1.20125129\n",
      "Iteration 3086, loss = 1.20123571\n",
      "Iteration 3087, loss = 1.20121582\n",
      "Iteration 3088, loss = 1.20119848\n",
      "Iteration 3089, loss = 1.20117937\n",
      "Iteration 3090, loss = 1.20116611\n",
      "Iteration 3091, loss = 1.20114538\n",
      "Iteration 3092, loss = 1.20112687\n",
      "Iteration 3093, loss = 1.20111082\n",
      "Iteration 3094, loss = 1.20109190\n",
      "Iteration 3095, loss = 1.20107835\n",
      "Iteration 3096, loss = 1.20105594\n",
      "Iteration 3097, loss = 1.20103813\n",
      "Iteration 3098, loss = 1.20102106\n",
      "Iteration 3099, loss = 1.20100481\n",
      "Iteration 3100, loss = 1.20098690\n",
      "Iteration 3101, loss = 1.20097240\n",
      "Iteration 3102, loss = 1.20095044\n",
      "Iteration 3103, loss = 1.20093520\n",
      "Iteration 3104, loss = 1.20091511\n",
      "Iteration 3105, loss = 1.20089892\n",
      "Iteration 3106, loss = 1.20088439\n",
      "Iteration 3107, loss = 1.20086254\n",
      "Iteration 3108, loss = 1.20084554\n",
      "Iteration 3109, loss = 1.20082899\n",
      "Iteration 3110, loss = 1.20080962\n",
      "Iteration 3111, loss = 1.20079402\n",
      "Iteration 3112, loss = 1.20077996\n",
      "Iteration 3113, loss = 1.20075961\n",
      "Iteration 3114, loss = 1.20074011\n",
      "Iteration 3115, loss = 1.20072678\n",
      "Iteration 3116, loss = 1.20070952\n",
      "Iteration 3117, loss = 1.20069009\n",
      "Iteration 3118, loss = 1.20067079\n",
      "Iteration 3119, loss = 1.20065504\n",
      "Iteration 3120, loss = 1.20063635\n",
      "Iteration 3121, loss = 1.20062271\n",
      "Iteration 3122, loss = 1.20060143\n",
      "Iteration 3123, loss = 1.20058525\n",
      "Iteration 3124, loss = 1.20056548\n",
      "Iteration 3125, loss = 1.20054916\n",
      "Iteration 3126, loss = 1.20053093\n",
      "Iteration 3127, loss = 1.20051338\n",
      "Iteration 3128, loss = 1.20049646\n",
      "Iteration 3129, loss = 1.20048082\n",
      "Iteration 3130, loss = 1.20046253\n",
      "Iteration 3131, loss = 1.20044413\n",
      "Iteration 3132, loss = 1.20042659\n",
      "Iteration 3133, loss = 1.20041003\n",
      "Iteration 3134, loss = 1.20039242\n",
      "Iteration 3135, loss = 1.20037891\n",
      "Iteration 3136, loss = 1.20036255\n",
      "Iteration 3137, loss = 1.20034197\n",
      "Iteration 3138, loss = 1.20032407\n",
      "Iteration 3139, loss = 1.20030936\n",
      "Iteration 3140, loss = 1.20029256\n",
      "Iteration 3141, loss = 1.20027220\n",
      "Iteration 3142, loss = 1.20025517\n",
      "Iteration 3143, loss = 1.20023918\n",
      "Iteration 3144, loss = 1.20022037\n",
      "Iteration 3145, loss = 1.20020436\n",
      "Iteration 3146, loss = 1.20018570\n",
      "Iteration 3147, loss = 1.20016871\n",
      "Iteration 3148, loss = 1.20015385\n",
      "Iteration 3149, loss = 1.20013439\n",
      "Iteration 3150, loss = 1.20011713\n",
      "Iteration 3151, loss = 1.20009973\n",
      "Iteration 3152, loss = 1.20008379\n",
      "Iteration 3153, loss = 1.20006603\n",
      "Iteration 3154, loss = 1.20004891\n",
      "Iteration 3155, loss = 1.20003283\n",
      "Iteration 3156, loss = 1.20001415\n",
      "Iteration 3157, loss = 1.19999806\n",
      "Iteration 3158, loss = 1.19997917\n",
      "Iteration 3159, loss = 1.19996255\n",
      "Iteration 3160, loss = 1.19994759\n",
      "Iteration 3161, loss = 1.19993150\n",
      "Iteration 3162, loss = 1.19991323\n",
      "Iteration 3163, loss = 1.19989707\n",
      "Iteration 3164, loss = 1.19987916\n",
      "Iteration 3165, loss = 1.19985933\n",
      "Iteration 3166, loss = 1.19984478\n",
      "Iteration 3167, loss = 1.19982651\n",
      "Iteration 3168, loss = 1.19981108\n",
      "Iteration 3169, loss = 1.19979644\n",
      "Iteration 3170, loss = 1.19977700\n",
      "Iteration 3171, loss = 1.19975971\n",
      "Iteration 3172, loss = 1.19974357\n",
      "Iteration 3173, loss = 1.19972269\n",
      "Iteration 3174, loss = 1.19970518\n",
      "Iteration 3175, loss = 1.19968953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3176, loss = 1.19967067\n",
      "Iteration 3177, loss = 1.19965474\n",
      "Iteration 3178, loss = 1.19963881\n",
      "Iteration 3179, loss = 1.19962053\n",
      "Iteration 3180, loss = 1.19960599\n",
      "Iteration 3181, loss = 1.19959073\n",
      "Iteration 3182, loss = 1.19957423\n",
      "Iteration 3183, loss = 1.19955265\n",
      "Iteration 3184, loss = 1.19953520\n",
      "Iteration 3185, loss = 1.19951958\n",
      "Iteration 3186, loss = 1.19950179\n",
      "Iteration 3187, loss = 1.19948551\n",
      "Iteration 3188, loss = 1.19946817\n",
      "Iteration 3189, loss = 1.19945131\n",
      "Iteration 3190, loss = 1.19943535\n",
      "Iteration 3191, loss = 1.19941844\n",
      "Iteration 3192, loss = 1.19940985\n",
      "Iteration 3193, loss = 1.19938437\n",
      "Iteration 3194, loss = 1.19936685\n",
      "Iteration 3195, loss = 1.19934995\n",
      "Iteration 3196, loss = 1.19933184\n",
      "Iteration 3197, loss = 1.19931510\n",
      "Iteration 3198, loss = 1.19929840\n",
      "Iteration 3199, loss = 1.19928151\n",
      "Iteration 3200, loss = 1.19926636\n",
      "Iteration 3201, loss = 1.19925108\n",
      "Iteration 3202, loss = 1.19923160\n",
      "Iteration 3203, loss = 1.19921484\n",
      "Iteration 3204, loss = 1.19919909\n",
      "Iteration 3205, loss = 1.19918150\n",
      "Iteration 3206, loss = 1.19916349\n",
      "Iteration 3207, loss = 1.19914909\n",
      "Iteration 3208, loss = 1.19913549\n",
      "Iteration 3209, loss = 1.19911304\n",
      "Iteration 3210, loss = 1.19909882\n",
      "Iteration 3211, loss = 1.19908154\n",
      "Iteration 3212, loss = 1.19906465\n",
      "Iteration 3213, loss = 1.19904606\n",
      "Iteration 3214, loss = 1.19902954\n",
      "Iteration 3215, loss = 1.19901457\n",
      "Iteration 3216, loss = 1.19899551\n",
      "Iteration 3217, loss = 1.19897864\n",
      "Iteration 3218, loss = 1.19896319\n",
      "Iteration 3219, loss = 1.19894833\n",
      "Iteration 3220, loss = 1.19893139\n",
      "Iteration 3221, loss = 1.19891565\n",
      "Iteration 3222, loss = 1.19889686\n",
      "Iteration 3223, loss = 1.19888113\n",
      "Iteration 3224, loss = 1.19886358\n",
      "Iteration 3225, loss = 1.19884653\n",
      "Iteration 3226, loss = 1.19882943\n",
      "Iteration 3227, loss = 1.19881323\n",
      "Iteration 3228, loss = 1.19879821\n",
      "Iteration 3229, loss = 1.19877940\n",
      "Iteration 3230, loss = 1.19876154\n",
      "Iteration 3231, loss = 1.19874958\n",
      "Iteration 3232, loss = 1.19872992\n",
      "Iteration 3233, loss = 1.19871831\n",
      "Iteration 3234, loss = 1.19869698\n",
      "Iteration 3235, loss = 1.19867944\n",
      "Iteration 3236, loss = 1.19866350\n",
      "Iteration 3237, loss = 1.19864759\n",
      "Iteration 3238, loss = 1.19863169\n",
      "Iteration 3239, loss = 1.19861324\n",
      "Iteration 3240, loss = 1.19859651\n",
      "Iteration 3241, loss = 1.19857990\n",
      "Iteration 3242, loss = 1.19856355\n",
      "Iteration 3243, loss = 1.19854487\n",
      "Iteration 3244, loss = 1.19852928\n",
      "Iteration 3245, loss = 1.19851458\n",
      "Iteration 3246, loss = 1.19849724\n",
      "Iteration 3247, loss = 1.19847993\n",
      "Iteration 3248, loss = 1.19846378\n",
      "Iteration 3249, loss = 1.19844591\n",
      "Iteration 3250, loss = 1.19843007\n",
      "Iteration 3251, loss = 1.19841280\n",
      "Iteration 3252, loss = 1.19839572\n",
      "Iteration 3253, loss = 1.19838200\n",
      "Iteration 3254, loss = 1.19836615\n",
      "Iteration 3255, loss = 1.19834714\n",
      "Iteration 3256, loss = 1.19833168\n",
      "Iteration 3257, loss = 1.19831333\n",
      "Iteration 3258, loss = 1.19829931\n",
      "Iteration 3259, loss = 1.19828104\n",
      "Iteration 3260, loss = 1.19826597\n",
      "Iteration 3261, loss = 1.19824876\n",
      "Iteration 3262, loss = 1.19823057\n",
      "Iteration 3263, loss = 1.19821840\n",
      "Iteration 3264, loss = 1.19819877\n",
      "Iteration 3265, loss = 1.19818149\n",
      "Iteration 3266, loss = 1.19816413\n",
      "Iteration 3267, loss = 1.19814807\n",
      "Iteration 3268, loss = 1.19813358\n",
      "Iteration 3269, loss = 1.19811582\n",
      "Iteration 3270, loss = 1.19809875\n",
      "Iteration 3271, loss = 1.19808280\n",
      "Iteration 3272, loss = 1.19806728\n",
      "Iteration 3273, loss = 1.19804911\n",
      "Iteration 3274, loss = 1.19803559\n",
      "Iteration 3275, loss = 1.19801708\n",
      "Iteration 3276, loss = 1.19799957\n",
      "Iteration 3277, loss = 1.19798307\n",
      "Iteration 3278, loss = 1.19796868\n",
      "Iteration 3279, loss = 1.19795086\n",
      "Iteration 3280, loss = 1.19793536\n",
      "Iteration 3281, loss = 1.19791767\n",
      "Iteration 3282, loss = 1.19790599\n",
      "Iteration 3283, loss = 1.19788758\n",
      "Iteration 3284, loss = 1.19786941\n",
      "Iteration 3285, loss = 1.19785992\n",
      "Iteration 3286, loss = 1.19783571\n",
      "Iteration 3287, loss = 1.19782121\n",
      "Iteration 3288, loss = 1.19780425\n",
      "Iteration 3289, loss = 1.19779057\n",
      "Iteration 3290, loss = 1.19777519\n",
      "Iteration 3291, loss = 1.19775411\n",
      "Iteration 3292, loss = 1.19774116\n",
      "Iteration 3293, loss = 1.19772319\n",
      "Iteration 3294, loss = 1.19770521\n",
      "Iteration 3295, loss = 1.19769425\n",
      "Iteration 3296, loss = 1.19767291\n",
      "Iteration 3297, loss = 1.19766044\n",
      "Iteration 3298, loss = 1.19765037\n",
      "Iteration 3299, loss = 1.19762589\n",
      "Iteration 3300, loss = 1.19761052\n",
      "Iteration 3301, loss = 1.19759265\n",
      "Iteration 3302, loss = 1.19758005\n",
      "Iteration 3303, loss = 1.19756257\n",
      "Iteration 3304, loss = 1.19754591\n",
      "Iteration 3305, loss = 1.19752794\n",
      "Iteration 3306, loss = 1.19751069\n",
      "Iteration 3307, loss = 1.19749674\n",
      "Iteration 3308, loss = 1.19748105\n",
      "Iteration 3309, loss = 1.19746460\n",
      "Iteration 3310, loss = 1.19744744\n",
      "Iteration 3311, loss = 1.19743348\n",
      "Iteration 3312, loss = 1.19741360\n",
      "Iteration 3313, loss = 1.19739820\n",
      "Iteration 3314, loss = 1.19738184\n",
      "Iteration 3315, loss = 1.19736446\n",
      "Iteration 3316, loss = 1.19734900\n",
      "Iteration 3317, loss = 1.19733307\n",
      "Iteration 3318, loss = 1.19731682\n",
      "Iteration 3319, loss = 1.19730108\n",
      "Iteration 3320, loss = 1.19728837\n",
      "Iteration 3321, loss = 1.19726876\n",
      "Iteration 3322, loss = 1.19725717\n",
      "Iteration 3323, loss = 1.19723758\n",
      "Iteration 3324, loss = 1.19721999\n",
      "Iteration 3325, loss = 1.19720461\n",
      "Iteration 3326, loss = 1.19718894\n",
      "Iteration 3327, loss = 1.19717694\n",
      "Iteration 3328, loss = 1.19715675\n",
      "Iteration 3329, loss = 1.19714060\n",
      "Iteration 3330, loss = 1.19712404\n",
      "Iteration 3331, loss = 1.19711011\n",
      "Iteration 3332, loss = 1.19709176\n",
      "Iteration 3333, loss = 1.19707714\n",
      "Iteration 3334, loss = 1.19706000\n",
      "Iteration 3335, loss = 1.19704274\n",
      "Iteration 3336, loss = 1.19702769\n",
      "Iteration 3337, loss = 1.19701053\n",
      "Iteration 3338, loss = 1.19699553\n",
      "Iteration 3339, loss = 1.19698068\n",
      "Iteration 3340, loss = 1.19696259\n",
      "Iteration 3341, loss = 1.19695368\n",
      "Iteration 3342, loss = 1.19693102\n",
      "Iteration 3343, loss = 1.19691670\n",
      "Iteration 3344, loss = 1.19689951\n",
      "Iteration 3345, loss = 1.19688553\n",
      "Iteration 3346, loss = 1.19687128\n",
      "Iteration 3347, loss = 1.19685524\n",
      "Iteration 3348, loss = 1.19683501\n",
      "Iteration 3349, loss = 1.19681973\n",
      "Iteration 3350, loss = 1.19680155\n",
      "Iteration 3351, loss = 1.19678517\n",
      "Iteration 3352, loss = 1.19677033\n",
      "Iteration 3353, loss = 1.19676001\n",
      "Iteration 3354, loss = 1.19674250\n",
      "Iteration 3355, loss = 1.19672385\n",
      "Iteration 3356, loss = 1.19671028\n",
      "Iteration 3357, loss = 1.19669789\n",
      "Iteration 3358, loss = 1.19667458\n",
      "Iteration 3359, loss = 1.19665968\n",
      "Iteration 3360, loss = 1.19664263\n",
      "Iteration 3361, loss = 1.19662817\n",
      "Iteration 3362, loss = 1.19661229\n",
      "Iteration 3363, loss = 1.19659681\n",
      "Iteration 3364, loss = 1.19658255\n",
      "Iteration 3365, loss = 1.19656606\n",
      "Iteration 3366, loss = 1.19655165\n",
      "Iteration 3367, loss = 1.19653243\n",
      "Iteration 3368, loss = 1.19651856\n",
      "Iteration 3369, loss = 1.19650007\n",
      "Iteration 3370, loss = 1.19648488\n",
      "Iteration 3371, loss = 1.19646925\n",
      "Iteration 3372, loss = 1.19645345\n",
      "Iteration 3373, loss = 1.19643863\n",
      "Iteration 3374, loss = 1.19642275\n",
      "Iteration 3375, loss = 1.19640770\n",
      "Iteration 3376, loss = 1.19639093\n",
      "Iteration 3377, loss = 1.19637491\n",
      "Iteration 3378, loss = 1.19636275\n",
      "Iteration 3379, loss = 1.19634348\n",
      "Iteration 3380, loss = 1.19632721\n",
      "Iteration 3381, loss = 1.19631180\n",
      "Iteration 3382, loss = 1.19629771\n",
      "Iteration 3383, loss = 1.19628195\n",
      "Iteration 3384, loss = 1.19626653\n",
      "Iteration 3385, loss = 1.19625058\n",
      "Iteration 3386, loss = 1.19623670\n",
      "Iteration 3387, loss = 1.19622127\n",
      "Iteration 3388, loss = 1.19620333\n",
      "Iteration 3389, loss = 1.19618662\n",
      "Iteration 3390, loss = 1.19617577\n",
      "Iteration 3391, loss = 1.19615606\n",
      "Iteration 3392, loss = 1.19613976\n",
      "Iteration 3393, loss = 1.19612348\n",
      "Iteration 3394, loss = 1.19610728\n",
      "Iteration 3395, loss = 1.19609179\n",
      "Iteration 3396, loss = 1.19607921\n",
      "Iteration 3397, loss = 1.19606377\n",
      "Iteration 3398, loss = 1.19604626\n",
      "Iteration 3399, loss = 1.19602887\n",
      "Iteration 3400, loss = 1.19601351\n",
      "Iteration 3401, loss = 1.19599831\n",
      "Iteration 3402, loss = 1.19598261\n",
      "Iteration 3403, loss = 1.19596585\n",
      "Iteration 3404, loss = 1.19595063\n",
      "Iteration 3405, loss = 1.19593500\n",
      "Iteration 3406, loss = 1.19592070\n",
      "Iteration 3407, loss = 1.19590386\n",
      "Iteration 3408, loss = 1.19588766\n",
      "Iteration 3409, loss = 1.19587391\n",
      "Iteration 3410, loss = 1.19585769\n",
      "Iteration 3411, loss = 1.19584377\n",
      "Iteration 3412, loss = 1.19582583\n",
      "Iteration 3413, loss = 1.19580989\n",
      "Iteration 3414, loss = 1.19579681\n",
      "Iteration 3415, loss = 1.19578365\n",
      "Iteration 3416, loss = 1.19576349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3417, loss = 1.19574732\n",
      "Iteration 3418, loss = 1.19573149\n",
      "Iteration 3419, loss = 1.19571742\n",
      "Iteration 3420, loss = 1.19570092\n",
      "Iteration 3421, loss = 1.19568648\n",
      "Iteration 3422, loss = 1.19566990\n",
      "Iteration 3423, loss = 1.19565728\n",
      "Iteration 3424, loss = 1.19563945\n",
      "Iteration 3425, loss = 1.19562299\n",
      "Iteration 3426, loss = 1.19560860\n",
      "Iteration 3427, loss = 1.19559146\n",
      "Iteration 3428, loss = 1.19557877\n",
      "Iteration 3429, loss = 1.19556197\n",
      "Iteration 3430, loss = 1.19554688\n",
      "Iteration 3431, loss = 1.19553004\n",
      "Iteration 3432, loss = 1.19551575\n",
      "Iteration 3433, loss = 1.19550302\n",
      "Iteration 3434, loss = 1.19548419\n",
      "Iteration 3435, loss = 1.19546906\n",
      "Iteration 3436, loss = 1.19546257\n",
      "Iteration 3437, loss = 1.19544271\n",
      "Iteration 3438, loss = 1.19542480\n",
      "Iteration 3439, loss = 1.19541363\n",
      "Iteration 3440, loss = 1.19539323\n",
      "Iteration 3441, loss = 1.19537860\n",
      "Iteration 3442, loss = 1.19536037\n",
      "Iteration 3443, loss = 1.19534695\n",
      "Iteration 3444, loss = 1.19532999\n",
      "Iteration 3445, loss = 1.19531613\n",
      "Iteration 3446, loss = 1.19530010\n",
      "Iteration 3447, loss = 1.19528450\n",
      "Iteration 3448, loss = 1.19527124\n",
      "Iteration 3449, loss = 1.19525305\n",
      "Iteration 3450, loss = 1.19523863\n",
      "Iteration 3451, loss = 1.19522253\n",
      "Iteration 3452, loss = 1.19521111\n",
      "Iteration 3453, loss = 1.19519265\n",
      "Iteration 3454, loss = 1.19517592\n",
      "Iteration 3455, loss = 1.19516156\n",
      "Iteration 3456, loss = 1.19514652\n",
      "Iteration 3457, loss = 1.19513074\n",
      "Iteration 3458, loss = 1.19511595\n",
      "Iteration 3459, loss = 1.19510020\n",
      "Iteration 3460, loss = 1.19508670\n",
      "Iteration 3461, loss = 1.19506917\n",
      "Iteration 3462, loss = 1.19506433\n",
      "Iteration 3463, loss = 1.19503762\n",
      "Iteration 3464, loss = 1.19502359\n",
      "Iteration 3465, loss = 1.19501338\n",
      "Iteration 3466, loss = 1.19499311\n",
      "Iteration 3467, loss = 1.19497834\n",
      "Iteration 3468, loss = 1.19496447\n",
      "Iteration 3469, loss = 1.19494999\n",
      "Iteration 3470, loss = 1.19493102\n",
      "Iteration 3471, loss = 1.19491852\n",
      "Iteration 3472, loss = 1.19490489\n",
      "Iteration 3473, loss = 1.19488606\n",
      "Iteration 3474, loss = 1.19487117\n",
      "Iteration 3475, loss = 1.19485413\n",
      "Iteration 3476, loss = 1.19484346\n",
      "Iteration 3477, loss = 1.19482412\n",
      "Iteration 3478, loss = 1.19481172\n",
      "Iteration 3479, loss = 1.19479465\n",
      "Iteration 3480, loss = 1.19478323\n",
      "Iteration 3481, loss = 1.19476455\n",
      "Iteration 3482, loss = 1.19474904\n",
      "Iteration 3483, loss = 1.19473339\n",
      "Iteration 3484, loss = 1.19471921\n",
      "Iteration 3485, loss = 1.19470275\n",
      "Iteration 3486, loss = 1.19468731\n",
      "Iteration 3487, loss = 1.19467291\n",
      "Iteration 3488, loss = 1.19465763\n",
      "Iteration 3489, loss = 1.19464182\n",
      "Iteration 3490, loss = 1.19462739\n",
      "Iteration 3491, loss = 1.19461122\n",
      "Iteration 3492, loss = 1.19459845\n",
      "Iteration 3493, loss = 1.19458277\n",
      "Iteration 3494, loss = 1.19456793\n",
      "Iteration 3495, loss = 1.19455150\n",
      "Iteration 3496, loss = 1.19453868\n",
      "Iteration 3497, loss = 1.19452096\n",
      "Iteration 3498, loss = 1.19450626\n",
      "Iteration 3499, loss = 1.19449046\n",
      "Iteration 3500, loss = 1.19447602\n",
      "Iteration 3501, loss = 1.19446256\n",
      "Iteration 3502, loss = 1.19444512\n",
      "Iteration 3503, loss = 1.19442968\n",
      "Iteration 3504, loss = 1.19441459\n",
      "Iteration 3505, loss = 1.19440162\n",
      "Iteration 3506, loss = 1.19438759\n",
      "Iteration 3507, loss = 1.19437368\n",
      "Iteration 3508, loss = 1.19435493\n",
      "Iteration 3509, loss = 1.19433900\n",
      "Iteration 3510, loss = 1.19432740\n",
      "Iteration 3511, loss = 1.19431168\n",
      "Iteration 3512, loss = 1.19429630\n",
      "Iteration 3513, loss = 1.19427902\n",
      "Iteration 3514, loss = 1.19426456\n",
      "Iteration 3515, loss = 1.19424921\n",
      "Iteration 3516, loss = 1.19423709\n",
      "Iteration 3517, loss = 1.19421876\n",
      "Iteration 3518, loss = 1.19420467\n",
      "Iteration 3519, loss = 1.19418915\n",
      "Iteration 3520, loss = 1.19417432\n",
      "Iteration 3521, loss = 1.19415924\n",
      "Iteration 3522, loss = 1.19414361\n",
      "Iteration 3523, loss = 1.19413164\n",
      "Iteration 3524, loss = 1.19411668\n",
      "Iteration 3525, loss = 1.19409951\n",
      "Iteration 3526, loss = 1.19408524\n",
      "Iteration 3527, loss = 1.19406895\n",
      "Iteration 3528, loss = 1.19405452\n",
      "Iteration 3529, loss = 1.19404702\n",
      "Iteration 3530, loss = 1.19402519\n",
      "Iteration 3531, loss = 1.19401146\n",
      "Iteration 3532, loss = 1.19399839\n",
      "Iteration 3533, loss = 1.19397987\n",
      "Iteration 3534, loss = 1.19396564\n",
      "Iteration 3535, loss = 1.19395131\n",
      "Iteration 3536, loss = 1.19393761\n",
      "Iteration 3537, loss = 1.19392107\n",
      "Iteration 3538, loss = 1.19390732\n",
      "Iteration 3539, loss = 1.19389066\n",
      "Iteration 3540, loss = 1.19387683\n",
      "Iteration 3541, loss = 1.19386182\n",
      "Iteration 3542, loss = 1.19384689\n",
      "Iteration 3543, loss = 1.19383207\n",
      "Iteration 3544, loss = 1.19381861\n",
      "Iteration 3545, loss = 1.19380761\n",
      "Iteration 3546, loss = 1.19378690\n",
      "Iteration 3547, loss = 1.19377221\n",
      "Iteration 3548, loss = 1.19375932\n",
      "Iteration 3549, loss = 1.19374144\n",
      "Iteration 3550, loss = 1.19372802\n",
      "Iteration 3551, loss = 1.19371174\n",
      "Iteration 3552, loss = 1.19369788\n",
      "Iteration 3553, loss = 1.19368275\n",
      "Iteration 3554, loss = 1.19366822\n",
      "Iteration 3555, loss = 1.19365598\n",
      "Iteration 3556, loss = 1.19363803\n",
      "Iteration 3557, loss = 1.19362294\n",
      "Iteration 3558, loss = 1.19360851\n",
      "Iteration 3559, loss = 1.19359330\n",
      "Iteration 3560, loss = 1.19357862\n",
      "Iteration 3561, loss = 1.19356662\n",
      "Iteration 3562, loss = 1.19354991\n",
      "Iteration 3563, loss = 1.19353418\n",
      "Iteration 3564, loss = 1.19351939\n",
      "Iteration 3565, loss = 1.19350552\n",
      "Iteration 3566, loss = 1.19348936\n",
      "Iteration 3567, loss = 1.19347543\n",
      "Iteration 3568, loss = 1.19346106\n",
      "Iteration 3569, loss = 1.19344643\n",
      "Iteration 3570, loss = 1.19343066\n",
      "Iteration 3571, loss = 1.19341817\n",
      "Iteration 3572, loss = 1.19340054\n",
      "Iteration 3573, loss = 1.19338884\n",
      "Iteration 3574, loss = 1.19337598\n",
      "Iteration 3575, loss = 1.19335792\n",
      "Iteration 3576, loss = 1.19334220\n",
      "Iteration 3577, loss = 1.19332786\n",
      "Iteration 3578, loss = 1.19331402\n",
      "Iteration 3579, loss = 1.19330070\n",
      "Iteration 3580, loss = 1.19328358\n",
      "Iteration 3581, loss = 1.19327003\n",
      "Iteration 3582, loss = 1.19325506\n",
      "Iteration 3583, loss = 1.19323941\n",
      "Iteration 3584, loss = 1.19322633\n",
      "Iteration 3585, loss = 1.19321287\n",
      "Iteration 3586, loss = 1.19319697\n",
      "Iteration 3587, loss = 1.19318131\n",
      "Iteration 3588, loss = 1.19316585\n",
      "Iteration 3589, loss = 1.19315145\n",
      "Iteration 3590, loss = 1.19313708\n",
      "Iteration 3591, loss = 1.19312192\n",
      "Iteration 3592, loss = 1.19310930\n",
      "Iteration 3593, loss = 1.19309287\n",
      "Iteration 3594, loss = 1.19307954\n",
      "Iteration 3595, loss = 1.19306329\n",
      "Iteration 3596, loss = 1.19305210\n",
      "Iteration 3597, loss = 1.19303919\n",
      "Iteration 3598, loss = 1.19302171\n",
      "Iteration 3599, loss = 1.19300758\n",
      "Iteration 3600, loss = 1.19299271\n",
      "Iteration 3601, loss = 1.19297761\n",
      "Iteration 3602, loss = 1.19296385\n",
      "Iteration 3603, loss = 1.19294664\n",
      "Iteration 3604, loss = 1.19293310\n",
      "Iteration 3605, loss = 1.19291789\n",
      "Iteration 3606, loss = 1.19290363\n",
      "Iteration 3607, loss = 1.19288817\n",
      "Iteration 3608, loss = 1.19287365\n",
      "Iteration 3609, loss = 1.19286032\n",
      "Iteration 3610, loss = 1.19284586\n",
      "Iteration 3611, loss = 1.19283100\n",
      "Iteration 3612, loss = 1.19281665\n",
      "Iteration 3613, loss = 1.19280116\n",
      "Iteration 3614, loss = 1.19278759\n",
      "Iteration 3615, loss = 1.19277101\n",
      "Iteration 3616, loss = 1.19275839\n",
      "Iteration 3617, loss = 1.19274580\n",
      "Iteration 3618, loss = 1.19272978\n",
      "Iteration 3619, loss = 1.19271418\n",
      "Iteration 3620, loss = 1.19269840\n",
      "Iteration 3621, loss = 1.19268475\n",
      "Iteration 3622, loss = 1.19267144\n",
      "Iteration 3623, loss = 1.19265789\n",
      "Iteration 3624, loss = 1.19264158\n",
      "Iteration 3625, loss = 1.19262985\n",
      "Iteration 3626, loss = 1.19261198\n",
      "Iteration 3627, loss = 1.19260118\n",
      "Iteration 3628, loss = 1.19258412\n",
      "Iteration 3629, loss = 1.19257368\n",
      "Iteration 3630, loss = 1.19255445\n",
      "Iteration 3631, loss = 1.19253986\n",
      "Iteration 3632, loss = 1.19252532\n",
      "Iteration 3633, loss = 1.19251241\n",
      "Iteration 3634, loss = 1.19249940\n",
      "Iteration 3635, loss = 1.19248589\n",
      "Iteration 3636, loss = 1.19246722\n",
      "Iteration 3637, loss = 1.19245632\n",
      "Iteration 3638, loss = 1.19244198\n",
      "Iteration 3639, loss = 1.19242715\n",
      "Iteration 3640, loss = 1.19241549\n",
      "Iteration 3641, loss = 1.19239499\n",
      "Iteration 3642, loss = 1.19238115\n",
      "Iteration 3643, loss = 1.19236777\n",
      "Iteration 3644, loss = 1.19235278\n",
      "Iteration 3645, loss = 1.19233837\n",
      "Iteration 3646, loss = 1.19232681\n",
      "Iteration 3647, loss = 1.19230922\n",
      "Iteration 3648, loss = 1.19229515\n",
      "Iteration 3649, loss = 1.19228562\n",
      "Iteration 3650, loss = 1.19227194\n",
      "Iteration 3651, loss = 1.19225396\n",
      "Iteration 3652, loss = 1.19223833\n",
      "Iteration 3653, loss = 1.19222331\n",
      "Iteration 3654, loss = 1.19221218\n",
      "Iteration 3655, loss = 1.19219530\n",
      "Iteration 3656, loss = 1.19218081\n",
      "Iteration 3657, loss = 1.19216701\n",
      "Iteration 3658, loss = 1.19215635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3659, loss = 1.19214030\n",
      "Iteration 3660, loss = 1.19212345\n",
      "Iteration 3661, loss = 1.19210779\n",
      "Iteration 3662, loss = 1.19209655\n",
      "Iteration 3663, loss = 1.19207990\n",
      "Iteration 3664, loss = 1.19206964\n",
      "Iteration 3665, loss = 1.19205267\n",
      "Iteration 3666, loss = 1.19203841\n",
      "Iteration 3667, loss = 1.19202387\n",
      "Iteration 3668, loss = 1.19201116\n",
      "Iteration 3669, loss = 1.19199708\n",
      "Iteration 3670, loss = 1.19198125\n",
      "Iteration 3671, loss = 1.19197613\n",
      "Iteration 3672, loss = 1.19195505\n",
      "Iteration 3673, loss = 1.19193987\n",
      "Iteration 3674, loss = 1.19192846\n",
      "Iteration 3675, loss = 1.19191073\n",
      "Iteration 3676, loss = 1.19189782\n",
      "Iteration 3677, loss = 1.19188097\n",
      "Iteration 3678, loss = 1.19187027\n",
      "Iteration 3679, loss = 1.19185261\n",
      "Iteration 3680, loss = 1.19183965\n",
      "Iteration 3681, loss = 1.19182778\n",
      "Iteration 3682, loss = 1.19180883\n",
      "Iteration 3683, loss = 1.19179860\n",
      "Iteration 3684, loss = 1.19178775\n",
      "Iteration 3685, loss = 1.19177074\n",
      "Iteration 3686, loss = 1.19175491\n",
      "Iteration 3687, loss = 1.19173856\n",
      "Iteration 3688, loss = 1.19172529\n",
      "Iteration 3689, loss = 1.19170974\n",
      "Iteration 3690, loss = 1.19169634\n",
      "Iteration 3691, loss = 1.19168466\n",
      "Iteration 3692, loss = 1.19166822\n",
      "Iteration 3693, loss = 1.19165416\n",
      "Iteration 3694, loss = 1.19163961\n",
      "Iteration 3695, loss = 1.19162521\n",
      "Iteration 3696, loss = 1.19161201\n",
      "Iteration 3697, loss = 1.19159741\n",
      "Iteration 3698, loss = 1.19158247\n",
      "Iteration 3699, loss = 1.19156848\n",
      "Iteration 3700, loss = 1.19155453\n",
      "Iteration 3701, loss = 1.19153972\n",
      "Iteration 3702, loss = 1.19152758\n",
      "Iteration 3703, loss = 1.19151078\n",
      "Iteration 3704, loss = 1.19149881\n",
      "Iteration 3705, loss = 1.19148349\n",
      "Iteration 3706, loss = 1.19147122\n",
      "Iteration 3707, loss = 1.19145812\n",
      "Iteration 3708, loss = 1.19144183\n",
      "Iteration 3709, loss = 1.19143469\n",
      "Iteration 3710, loss = 1.19141356\n",
      "Iteration 3711, loss = 1.19140171\n",
      "Iteration 3712, loss = 1.19138570\n",
      "Iteration 3713, loss = 1.19137306\n",
      "Iteration 3714, loss = 1.19135918\n",
      "Iteration 3715, loss = 1.19135500\n",
      "Iteration 3716, loss = 1.19132962\n",
      "Iteration 3717, loss = 1.19131591\n",
      "Iteration 3718, loss = 1.19130015\n",
      "Iteration 3719, loss = 1.19129038\n",
      "Iteration 3720, loss = 1.19127285\n",
      "Iteration 3721, loss = 1.19125930\n",
      "Iteration 3722, loss = 1.19124423\n",
      "Iteration 3723, loss = 1.19123013\n",
      "Iteration 3724, loss = 1.19121898\n",
      "Iteration 3725, loss = 1.19120217\n",
      "Iteration 3726, loss = 1.19118766\n",
      "Iteration 3727, loss = 1.19117352\n",
      "Iteration 3728, loss = 1.19115985\n",
      "Iteration 3729, loss = 1.19114517\n",
      "Iteration 3730, loss = 1.19113320\n",
      "Iteration 3731, loss = 1.19111948\n",
      "Iteration 3732, loss = 1.19110484\n",
      "Iteration 3733, loss = 1.19109257\n",
      "Iteration 3734, loss = 1.19107584\n",
      "Iteration 3735, loss = 1.19106333\n",
      "Iteration 3736, loss = 1.19105073\n",
      "Iteration 3737, loss = 1.19103421\n",
      "Iteration 3738, loss = 1.19102058\n",
      "Iteration 3739, loss = 1.19100847\n",
      "Iteration 3740, loss = 1.19099453\n",
      "Iteration 3741, loss = 1.19098196\n",
      "Iteration 3742, loss = 1.19096670\n",
      "Iteration 3743, loss = 1.19095253\n",
      "Iteration 3744, loss = 1.19093686\n",
      "Iteration 3745, loss = 1.19092472\n",
      "Iteration 3746, loss = 1.19090830\n",
      "Iteration 3747, loss = 1.19089411\n",
      "Iteration 3748, loss = 1.19087984\n",
      "Iteration 3749, loss = 1.19086534\n",
      "Iteration 3750, loss = 1.19085441\n",
      "Iteration 3751, loss = 1.19083864\n",
      "Iteration 3752, loss = 1.19082499\n",
      "Iteration 3753, loss = 1.19080994\n",
      "Iteration 3754, loss = 1.19079538\n",
      "Iteration 3755, loss = 1.19078959\n",
      "Iteration 3756, loss = 1.19077064\n",
      "Iteration 3757, loss = 1.19075481\n",
      "Iteration 3758, loss = 1.19073943\n",
      "Iteration 3759, loss = 1.19072937\n",
      "Iteration 3760, loss = 1.19071290\n",
      "Iteration 3761, loss = 1.19069940\n",
      "Iteration 3762, loss = 1.19068631\n",
      "Iteration 3763, loss = 1.19067065\n",
      "Iteration 3764, loss = 1.19065852\n",
      "Iteration 3765, loss = 1.19064655\n",
      "Iteration 3766, loss = 1.19063207\n",
      "Iteration 3767, loss = 1.19061636\n",
      "Iteration 3768, loss = 1.19060394\n",
      "Iteration 3769, loss = 1.19058865\n",
      "Iteration 3770, loss = 1.19057403\n",
      "Iteration 3771, loss = 1.19056217\n",
      "Iteration 3772, loss = 1.19054590\n",
      "Iteration 3773, loss = 1.19053356\n",
      "Iteration 3774, loss = 1.19051865\n",
      "Iteration 3775, loss = 1.19050323\n",
      "Iteration 3776, loss = 1.19049070\n",
      "Iteration 3777, loss = 1.19047792\n",
      "Iteration 3778, loss = 1.19046623\n",
      "Iteration 3779, loss = 1.19044854\n",
      "Iteration 3780, loss = 1.19043803\n",
      "Iteration 3781, loss = 1.19042185\n",
      "Iteration 3782, loss = 1.19040931\n",
      "Iteration 3783, loss = 1.19039346\n",
      "Iteration 3784, loss = 1.19037975\n",
      "Iteration 3785, loss = 1.19037188\n",
      "Iteration 3786, loss = 1.19035207\n",
      "Iteration 3787, loss = 1.19033838\n",
      "Iteration 3788, loss = 1.19032611\n",
      "Iteration 3789, loss = 1.19031474\n",
      "Iteration 3790, loss = 1.19029772\n",
      "Iteration 3791, loss = 1.19028276\n",
      "Iteration 3792, loss = 1.19027363\n",
      "Iteration 3793, loss = 1.19025541\n",
      "Iteration 3794, loss = 1.19024572\n",
      "Iteration 3795, loss = 1.19022959\n",
      "Iteration 3796, loss = 1.19021617\n",
      "Iteration 3797, loss = 1.19020106\n",
      "Iteration 3798, loss = 1.19018886\n",
      "Iteration 3799, loss = 1.19017383\n",
      "Iteration 3800, loss = 1.19015862\n",
      "Iteration 3801, loss = 1.19014627\n",
      "Iteration 3802, loss = 1.19013348\n",
      "Iteration 3803, loss = 1.19011938\n",
      "Iteration 3804, loss = 1.19010795\n",
      "Iteration 3805, loss = 1.19009378\n",
      "Iteration 3806, loss = 1.19007819\n",
      "Iteration 3807, loss = 1.19006354\n",
      "Iteration 3808, loss = 1.19004947\n",
      "Iteration 3809, loss = 1.19003542\n",
      "Iteration 3810, loss = 1.19002143\n",
      "Iteration 3811, loss = 1.19000810\n",
      "Iteration 3812, loss = 1.18999642\n",
      "Iteration 3813, loss = 1.18998104\n",
      "Iteration 3814, loss = 1.18996676\n",
      "Iteration 3815, loss = 1.18995294\n",
      "Iteration 3816, loss = 1.18994007\n",
      "Iteration 3817, loss = 1.18992549\n",
      "Iteration 3818, loss = 1.18991222\n",
      "Iteration 3819, loss = 1.18989690\n",
      "Iteration 3820, loss = 1.18988687\n",
      "Iteration 3821, loss = 1.18987106\n",
      "Iteration 3822, loss = 1.18985737\n",
      "Iteration 3823, loss = 1.18984412\n",
      "Iteration 3824, loss = 1.18983033\n",
      "Iteration 3825, loss = 1.18981550\n",
      "Iteration 3826, loss = 1.18980332\n",
      "Iteration 3827, loss = 1.18978730\n",
      "Iteration 3828, loss = 1.18977676\n",
      "Iteration 3829, loss = 1.18976087\n",
      "Iteration 3830, loss = 1.18974741\n",
      "Iteration 3831, loss = 1.18973484\n",
      "Iteration 3832, loss = 1.18971932\n",
      "Iteration 3833, loss = 1.18970538\n",
      "Iteration 3834, loss = 1.18969286\n",
      "Iteration 3835, loss = 1.18968442\n",
      "Iteration 3836, loss = 1.18966828\n",
      "Iteration 3837, loss = 1.18965581\n",
      "Iteration 3838, loss = 1.18964105\n",
      "Iteration 3839, loss = 1.18962363\n",
      "Iteration 3840, loss = 1.18961061\n",
      "Iteration 3841, loss = 1.18959643\n",
      "Iteration 3842, loss = 1.18958249\n",
      "Iteration 3843, loss = 1.18957121\n",
      "Iteration 3844, loss = 1.18955744\n",
      "Iteration 3845, loss = 1.18954483\n",
      "Iteration 3846, loss = 1.18952816\n",
      "Iteration 3847, loss = 1.18951522\n",
      "Iteration 3848, loss = 1.18950229\n",
      "Iteration 3849, loss = 1.18949302\n",
      "Iteration 3850, loss = 1.18947701\n",
      "Iteration 3851, loss = 1.18946269\n",
      "Iteration 3852, loss = 1.18944953\n",
      "Iteration 3853, loss = 1.18943431\n",
      "Iteration 3854, loss = 1.18942276\n",
      "Iteration 3855, loss = 1.18940724\n",
      "Iteration 3856, loss = 1.18939351\n",
      "Iteration 3857, loss = 1.18937973\n",
      "Iteration 3858, loss = 1.18937141\n",
      "Iteration 3859, loss = 1.18935721\n",
      "Iteration 3860, loss = 1.18933960\n",
      "Iteration 3861, loss = 1.18932599\n",
      "Iteration 3862, loss = 1.18931168\n",
      "Iteration 3863, loss = 1.18929937\n",
      "Iteration 3864, loss = 1.18928474\n",
      "Iteration 3865, loss = 1.18927028\n",
      "Iteration 3866, loss = 1.18925788\n",
      "Iteration 3867, loss = 1.18924370\n",
      "Iteration 3868, loss = 1.18923277\n",
      "Iteration 3869, loss = 1.18921674\n",
      "Iteration 3870, loss = 1.18920473\n",
      "Iteration 3871, loss = 1.18919017\n",
      "Iteration 3872, loss = 1.18917794\n",
      "Iteration 3873, loss = 1.18916797\n",
      "Iteration 3874, loss = 1.18915004\n",
      "Iteration 3875, loss = 1.18913593\n",
      "Iteration 3876, loss = 1.18912617\n",
      "Iteration 3877, loss = 1.18910967\n",
      "Iteration 3878, loss = 1.18909744\n",
      "Iteration 3879, loss = 1.18908547\n",
      "Iteration 3880, loss = 1.18907628\n",
      "Iteration 3881, loss = 1.18906246\n",
      "Iteration 3882, loss = 1.18904248\n",
      "Iteration 3883, loss = 1.18902739\n",
      "Iteration 3884, loss = 1.18901660\n",
      "Iteration 3885, loss = 1.18900157\n",
      "Iteration 3886, loss = 1.18898872\n",
      "Iteration 3887, loss = 1.18897593\n",
      "Iteration 3888, loss = 1.18896201\n",
      "Iteration 3889, loss = 1.18894856\n",
      "Iteration 3890, loss = 1.18893416\n",
      "Iteration 3891, loss = 1.18892357\n",
      "Iteration 3892, loss = 1.18890781\n",
      "Iteration 3893, loss = 1.18889423\n",
      "Iteration 3894, loss = 1.18887860\n",
      "Iteration 3895, loss = 1.18886618\n",
      "Iteration 3896, loss = 1.18885517\n",
      "Iteration 3897, loss = 1.18884129\n",
      "Iteration 3898, loss = 1.18882825\n",
      "Iteration 3899, loss = 1.18881583\n",
      "Iteration 3900, loss = 1.18880056\n",
      "Iteration 3901, loss = 1.18878705\n",
      "Iteration 3902, loss = 1.18877513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3903, loss = 1.18875977\n",
      "Iteration 3904, loss = 1.18874565\n",
      "Iteration 3905, loss = 1.18873408\n",
      "Iteration 3906, loss = 1.18871970\n",
      "Iteration 3907, loss = 1.18870505\n",
      "Iteration 3908, loss = 1.18869130\n",
      "Iteration 3909, loss = 1.18867872\n",
      "Iteration 3910, loss = 1.18866520\n",
      "Iteration 3911, loss = 1.18865159\n",
      "Iteration 3912, loss = 1.18863874\n",
      "Iteration 3913, loss = 1.18863128\n",
      "Iteration 3914, loss = 1.18861073\n",
      "Iteration 3915, loss = 1.18859716\n",
      "Iteration 3916, loss = 1.18858836\n",
      "Iteration 3917, loss = 1.18857138\n",
      "Iteration 3918, loss = 1.18855949\n",
      "Iteration 3919, loss = 1.18854380\n",
      "Iteration 3920, loss = 1.18853328\n",
      "Iteration 3921, loss = 1.18852347\n",
      "Iteration 3922, loss = 1.18850340\n",
      "Iteration 3923, loss = 1.18848975\n",
      "Iteration 3924, loss = 1.18847693\n",
      "Iteration 3925, loss = 1.18846664\n",
      "Iteration 3926, loss = 1.18845094\n",
      "Iteration 3927, loss = 1.18844343\n",
      "Iteration 3928, loss = 1.18842449\n",
      "Iteration 3929, loss = 1.18841239\n",
      "Iteration 3930, loss = 1.18839792\n",
      "Iteration 3931, loss = 1.18838748\n",
      "Iteration 3932, loss = 1.18837199\n",
      "Iteration 3933, loss = 1.18835848\n",
      "Iteration 3934, loss = 1.18834410\n",
      "Iteration 3935, loss = 1.18833348\n",
      "Iteration 3936, loss = 1.18831717\n",
      "Iteration 3937, loss = 1.18830331\n",
      "Iteration 3938, loss = 1.18828981\n",
      "Iteration 3939, loss = 1.18827663\n",
      "Iteration 3940, loss = 1.18826895\n",
      "Iteration 3941, loss = 1.18825034\n",
      "Iteration 3942, loss = 1.18824185\n",
      "Iteration 3943, loss = 1.18822466\n",
      "Iteration 3944, loss = 1.18821099\n",
      "Iteration 3945, loss = 1.18819749\n",
      "Iteration 3946, loss = 1.18818632\n",
      "Iteration 3947, loss = 1.18817033\n",
      "Iteration 3948, loss = 1.18815846\n",
      "Iteration 3949, loss = 1.18814528\n",
      "Iteration 3950, loss = 1.18813203\n",
      "Iteration 3951, loss = 1.18812092\n",
      "Iteration 3952, loss = 1.18810854\n",
      "Iteration 3953, loss = 1.18809194\n",
      "Iteration 3954, loss = 1.18807925\n",
      "Iteration 3955, loss = 1.18806663\n",
      "Iteration 3956, loss = 1.18805236\n",
      "Iteration 3957, loss = 1.18803925\n",
      "Iteration 3958, loss = 1.18802711\n",
      "Iteration 3959, loss = 1.18801541\n",
      "Iteration 3960, loss = 1.18800134\n",
      "Iteration 3961, loss = 1.18798597\n",
      "Iteration 3962, loss = 1.18797319\n",
      "Iteration 3963, loss = 1.18796102\n",
      "Iteration 3964, loss = 1.18794558\n",
      "Iteration 3965, loss = 1.18793267\n",
      "Iteration 3966, loss = 1.18792011\n",
      "Iteration 3967, loss = 1.18790661\n",
      "Iteration 3968, loss = 1.18789728\n",
      "Iteration 3969, loss = 1.18788392\n",
      "Iteration 3970, loss = 1.18786678\n",
      "Iteration 3971, loss = 1.18785388\n",
      "Iteration 3972, loss = 1.18784087\n",
      "Iteration 3973, loss = 1.18782824\n",
      "Iteration 3974, loss = 1.18781398\n",
      "Iteration 3975, loss = 1.18780166\n",
      "Iteration 3976, loss = 1.18779083\n",
      "Iteration 3977, loss = 1.18777608\n",
      "Iteration 3978, loss = 1.18776288\n",
      "Iteration 3979, loss = 1.18775397\n",
      "Iteration 3980, loss = 1.18774212\n",
      "Iteration 3981, loss = 1.18772196\n",
      "Iteration 3982, loss = 1.18770989\n",
      "Iteration 3983, loss = 1.18769907\n",
      "Iteration 3984, loss = 1.18768307\n",
      "Iteration 3985, loss = 1.18767293\n",
      "Iteration 3986, loss = 1.18765661\n",
      "Iteration 3987, loss = 1.18764465\n",
      "Iteration 3988, loss = 1.18763391\n",
      "Iteration 3989, loss = 1.18761910\n",
      "Iteration 3990, loss = 1.18760730\n",
      "Iteration 3991, loss = 1.18759168\n",
      "Iteration 3992, loss = 1.18758185\n",
      "Iteration 3993, loss = 1.18756551\n",
      "Iteration 3994, loss = 1.18755349\n",
      "Iteration 3995, loss = 1.18753869\n",
      "Iteration 3996, loss = 1.18752749\n",
      "Iteration 3997, loss = 1.18751249\n",
      "Iteration 3998, loss = 1.18749895\n",
      "Iteration 3999, loss = 1.18748728\n",
      "Iteration 4000, loss = 1.18747495\n",
      "Iteration 4001, loss = 1.18746379\n",
      "Iteration 4002, loss = 1.18745319\n",
      "Iteration 4003, loss = 1.18743859\n",
      "Iteration 4004, loss = 1.18742156\n",
      "Iteration 4005, loss = 1.18740800\n",
      "Iteration 4006, loss = 1.18739566\n",
      "Iteration 4007, loss = 1.18738166\n",
      "Iteration 4008, loss = 1.18736889\n",
      "Iteration 4009, loss = 1.18736023\n",
      "Iteration 4010, loss = 1.18734257\n",
      "Iteration 4011, loss = 1.18733064\n",
      "Iteration 4012, loss = 1.18731691\n",
      "Iteration 4013, loss = 1.18730508\n",
      "Iteration 4014, loss = 1.18729118\n",
      "Iteration 4015, loss = 1.18727787\n",
      "Iteration 4016, loss = 1.18726414\n",
      "Iteration 4017, loss = 1.18725004\n",
      "Iteration 4018, loss = 1.18723737\n",
      "Iteration 4019, loss = 1.18722550\n",
      "Iteration 4020, loss = 1.18721052\n",
      "Iteration 4021, loss = 1.18719916\n",
      "Iteration 4022, loss = 1.18718640\n",
      "Iteration 4023, loss = 1.18717499\n",
      "Iteration 4024, loss = 1.18715939\n",
      "Iteration 4025, loss = 1.18714618\n",
      "Iteration 4026, loss = 1.18713288\n",
      "Iteration 4027, loss = 1.18711895\n",
      "Iteration 4028, loss = 1.18710834\n",
      "Iteration 4029, loss = 1.18709342\n",
      "Iteration 4030, loss = 1.18708326\n",
      "Iteration 4031, loss = 1.18707134\n",
      "Iteration 4032, loss = 1.18705395\n",
      "Iteration 4033, loss = 1.18704056\n",
      "Iteration 4034, loss = 1.18702942\n",
      "Iteration 4035, loss = 1.18701738\n",
      "Iteration 4036, loss = 1.18700979\n",
      "Iteration 4037, loss = 1.18699304\n",
      "Iteration 4038, loss = 1.18697701\n",
      "Iteration 4039, loss = 1.18696627\n",
      "Iteration 4040, loss = 1.18695140\n",
      "Iteration 4041, loss = 1.18693929\n",
      "Iteration 4042, loss = 1.18692391\n",
      "Iteration 4043, loss = 1.18691329\n",
      "Iteration 4044, loss = 1.18690029\n",
      "Iteration 4045, loss = 1.18689041\n",
      "Iteration 4046, loss = 1.18687322\n",
      "Iteration 4047, loss = 1.18686015\n",
      "Iteration 4048, loss = 1.18684962\n",
      "Iteration 4049, loss = 1.18683453\n",
      "Iteration 4050, loss = 1.18682461\n",
      "Iteration 4051, loss = 1.18680802\n",
      "Iteration 4052, loss = 1.18679896\n",
      "Iteration 4053, loss = 1.18678182\n",
      "Iteration 4054, loss = 1.18677142\n",
      "Iteration 4055, loss = 1.18675608\n",
      "Iteration 4056, loss = 1.18674333\n",
      "Iteration 4057, loss = 1.18673180\n",
      "Iteration 4058, loss = 1.18671877\n",
      "Iteration 4059, loss = 1.18670481\n",
      "Iteration 4060, loss = 1.18669162\n",
      "Iteration 4061, loss = 1.18668004\n",
      "Iteration 4062, loss = 1.18666604\n",
      "Iteration 4063, loss = 1.18665198\n",
      "Iteration 4064, loss = 1.18664061\n",
      "Iteration 4065, loss = 1.18662796\n",
      "Iteration 4066, loss = 1.18661403\n",
      "Iteration 4067, loss = 1.18660103\n",
      "Iteration 4068, loss = 1.18658739\n",
      "Iteration 4069, loss = 1.18657549\n",
      "Iteration 4070, loss = 1.18656182\n",
      "Iteration 4071, loss = 1.18654972\n",
      "Iteration 4072, loss = 1.18653765\n",
      "Iteration 4073, loss = 1.18652486\n",
      "Iteration 4074, loss = 1.18651129\n",
      "Iteration 4075, loss = 1.18649945\n",
      "Iteration 4076, loss = 1.18648646\n",
      "Iteration 4077, loss = 1.18647391\n",
      "Iteration 4078, loss = 1.18645816\n",
      "Iteration 4079, loss = 1.18644546\n",
      "Iteration 4080, loss = 1.18643388\n",
      "Iteration 4081, loss = 1.18642069\n",
      "Iteration 4082, loss = 1.18640779\n",
      "Iteration 4083, loss = 1.18639484\n",
      "Iteration 4084, loss = 1.18638135\n",
      "Iteration 4085, loss = 1.18637047\n",
      "Iteration 4086, loss = 1.18635875\n",
      "Iteration 4087, loss = 1.18634505\n",
      "Iteration 4088, loss = 1.18633391\n",
      "Iteration 4089, loss = 1.18631660\n",
      "Iteration 4090, loss = 1.18630559\n",
      "Iteration 4091, loss = 1.18629303\n",
      "Iteration 4092, loss = 1.18628244\n",
      "Iteration 4093, loss = 1.18626867\n",
      "Iteration 4094, loss = 1.18625366\n",
      "Iteration 4095, loss = 1.18624033\n",
      "Iteration 4096, loss = 1.18622992\n",
      "Iteration 4097, loss = 1.18621441\n",
      "Iteration 4098, loss = 1.18620283\n",
      "Iteration 4099, loss = 1.18618931\n",
      "Iteration 4100, loss = 1.18617577\n",
      "Iteration 4101, loss = 1.18616308\n",
      "Iteration 4102, loss = 1.18615951\n",
      "Iteration 4103, loss = 1.18613625\n",
      "Iteration 4104, loss = 1.18612495\n",
      "Iteration 4105, loss = 1.18611191\n",
      "Iteration 4106, loss = 1.18610066\n",
      "Iteration 4107, loss = 1.18609033\n",
      "Iteration 4108, loss = 1.18607421\n",
      "Iteration 4109, loss = 1.18605932\n",
      "Iteration 4110, loss = 1.18604680\n",
      "Iteration 4111, loss = 1.18603442\n",
      "Iteration 4112, loss = 1.18602225\n",
      "Iteration 4113, loss = 1.18600853\n",
      "Iteration 4114, loss = 1.18599677\n",
      "Iteration 4115, loss = 1.18598594\n",
      "Iteration 4116, loss = 1.18597152\n",
      "Iteration 4117, loss = 1.18595852\n",
      "Iteration 4118, loss = 1.18594429\n",
      "Iteration 4119, loss = 1.18593206\n",
      "Iteration 4120, loss = 1.18592202\n",
      "Iteration 4121, loss = 1.18590957\n",
      "Iteration 4122, loss = 1.18589589\n",
      "Iteration 4123, loss = 1.18588195\n",
      "Iteration 4124, loss = 1.18586745\n",
      "Iteration 4125, loss = 1.18585689\n",
      "Iteration 4126, loss = 1.18585010\n",
      "Iteration 4127, loss = 1.18582929\n",
      "Iteration 4128, loss = 1.18581941\n",
      "Iteration 4129, loss = 1.18580840\n",
      "Iteration 4130, loss = 1.18579362\n",
      "Iteration 4131, loss = 1.18578050\n",
      "Iteration 4132, loss = 1.18576793\n",
      "Iteration 4133, loss = 1.18575425\n",
      "Iteration 4134, loss = 1.18574098\n",
      "Iteration 4135, loss = 1.18573385\n",
      "Iteration 4136, loss = 1.18571764\n",
      "Iteration 4137, loss = 1.18570459\n",
      "Iteration 4138, loss = 1.18568943\n",
      "Iteration 4139, loss = 1.18567752\n",
      "Iteration 4140, loss = 1.18566567\n",
      "Iteration 4141, loss = 1.18565190\n",
      "Iteration 4142, loss = 1.18564003\n",
      "Iteration 4143, loss = 1.18562629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4144, loss = 1.18561529\n",
      "Iteration 4145, loss = 1.18560041\n",
      "Iteration 4146, loss = 1.18558828\n",
      "Iteration 4147, loss = 1.18557602\n",
      "Iteration 4148, loss = 1.18556717\n",
      "Iteration 4149, loss = 1.18554964\n",
      "Iteration 4150, loss = 1.18553801\n",
      "Iteration 4151, loss = 1.18552539\n",
      "Iteration 4152, loss = 1.18551226\n",
      "Iteration 4153, loss = 1.18549914\n",
      "Iteration 4154, loss = 1.18548553\n",
      "Iteration 4155, loss = 1.18547450\n",
      "Iteration 4156, loss = 1.18546589\n",
      "Iteration 4157, loss = 1.18544827\n",
      "Iteration 4158, loss = 1.18543691\n",
      "Iteration 4159, loss = 1.18542399\n",
      "Iteration 4160, loss = 1.18541266\n",
      "Iteration 4161, loss = 1.18540097\n",
      "Iteration 4162, loss = 1.18538487\n",
      "Iteration 4163, loss = 1.18537736\n",
      "Iteration 4164, loss = 1.18535916\n",
      "Iteration 4165, loss = 1.18534757\n",
      "Iteration 4166, loss = 1.18533716\n",
      "Iteration 4167, loss = 1.18532228\n",
      "Iteration 4168, loss = 1.18530827\n",
      "Iteration 4169, loss = 1.18529609\n",
      "Iteration 4170, loss = 1.18528290\n",
      "Iteration 4171, loss = 1.18527293\n",
      "Iteration 4172, loss = 1.18525776\n",
      "Iteration 4173, loss = 1.18524579\n",
      "Iteration 4174, loss = 1.18523371\n",
      "Iteration 4175, loss = 1.18522063\n",
      "Iteration 4176, loss = 1.18520684\n",
      "Iteration 4177, loss = 1.18519668\n",
      "Iteration 4178, loss = 1.18518148\n",
      "Iteration 4179, loss = 1.18518857\n",
      "Iteration 4180, loss = 1.18515802\n",
      "Iteration 4181, loss = 1.18514681\n",
      "Iteration 4182, loss = 1.18513538\n",
      "Iteration 4183, loss = 1.18511902\n",
      "Iteration 4184, loss = 1.18510868\n",
      "Iteration 4185, loss = 1.18509559\n",
      "Iteration 4186, loss = 1.18508325\n",
      "Iteration 4187, loss = 1.18506890\n",
      "Iteration 4188, loss = 1.18505756\n",
      "Iteration 4189, loss = 1.18504587\n",
      "Iteration 4190, loss = 1.18503241\n",
      "Iteration 4191, loss = 1.18501941\n",
      "Iteration 4192, loss = 1.18500954\n",
      "Iteration 4193, loss = 1.18499501\n",
      "Iteration 4194, loss = 1.18498305\n",
      "Iteration 4195, loss = 1.18496921\n",
      "Iteration 4196, loss = 1.18495731\n",
      "Iteration 4197, loss = 1.18494414\n",
      "Iteration 4198, loss = 1.18493133\n",
      "Iteration 4199, loss = 1.18491949\n",
      "Iteration 4200, loss = 1.18490512\n",
      "Iteration 4201, loss = 1.18489603\n",
      "Iteration 4202, loss = 1.18488121\n",
      "Iteration 4203, loss = 1.18486722\n",
      "Iteration 4204, loss = 1.18485632\n",
      "Iteration 4205, loss = 1.18484415\n",
      "Iteration 4206, loss = 1.18483156\n",
      "Iteration 4207, loss = 1.18481886\n",
      "Iteration 4208, loss = 1.18480815\n",
      "Iteration 4209, loss = 1.18479372\n",
      "Iteration 4210, loss = 1.18478071\n",
      "Iteration 4211, loss = 1.18476953\n",
      "Iteration 4212, loss = 1.18475877\n",
      "Iteration 4213, loss = 1.18474406\n",
      "Iteration 4214, loss = 1.18473603\n",
      "Iteration 4215, loss = 1.18471842\n",
      "Iteration 4216, loss = 1.18470513\n",
      "Iteration 4217, loss = 1.18469317\n",
      "Iteration 4218, loss = 1.18468036\n",
      "Iteration 4219, loss = 1.18466795\n",
      "Iteration 4220, loss = 1.18465687\n",
      "Iteration 4221, loss = 1.18464567\n",
      "Iteration 4222, loss = 1.18463290\n",
      "Iteration 4223, loss = 1.18461787\n",
      "Iteration 4224, loss = 1.18460554\n",
      "Iteration 4225, loss = 1.18459461\n",
      "Iteration 4226, loss = 1.18458443\n",
      "Iteration 4227, loss = 1.18457129\n",
      "Iteration 4228, loss = 1.18455912\n",
      "Iteration 4229, loss = 1.18454437\n",
      "Iteration 4230, loss = 1.18453187\n",
      "Iteration 4231, loss = 1.18452160\n",
      "Iteration 4232, loss = 1.18450696\n",
      "Iteration 4233, loss = 1.18449346\n",
      "Iteration 4234, loss = 1.18448053\n",
      "Iteration 4235, loss = 1.18447470\n",
      "Iteration 4236, loss = 1.18445834\n",
      "Iteration 4237, loss = 1.18444684\n",
      "Iteration 4238, loss = 1.18443548\n",
      "Iteration 4239, loss = 1.18442279\n",
      "Iteration 4240, loss = 1.18441045\n",
      "Iteration 4241, loss = 1.18439784\n",
      "Iteration 4242, loss = 1.18438451\n",
      "Iteration 4243, loss = 1.18437032\n",
      "Iteration 4244, loss = 1.18435739\n",
      "Iteration 4245, loss = 1.18434499\n",
      "Iteration 4246, loss = 1.18433531\n",
      "Iteration 4247, loss = 1.18431996\n",
      "Iteration 4248, loss = 1.18430777\n",
      "Iteration 4249, loss = 1.18429475\n",
      "Iteration 4250, loss = 1.18428255\n",
      "Iteration 4251, loss = 1.18427795\n",
      "Iteration 4252, loss = 1.18425741\n",
      "Iteration 4253, loss = 1.18424578\n",
      "Iteration 4254, loss = 1.18423414\n",
      "Iteration 4255, loss = 1.18422444\n",
      "Iteration 4256, loss = 1.18421171\n",
      "Iteration 4257, loss = 1.18419650\n",
      "Iteration 4258, loss = 1.18418271\n",
      "Iteration 4259, loss = 1.18417587\n",
      "Iteration 4260, loss = 1.18415730\n",
      "Iteration 4261, loss = 1.18414594\n",
      "Iteration 4262, loss = 1.18413400\n",
      "Iteration 4263, loss = 1.18412122\n",
      "Iteration 4264, loss = 1.18410931\n",
      "Iteration 4265, loss = 1.18409553\n",
      "Iteration 4266, loss = 1.18408351\n",
      "Iteration 4267, loss = 1.18407155\n",
      "Iteration 4268, loss = 1.18405958\n",
      "Iteration 4269, loss = 1.18404601\n",
      "Iteration 4270, loss = 1.18403386\n",
      "Iteration 4271, loss = 1.18402263\n",
      "Iteration 4272, loss = 1.18401021\n",
      "Iteration 4273, loss = 1.18399756\n",
      "Iteration 4274, loss = 1.18398484\n",
      "Iteration 4275, loss = 1.18397279\n",
      "Iteration 4276, loss = 1.18396395\n",
      "Iteration 4277, loss = 1.18394839\n",
      "Iteration 4278, loss = 1.18393843\n",
      "Iteration 4279, loss = 1.18392174\n",
      "Iteration 4280, loss = 1.18391102\n",
      "Iteration 4281, loss = 1.18389747\n",
      "Iteration 4282, loss = 1.18388450\n",
      "Iteration 4283, loss = 1.18387235\n",
      "Iteration 4284, loss = 1.18386005\n",
      "Iteration 4285, loss = 1.18384917\n",
      "Iteration 4286, loss = 1.18383881\n",
      "Iteration 4287, loss = 1.18382469\n",
      "Iteration 4288, loss = 1.18381046\n",
      "Iteration 4289, loss = 1.18379823\n",
      "Iteration 4290, loss = 1.18378619\n",
      "Iteration 4291, loss = 1.18377465\n",
      "Iteration 4292, loss = 1.18376152\n",
      "Iteration 4293, loss = 1.18374986\n",
      "Iteration 4294, loss = 1.18373639\n",
      "Iteration 4295, loss = 1.18372642\n",
      "Iteration 4296, loss = 1.18371697\n",
      "Iteration 4297, loss = 1.18370345\n",
      "Iteration 4298, loss = 1.18369394\n",
      "Iteration 4299, loss = 1.18367707\n",
      "Iteration 4300, loss = 1.18366504\n",
      "Iteration 4301, loss = 1.18365138\n",
      "Iteration 4302, loss = 1.18363779\n",
      "Iteration 4303, loss = 1.18362676\n",
      "Iteration 4304, loss = 1.18361545\n",
      "Iteration 4305, loss = 1.18360112\n",
      "Iteration 4306, loss = 1.18358999\n",
      "Iteration 4307, loss = 1.18357647\n",
      "Iteration 4308, loss = 1.18356667\n",
      "Iteration 4309, loss = 1.18355221\n",
      "Iteration 4310, loss = 1.18354015\n",
      "Iteration 4311, loss = 1.18352786\n",
      "Iteration 4312, loss = 1.18351677\n",
      "Iteration 4313, loss = 1.18350285\n",
      "Iteration 4314, loss = 1.18349215\n",
      "Iteration 4315, loss = 1.18347745\n",
      "Iteration 4316, loss = 1.18346571\n",
      "Iteration 4317, loss = 1.18345499\n",
      "Iteration 4318, loss = 1.18344007\n",
      "Iteration 4319, loss = 1.18342804\n",
      "Iteration 4320, loss = 1.18341857\n",
      "Iteration 4321, loss = 1.18340385\n",
      "Iteration 4322, loss = 1.18339628\n",
      "Iteration 4323, loss = 1.18338120\n",
      "Iteration 4324, loss = 1.18337595\n",
      "Iteration 4325, loss = 1.18335881\n",
      "Iteration 4326, loss = 1.18334192\n",
      "Iteration 4327, loss = 1.18333547\n",
      "Iteration 4328, loss = 1.18331961\n",
      "Iteration 4329, loss = 1.18330527\n",
      "Iteration 4330, loss = 1.18329258\n",
      "Iteration 4331, loss = 1.18328156\n",
      "Iteration 4332, loss = 1.18327132\n",
      "Iteration 4333, loss = 1.18325879\n",
      "Iteration 4334, loss = 1.18324510\n",
      "Iteration 4335, loss = 1.18323284\n",
      "Iteration 4336, loss = 1.18322367\n",
      "Iteration 4337, loss = 1.18320739\n",
      "Iteration 4338, loss = 1.18319553\n",
      "Iteration 4339, loss = 1.18318619\n",
      "Iteration 4340, loss = 1.18317112\n",
      "Iteration 4341, loss = 1.18315943\n",
      "Iteration 4342, loss = 1.18314983\n",
      "Iteration 4343, loss = 1.18313612\n",
      "Iteration 4344, loss = 1.18312171\n",
      "Iteration 4345, loss = 1.18311006\n",
      "Iteration 4346, loss = 1.18309816\n",
      "Iteration 4347, loss = 1.18308626\n",
      "Iteration 4348, loss = 1.18307222\n",
      "Iteration 4349, loss = 1.18306684\n",
      "Iteration 4350, loss = 1.18304895\n",
      "Iteration 4351, loss = 1.18303640\n",
      "Iteration 4352, loss = 1.18302952\n",
      "Iteration 4353, loss = 1.18301178\n",
      "Iteration 4354, loss = 1.18299970\n",
      "Iteration 4355, loss = 1.18298729\n",
      "Iteration 4356, loss = 1.18297540\n",
      "Iteration 4357, loss = 1.18296334\n",
      "Iteration 4358, loss = 1.18295403\n",
      "Iteration 4359, loss = 1.18294361\n",
      "Iteration 4360, loss = 1.18292616\n",
      "Iteration 4361, loss = 1.18291518\n",
      "Iteration 4362, loss = 1.18290480\n",
      "Iteration 4363, loss = 1.18289313\n",
      "Iteration 4364, loss = 1.18287672\n",
      "Iteration 4365, loss = 1.18286674\n",
      "Iteration 4366, loss = 1.18285706\n",
      "Iteration 4367, loss = 1.18284204\n",
      "Iteration 4368, loss = 1.18282980\n",
      "Iteration 4369, loss = 1.18281811\n",
      "Iteration 4370, loss = 1.18280739\n",
      "Iteration 4371, loss = 1.18279323\n",
      "Iteration 4372, loss = 1.18278180\n",
      "Iteration 4373, loss = 1.18276725\n",
      "Iteration 4374, loss = 1.18275543\n",
      "Iteration 4375, loss = 1.18274564\n",
      "Iteration 4376, loss = 1.18273491\n",
      "Iteration 4377, loss = 1.18272246\n",
      "Iteration 4378, loss = 1.18270792\n",
      "Iteration 4379, loss = 1.18270378\n",
      "Iteration 4380, loss = 1.18268378\n",
      "Iteration 4381, loss = 1.18267074\n",
      "Iteration 4382, loss = 1.18266014\n",
      "Iteration 4383, loss = 1.18264569\n",
      "Iteration 4384, loss = 1.18263747\n",
      "Iteration 4385, loss = 1.18262498\n",
      "Iteration 4386, loss = 1.18260938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4387, loss = 1.18259791\n",
      "Iteration 4388, loss = 1.18258648\n",
      "Iteration 4389, loss = 1.18257455\n",
      "Iteration 4390, loss = 1.18256258\n",
      "Iteration 4391, loss = 1.18255330\n",
      "Iteration 4392, loss = 1.18253853\n",
      "Iteration 4393, loss = 1.18252786\n",
      "Iteration 4394, loss = 1.18251237\n",
      "Iteration 4395, loss = 1.18249889\n",
      "Iteration 4396, loss = 1.18248808\n",
      "Iteration 4397, loss = 1.18248033\n",
      "Iteration 4398, loss = 1.18246736\n",
      "Iteration 4399, loss = 1.18245121\n",
      "Iteration 4400, loss = 1.18243903\n",
      "Iteration 4401, loss = 1.18242701\n",
      "Iteration 4402, loss = 1.18241466\n",
      "Iteration 4403, loss = 1.18240184\n",
      "Iteration 4404, loss = 1.18239347\n",
      "Iteration 4405, loss = 1.18237939\n",
      "Iteration 4406, loss = 1.18236901\n",
      "Iteration 4407, loss = 1.18235437\n",
      "Iteration 4408, loss = 1.18234180\n",
      "Iteration 4409, loss = 1.18232922\n",
      "Iteration 4410, loss = 1.18231756\n",
      "Iteration 4411, loss = 1.18230500\n",
      "Iteration 4412, loss = 1.18229213\n",
      "Iteration 4413, loss = 1.18228100\n",
      "Iteration 4414, loss = 1.18227009\n",
      "Iteration 4415, loss = 1.18225550\n",
      "Iteration 4416, loss = 1.18224483\n",
      "Iteration 4417, loss = 1.18223601\n",
      "Iteration 4418, loss = 1.18222051\n",
      "Iteration 4419, loss = 1.18220928\n",
      "Iteration 4420, loss = 1.18219885\n",
      "Iteration 4421, loss = 1.18218621\n",
      "Iteration 4422, loss = 1.18217173\n",
      "Iteration 4423, loss = 1.18216144\n",
      "Iteration 4424, loss = 1.18214814\n",
      "Iteration 4425, loss = 1.18213626\n",
      "Iteration 4426, loss = 1.18212968\n",
      "Iteration 4427, loss = 1.18211411\n",
      "Iteration 4428, loss = 1.18210358\n",
      "Iteration 4429, loss = 1.18209035\n",
      "Iteration 4430, loss = 1.18207670\n",
      "Iteration 4431, loss = 1.18206321\n",
      "Iteration 4432, loss = 1.18205141\n",
      "Iteration 4433, loss = 1.18203948\n",
      "Iteration 4434, loss = 1.18202775\n",
      "Iteration 4435, loss = 1.18201650\n",
      "Iteration 4436, loss = 1.18200508\n",
      "Iteration 4437, loss = 1.18199021\n",
      "Iteration 4438, loss = 1.18198496\n",
      "Iteration 4439, loss = 1.18196805\n",
      "Iteration 4440, loss = 1.18195517\n",
      "Iteration 4441, loss = 1.18194266\n",
      "Iteration 4442, loss = 1.18192888\n",
      "Iteration 4443, loss = 1.18191800\n",
      "Iteration 4444, loss = 1.18190515\n",
      "Iteration 4445, loss = 1.18189887\n",
      "Iteration 4446, loss = 1.18188428\n",
      "Iteration 4447, loss = 1.18187490\n",
      "Iteration 4448, loss = 1.18186130\n",
      "Iteration 4449, loss = 1.18184803\n",
      "Iteration 4450, loss = 1.18183827\n",
      "Iteration 4451, loss = 1.18182696\n",
      "Iteration 4452, loss = 1.18181105\n",
      "Iteration 4453, loss = 1.18179697\n",
      "Iteration 4454, loss = 1.18178646\n",
      "Iteration 4455, loss = 1.18177449\n",
      "Iteration 4456, loss = 1.18176021\n",
      "Iteration 4457, loss = 1.18174954\n",
      "Iteration 4458, loss = 1.18173913\n",
      "Iteration 4459, loss = 1.18172429\n",
      "Iteration 4460, loss = 1.18171253\n",
      "Iteration 4461, loss = 1.18170666\n",
      "Iteration 4462, loss = 1.18169304\n",
      "Iteration 4463, loss = 1.18167499\n",
      "Iteration 4464, loss = 1.18167077\n",
      "Iteration 4465, loss = 1.18165563\n",
      "Iteration 4466, loss = 1.18164331\n",
      "Iteration 4467, loss = 1.18162646\n",
      "Iteration 4468, loss = 1.18161525\n",
      "Iteration 4469, loss = 1.18160599\n",
      "Iteration 4470, loss = 1.18159398\n",
      "Iteration 4471, loss = 1.18158026\n",
      "Iteration 4472, loss = 1.18157045\n",
      "Iteration 4473, loss = 1.18155511\n",
      "Iteration 4474, loss = 1.18154517\n",
      "Iteration 4475, loss = 1.18153140\n",
      "Iteration 4476, loss = 1.18151863\n",
      "Iteration 4477, loss = 1.18151042\n",
      "Iteration 4478, loss = 1.18149901\n",
      "Iteration 4479, loss = 1.18148216\n",
      "Iteration 4480, loss = 1.18146996\n",
      "Iteration 4481, loss = 1.18145851\n",
      "Iteration 4482, loss = 1.18145055\n",
      "Iteration 4483, loss = 1.18143518\n",
      "Iteration 4484, loss = 1.18142295\n",
      "Iteration 4485, loss = 1.18141027\n",
      "Iteration 4486, loss = 1.18139843\n",
      "Iteration 4487, loss = 1.18138651\n",
      "Iteration 4488, loss = 1.18137347\n",
      "Iteration 4489, loss = 1.18136324\n",
      "Iteration 4490, loss = 1.18135051\n",
      "Iteration 4491, loss = 1.18133910\n",
      "Iteration 4492, loss = 1.18132627\n",
      "Iteration 4493, loss = 1.18131315\n",
      "Iteration 4494, loss = 1.18130259\n",
      "Iteration 4495, loss = 1.18129080\n",
      "Iteration 4496, loss = 1.18127958\n",
      "Iteration 4497, loss = 1.18126876\n",
      "Iteration 4498, loss = 1.18125335\n",
      "Iteration 4499, loss = 1.18124287\n",
      "Iteration 4500, loss = 1.18123024\n",
      "Iteration 4501, loss = 1.18121660\n",
      "Iteration 4502, loss = 1.18120579\n",
      "Iteration 4503, loss = 1.18119515\n",
      "Iteration 4504, loss = 1.18118061\n",
      "Iteration 4505, loss = 1.18116834\n",
      "Iteration 4506, loss = 1.18116243\n",
      "Iteration 4507, loss = 1.18114531\n",
      "Iteration 4508, loss = 1.18113454\n",
      "Iteration 4509, loss = 1.18112183\n",
      "Iteration 4510, loss = 1.18110926\n",
      "Iteration 4511, loss = 1.18109974\n",
      "Iteration 4512, loss = 1.18109174\n",
      "Iteration 4513, loss = 1.18107186\n",
      "Iteration 4514, loss = 1.18106126\n",
      "Iteration 4515, loss = 1.18105068\n",
      "Iteration 4516, loss = 1.18104123\n",
      "Iteration 4517, loss = 1.18102765\n",
      "Iteration 4518, loss = 1.18101645\n",
      "Iteration 4519, loss = 1.18100663\n",
      "Iteration 4520, loss = 1.18099155\n",
      "Iteration 4521, loss = 1.18097745\n",
      "Iteration 4522, loss = 1.18096507\n",
      "Iteration 4523, loss = 1.18095514\n",
      "Iteration 4524, loss = 1.18094140\n",
      "Iteration 4525, loss = 1.18093109\n",
      "Iteration 4526, loss = 1.18091793\n",
      "Iteration 4527, loss = 1.18090800\n",
      "Iteration 4528, loss = 1.18089254\n",
      "Iteration 4529, loss = 1.18088029\n",
      "Iteration 4530, loss = 1.18086803\n",
      "Iteration 4531, loss = 1.18085567\n",
      "Iteration 4532, loss = 1.18084558\n",
      "Iteration 4533, loss = 1.18083314\n",
      "Iteration 4534, loss = 1.18081994\n",
      "Iteration 4535, loss = 1.18080950\n",
      "Iteration 4536, loss = 1.18079680\n",
      "Iteration 4537, loss = 1.18078526\n",
      "Iteration 4538, loss = 1.18077241\n",
      "Iteration 4539, loss = 1.18076130\n",
      "Iteration 4540, loss = 1.18074868\n",
      "Iteration 4541, loss = 1.18073548\n",
      "Iteration 4542, loss = 1.18072486\n",
      "Iteration 4543, loss = 1.18071231\n",
      "Iteration 4544, loss = 1.18070159\n",
      "Iteration 4545, loss = 1.18068923\n",
      "Iteration 4546, loss = 1.18068581\n",
      "Iteration 4547, loss = 1.18066652\n",
      "Iteration 4548, loss = 1.18065542\n",
      "Iteration 4549, loss = 1.18064421\n",
      "Iteration 4550, loss = 1.18062735\n",
      "Iteration 4551, loss = 1.18061605\n",
      "Iteration 4552, loss = 1.18060433\n",
      "Iteration 4553, loss = 1.18059479\n",
      "Iteration 4554, loss = 1.18058088\n",
      "Iteration 4555, loss = 1.18056862\n",
      "Iteration 4556, loss = 1.18055892\n",
      "Iteration 4557, loss = 1.18054324\n",
      "Iteration 4558, loss = 1.18053099\n",
      "Iteration 4559, loss = 1.18051961\n",
      "Iteration 4560, loss = 1.18050707\n",
      "Iteration 4561, loss = 1.18049526\n",
      "Iteration 4562, loss = 1.18048405\n",
      "Iteration 4563, loss = 1.18047396\n",
      "Iteration 4564, loss = 1.18045968\n",
      "Iteration 4565, loss = 1.18044835\n",
      "Iteration 4566, loss = 1.18043734\n",
      "Iteration 4567, loss = 1.18042371\n",
      "Iteration 4568, loss = 1.18041220\n",
      "Iteration 4569, loss = 1.18039986\n",
      "Iteration 4570, loss = 1.18038774\n",
      "Iteration 4571, loss = 1.18037671\n",
      "Iteration 4572, loss = 1.18036299\n",
      "Iteration 4573, loss = 1.18035273\n",
      "Iteration 4574, loss = 1.18033874\n",
      "Iteration 4575, loss = 1.18032970\n",
      "Iteration 4576, loss = 1.18031584\n",
      "Iteration 4577, loss = 1.18030281\n",
      "Iteration 4578, loss = 1.18029096\n",
      "Iteration 4579, loss = 1.18028071\n",
      "Iteration 4580, loss = 1.18026661\n",
      "Iteration 4581, loss = 1.18025778\n",
      "Iteration 4582, loss = 1.18024552\n",
      "Iteration 4583, loss = 1.18023262\n",
      "Iteration 4584, loss = 1.18022126\n",
      "Iteration 4585, loss = 1.18020681\n",
      "Iteration 4586, loss = 1.18020108\n",
      "Iteration 4587, loss = 1.18018339\n",
      "Iteration 4588, loss = 1.18017061\n",
      "Iteration 4589, loss = 1.18016047\n",
      "Iteration 4590, loss = 1.18014934\n",
      "Iteration 4591, loss = 1.18013640\n",
      "Iteration 4592, loss = 1.18012508\n",
      "Iteration 4593, loss = 1.18011415\n",
      "Iteration 4594, loss = 1.18009920\n",
      "Iteration 4595, loss = 1.18008685\n",
      "Iteration 4596, loss = 1.18007469\n",
      "Iteration 4597, loss = 1.18006441\n",
      "Iteration 4598, loss = 1.18005103\n",
      "Iteration 4599, loss = 1.18004061\n",
      "Iteration 4600, loss = 1.18002852\n",
      "Iteration 4601, loss = 1.18001413\n",
      "Iteration 4602, loss = 1.18000296\n",
      "Iteration 4603, loss = 1.17999192\n",
      "Iteration 4604, loss = 1.17997956\n",
      "Iteration 4605, loss = 1.17996924\n",
      "Iteration 4606, loss = 1.17995709\n",
      "Iteration 4607, loss = 1.17994512\n",
      "Iteration 4608, loss = 1.17993188\n",
      "Iteration 4609, loss = 1.17992116\n",
      "Iteration 4610, loss = 1.17990777\n",
      "Iteration 4611, loss = 1.17989448\n",
      "Iteration 4612, loss = 1.17988337\n",
      "Iteration 4613, loss = 1.17987108\n",
      "Iteration 4614, loss = 1.17986002\n",
      "Iteration 4615, loss = 1.17984891\n",
      "Iteration 4616, loss = 1.17983622\n",
      "Iteration 4617, loss = 1.17982445\n",
      "Iteration 4618, loss = 1.17981613\n",
      "Iteration 4619, loss = 1.17979963\n",
      "Iteration 4620, loss = 1.17978723\n",
      "Iteration 4621, loss = 1.17977509\n",
      "Iteration 4622, loss = 1.17976696\n",
      "Iteration 4623, loss = 1.17975195\n",
      "Iteration 4624, loss = 1.17974191\n",
      "Iteration 4625, loss = 1.17973046\n",
      "Iteration 4626, loss = 1.17972112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4627, loss = 1.17970623\n",
      "Iteration 4628, loss = 1.17969358\n",
      "Iteration 4629, loss = 1.17968304\n",
      "Iteration 4630, loss = 1.17967206\n",
      "Iteration 4631, loss = 1.17966108\n",
      "Iteration 4632, loss = 1.17964466\n",
      "Iteration 4633, loss = 1.17963253\n",
      "Iteration 4634, loss = 1.17962105\n",
      "Iteration 4635, loss = 1.17960836\n",
      "Iteration 4636, loss = 1.17959636\n",
      "Iteration 4637, loss = 1.17958703\n",
      "Iteration 4638, loss = 1.17957345\n",
      "Iteration 4639, loss = 1.17956358\n",
      "Iteration 4640, loss = 1.17955180\n",
      "Iteration 4641, loss = 1.17953802\n",
      "Iteration 4642, loss = 1.17952563\n",
      "Iteration 4643, loss = 1.17951437\n",
      "Iteration 4644, loss = 1.17950254\n",
      "Iteration 4645, loss = 1.17948890\n",
      "Iteration 4646, loss = 1.17948296\n",
      "Iteration 4647, loss = 1.17946863\n",
      "Iteration 4648, loss = 1.17945419\n",
      "Iteration 4649, loss = 1.17944233\n",
      "Iteration 4650, loss = 1.17943510\n",
      "Iteration 4651, loss = 1.17941828\n",
      "Iteration 4652, loss = 1.17940547\n",
      "Iteration 4653, loss = 1.17939499\n",
      "Iteration 4654, loss = 1.17938220\n",
      "Iteration 4655, loss = 1.17937303\n",
      "Iteration 4656, loss = 1.17936362\n",
      "Iteration 4657, loss = 1.17934844\n",
      "Iteration 4658, loss = 1.17933604\n",
      "Iteration 4659, loss = 1.17932408\n",
      "Iteration 4660, loss = 1.17931349\n",
      "Iteration 4661, loss = 1.17929914\n",
      "Iteration 4662, loss = 1.17928825\n",
      "Iteration 4663, loss = 1.17927893\n",
      "Iteration 4664, loss = 1.17926371\n",
      "Iteration 4665, loss = 1.17925324\n",
      "Iteration 4666, loss = 1.17924343\n",
      "Iteration 4667, loss = 1.17922989\n",
      "Iteration 4668, loss = 1.17921558\n",
      "Iteration 4669, loss = 1.17920423\n",
      "Iteration 4670, loss = 1.17919788\n",
      "Iteration 4671, loss = 1.17918151\n",
      "Iteration 4672, loss = 1.17917204\n",
      "Iteration 4673, loss = 1.17915841\n",
      "Iteration 4674, loss = 1.17914751\n",
      "Iteration 4675, loss = 1.17913688\n",
      "Iteration 4676, loss = 1.17912111\n",
      "Iteration 4677, loss = 1.17910925\n",
      "Iteration 4678, loss = 1.17910117\n",
      "Iteration 4679, loss = 1.17908553\n",
      "Iteration 4680, loss = 1.17907843\n",
      "Iteration 4681, loss = 1.17906359\n",
      "Iteration 4682, loss = 1.17905590\n",
      "Iteration 4683, loss = 1.17903821\n",
      "Iteration 4684, loss = 1.17902974\n",
      "Iteration 4685, loss = 1.17901888\n",
      "Iteration 4686, loss = 1.17900270\n",
      "Iteration 4687, loss = 1.17899285\n",
      "Iteration 4688, loss = 1.17898047\n",
      "Iteration 4689, loss = 1.17896794\n",
      "Iteration 4690, loss = 1.17895608\n",
      "Iteration 4691, loss = 1.17894359\n",
      "Iteration 4692, loss = 1.17893283\n",
      "Iteration 4693, loss = 1.17891985\n",
      "Iteration 4694, loss = 1.17891006\n",
      "Iteration 4695, loss = 1.17889556\n",
      "Iteration 4696, loss = 1.17888541\n",
      "Iteration 4697, loss = 1.17887388\n",
      "Iteration 4698, loss = 1.17886141\n",
      "Iteration 4699, loss = 1.17885096\n",
      "Iteration 4700, loss = 1.17883822\n",
      "Iteration 4701, loss = 1.17882855\n",
      "Iteration 4702, loss = 1.17881417\n",
      "Iteration 4703, loss = 1.17880249\n",
      "Iteration 4704, loss = 1.17879216\n",
      "Iteration 4705, loss = 1.17878046\n",
      "Iteration 4706, loss = 1.17876840\n",
      "Iteration 4707, loss = 1.17875641\n",
      "Iteration 4708, loss = 1.17874504\n",
      "Iteration 4709, loss = 1.17873281\n",
      "Iteration 4710, loss = 1.17871994\n",
      "Iteration 4711, loss = 1.17871017\n",
      "Iteration 4712, loss = 1.17869949\n",
      "Iteration 4713, loss = 1.17868636\n",
      "Iteration 4714, loss = 1.17867582\n",
      "Iteration 4715, loss = 1.17866090\n",
      "Iteration 4716, loss = 1.17865008\n",
      "Iteration 4717, loss = 1.17863899\n",
      "Iteration 4718, loss = 1.17862801\n",
      "Iteration 4719, loss = 1.17861491\n",
      "Iteration 4720, loss = 1.17860272\n",
      "Iteration 4721, loss = 1.17859537\n",
      "Iteration 4722, loss = 1.17857932\n",
      "Iteration 4723, loss = 1.17856745\n",
      "Iteration 4724, loss = 1.17855657\n",
      "Iteration 4725, loss = 1.17854445\n",
      "Iteration 4726, loss = 1.17853377\n",
      "Iteration 4727, loss = 1.17852485\n",
      "Iteration 4728, loss = 1.17850904\n",
      "Iteration 4729, loss = 1.17849697\n",
      "Iteration 4730, loss = 1.17848981\n",
      "Iteration 4731, loss = 1.17847544\n",
      "Iteration 4732, loss = 1.17846503\n",
      "Iteration 4733, loss = 1.17845313\n",
      "Iteration 4734, loss = 1.17844157\n",
      "Iteration 4735, loss = 1.17842833\n",
      "Iteration 4736, loss = 1.17841737\n",
      "Iteration 4737, loss = 1.17840831\n",
      "Iteration 4738, loss = 1.17839256\n",
      "Iteration 4739, loss = 1.17838184\n",
      "Iteration 4740, loss = 1.17836957\n",
      "Iteration 4741, loss = 1.17836548\n",
      "Iteration 4742, loss = 1.17835021\n",
      "Iteration 4743, loss = 1.17833403\n",
      "Iteration 4744, loss = 1.17832314\n",
      "Iteration 4745, loss = 1.17831316\n",
      "Iteration 4746, loss = 1.17829931\n",
      "Iteration 4747, loss = 1.17828789\n",
      "Iteration 4748, loss = 1.17827648\n",
      "Iteration 4749, loss = 1.17826788\n",
      "Iteration 4750, loss = 1.17825337\n",
      "Iteration 4751, loss = 1.17824364\n",
      "Iteration 4752, loss = 1.17822953\n",
      "Iteration 4753, loss = 1.17821914\n",
      "Iteration 4754, loss = 1.17820658\n",
      "Iteration 4755, loss = 1.17819424\n",
      "Iteration 4756, loss = 1.17818793\n",
      "Iteration 4757, loss = 1.17817303\n",
      "Iteration 4758, loss = 1.17816443\n",
      "Iteration 4759, loss = 1.17815041\n",
      "Iteration 4760, loss = 1.17813791\n",
      "Iteration 4761, loss = 1.17812637\n",
      "Iteration 4762, loss = 1.17811612\n",
      "Iteration 4763, loss = 1.17810348\n",
      "Iteration 4764, loss = 1.17809299\n",
      "Iteration 4765, loss = 1.17808432\n",
      "Iteration 4766, loss = 1.17806874\n",
      "Iteration 4767, loss = 1.17805746\n",
      "Iteration 4768, loss = 1.17804498\n",
      "Iteration 4769, loss = 1.17804393\n",
      "Iteration 4770, loss = 1.17802519\n",
      "Iteration 4771, loss = 1.17801064\n",
      "Iteration 4772, loss = 1.17799973\n",
      "Iteration 4773, loss = 1.17798896\n",
      "Iteration 4774, loss = 1.17797938\n",
      "Iteration 4775, loss = 1.17797200\n",
      "Iteration 4776, loss = 1.17795335\n",
      "Iteration 4777, loss = 1.17794396\n",
      "Iteration 4778, loss = 1.17792905\n",
      "Iteration 4779, loss = 1.17792081\n",
      "Iteration 4780, loss = 1.17790681\n",
      "Iteration 4781, loss = 1.17789662\n",
      "Iteration 4782, loss = 1.17788702\n",
      "Iteration 4783, loss = 1.17787439\n",
      "Iteration 4784, loss = 1.17786455\n",
      "Iteration 4785, loss = 1.17785148\n",
      "Iteration 4786, loss = 1.17784128\n",
      "Iteration 4787, loss = 1.17782720\n",
      "Iteration 4788, loss = 1.17782160\n",
      "Iteration 4789, loss = 1.17780905\n",
      "Iteration 4790, loss = 1.17780285\n",
      "Iteration 4791, loss = 1.17778179\n",
      "Iteration 4792, loss = 1.17777008\n",
      "Iteration 4793, loss = 1.17776329\n",
      "Iteration 4794, loss = 1.17774701\n",
      "Iteration 4795, loss = 1.17773681\n",
      "Iteration 4796, loss = 1.17772693\n",
      "Iteration 4797, loss = 1.17771627\n",
      "Iteration 4798, loss = 1.17770332\n",
      "Iteration 4799, loss = 1.17769016\n",
      "Iteration 4800, loss = 1.17767938\n",
      "Iteration 4801, loss = 1.17766767\n",
      "Iteration 4802, loss = 1.17765665\n",
      "Iteration 4803, loss = 1.17764411\n",
      "Iteration 4804, loss = 1.17763496\n",
      "Iteration 4805, loss = 1.17762322\n",
      "Iteration 4806, loss = 1.17761192\n",
      "Iteration 4807, loss = 1.17759975\n",
      "Iteration 4808, loss = 1.17758824\n",
      "Iteration 4809, loss = 1.17757514\n",
      "Iteration 4810, loss = 1.17756819\n",
      "Iteration 4811, loss = 1.17755665\n",
      "Iteration 4812, loss = 1.17754383\n",
      "Iteration 4813, loss = 1.17753127\n",
      "Iteration 4814, loss = 1.17751914\n",
      "Iteration 4815, loss = 1.17751272\n",
      "Iteration 4816, loss = 1.17749518\n",
      "Iteration 4817, loss = 1.17748562\n",
      "Iteration 4818, loss = 1.17747469\n",
      "Iteration 4819, loss = 1.17746215\n",
      "Iteration 4820, loss = 1.17745012\n",
      "Iteration 4821, loss = 1.17744146\n",
      "Iteration 4822, loss = 1.17742868\n",
      "Iteration 4823, loss = 1.17741880\n",
      "Iteration 4824, loss = 1.17740464\n",
      "Iteration 4825, loss = 1.17739351\n",
      "Iteration 4826, loss = 1.17738162\n",
      "Iteration 4827, loss = 1.17737153\n",
      "Iteration 4828, loss = 1.17736103\n",
      "Iteration 4829, loss = 1.17734994\n",
      "Iteration 4830, loss = 1.17733865\n",
      "Iteration 4831, loss = 1.17732684\n",
      "Iteration 4832, loss = 1.17731751\n",
      "Iteration 4833, loss = 1.17730369\n",
      "Iteration 4834, loss = 1.17729544\n",
      "Iteration 4835, loss = 1.17728335\n",
      "Iteration 4836, loss = 1.17727189\n",
      "Iteration 4837, loss = 1.17726257\n",
      "Iteration 4838, loss = 1.17724998\n",
      "Iteration 4839, loss = 1.17723640\n",
      "Iteration 4840, loss = 1.17722501\n",
      "Iteration 4841, loss = 1.17721581\n",
      "Iteration 4842, loss = 1.17720583\n",
      "Iteration 4843, loss = 1.17719059\n",
      "Iteration 4844, loss = 1.17717975\n",
      "Iteration 4845, loss = 1.17716790\n",
      "Iteration 4846, loss = 1.17715687\n",
      "Iteration 4847, loss = 1.17714986\n",
      "Iteration 4848, loss = 1.17713424\n",
      "Iteration 4849, loss = 1.17712360\n",
      "Iteration 4850, loss = 1.17711274\n",
      "Iteration 4851, loss = 1.17710235\n",
      "Iteration 4852, loss = 1.17709009\n",
      "Iteration 4853, loss = 1.17707847\n",
      "Iteration 4854, loss = 1.17706747\n",
      "Iteration 4855, loss = 1.17705775\n",
      "Iteration 4856, loss = 1.17704814\n",
      "Iteration 4857, loss = 1.17703373\n",
      "Iteration 4858, loss = 1.17702140\n",
      "Iteration 4859, loss = 1.17701136\n",
      "Iteration 4860, loss = 1.17700605\n",
      "Iteration 4861, loss = 1.17698888\n",
      "Iteration 4862, loss = 1.17697705\n",
      "Iteration 4863, loss = 1.17697057\n",
      "Iteration 4864, loss = 1.17695715\n",
      "Iteration 4865, loss = 1.17694506\n",
      "Iteration 4866, loss = 1.17693536\n",
      "Iteration 4867, loss = 1.17692177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4868, loss = 1.17691008\n",
      "Iteration 4869, loss = 1.17690014\n",
      "Iteration 4870, loss = 1.17688950\n",
      "Iteration 4871, loss = 1.17688060\n",
      "Iteration 4872, loss = 1.17686629\n",
      "Iteration 4873, loss = 1.17685515\n",
      "Iteration 4874, loss = 1.17684469\n",
      "Iteration 4875, loss = 1.17683532\n",
      "Iteration 4876, loss = 1.17682197\n",
      "Iteration 4877, loss = 1.17681173\n",
      "Iteration 4878, loss = 1.17679970\n",
      "Iteration 4879, loss = 1.17678990\n",
      "Iteration 4880, loss = 1.17677616\n",
      "Iteration 4881, loss = 1.17676449\n",
      "Iteration 4882, loss = 1.17675471\n",
      "Iteration 4883, loss = 1.17674304\n",
      "Iteration 4884, loss = 1.17673041\n",
      "Iteration 4885, loss = 1.17672001\n",
      "Iteration 4886, loss = 1.17671103\n",
      "Iteration 4887, loss = 1.17670464\n",
      "Iteration 4888, loss = 1.17668726\n",
      "Iteration 4889, loss = 1.17667883\n",
      "Iteration 4890, loss = 1.17666389\n",
      "Iteration 4891, loss = 1.17665311\n",
      "Iteration 4892, loss = 1.17664523\n",
      "Iteration 4893, loss = 1.17663240\n",
      "Iteration 4894, loss = 1.17662170\n",
      "Iteration 4895, loss = 1.17661158\n",
      "Iteration 4896, loss = 1.17659755\n",
      "Iteration 4897, loss = 1.17658657\n",
      "Iteration 4898, loss = 1.17657519\n",
      "Iteration 4899, loss = 1.17657125\n",
      "Iteration 4900, loss = 1.17655460\n",
      "Iteration 4901, loss = 1.17654255\n",
      "Iteration 4902, loss = 1.17653085\n",
      "Iteration 4903, loss = 1.17652641\n",
      "Iteration 4904, loss = 1.17651257\n",
      "Iteration 4905, loss = 1.17650405\n",
      "Iteration 4906, loss = 1.17648937\n",
      "Iteration 4907, loss = 1.17647828\n",
      "Iteration 4908, loss = 1.17646731\n",
      "Iteration 4909, loss = 1.17645558\n",
      "Iteration 4910, loss = 1.17644220\n",
      "Iteration 4911, loss = 1.17643535\n",
      "Iteration 4912, loss = 1.17642096\n",
      "Iteration 4913, loss = 1.17641020\n",
      "Iteration 4914, loss = 1.17640024\n",
      "Iteration 4915, loss = 1.17639210\n",
      "Iteration 4916, loss = 1.17637589\n",
      "Iteration 4917, loss = 1.17636551\n",
      "Iteration 4918, loss = 1.17635490\n",
      "Iteration 4919, loss = 1.17634408\n",
      "Iteration 4920, loss = 1.17633509\n",
      "Iteration 4921, loss = 1.17632372\n",
      "Iteration 4922, loss = 1.17631375\n",
      "Iteration 4923, loss = 1.17630334\n",
      "Iteration 4924, loss = 1.17629385\n",
      "Iteration 4925, loss = 1.17628051\n",
      "Iteration 4926, loss = 1.17627241\n",
      "Iteration 4927, loss = 1.17625616\n",
      "Iteration 4928, loss = 1.17624749\n",
      "Iteration 4929, loss = 1.17623400\n",
      "Iteration 4930, loss = 1.17622360\n",
      "Iteration 4931, loss = 1.17621207\n",
      "Iteration 4932, loss = 1.17620103\n",
      "Iteration 4933, loss = 1.17619129\n",
      "Iteration 4934, loss = 1.17618032\n",
      "Iteration 4935, loss = 1.17616852\n",
      "Iteration 4936, loss = 1.17615682\n",
      "Iteration 4937, loss = 1.17615240\n",
      "Iteration 4938, loss = 1.17613365\n",
      "Iteration 4939, loss = 1.17612367\n",
      "Iteration 4940, loss = 1.17611371\n",
      "Iteration 4941, loss = 1.17610202\n",
      "Iteration 4942, loss = 1.17609455\n",
      "Iteration 4943, loss = 1.17608279\n",
      "Iteration 4944, loss = 1.17606907\n",
      "Iteration 4945, loss = 1.17606042\n",
      "Iteration 4946, loss = 1.17604894\n",
      "Iteration 4947, loss = 1.17603625\n",
      "Iteration 4948, loss = 1.17602717\n",
      "Iteration 4949, loss = 1.17601531\n",
      "Iteration 4950, loss = 1.17600332\n",
      "Iteration 4951, loss = 1.17599417\n",
      "Iteration 4952, loss = 1.17598230\n",
      "Iteration 4953, loss = 1.17597094\n",
      "Iteration 4954, loss = 1.17596389\n",
      "Iteration 4955, loss = 1.17594863\n",
      "Iteration 4956, loss = 1.17593899\n",
      "Iteration 4957, loss = 1.17592704\n",
      "Iteration 4958, loss = 1.17591700\n",
      "Iteration 4959, loss = 1.17590562\n",
      "Iteration 4960, loss = 1.17589424\n",
      "Iteration 4961, loss = 1.17588594\n",
      "Iteration 4962, loss = 1.17587359\n",
      "Iteration 4963, loss = 1.17586145\n",
      "Iteration 4964, loss = 1.17585017\n",
      "Iteration 4965, loss = 1.17584321\n",
      "Iteration 4966, loss = 1.17583424\n",
      "Iteration 4967, loss = 1.17582783\n",
      "Iteration 4968, loss = 1.17580992\n",
      "Iteration 4969, loss = 1.17579752\n",
      "Iteration 4970, loss = 1.17579028\n",
      "Iteration 4971, loss = 1.17577758\n",
      "Iteration 4972, loss = 1.17576508\n",
      "Iteration 4973, loss = 1.17575282\n",
      "Iteration 4974, loss = 1.17574246\n",
      "Iteration 4975, loss = 1.17573447\n",
      "Iteration 4976, loss = 1.17572109\n",
      "Iteration 4977, loss = 1.17571285\n",
      "Iteration 4978, loss = 1.17570044\n",
      "Iteration 4979, loss = 1.17569002\n",
      "Iteration 4980, loss = 1.17567711\n",
      "Iteration 4981, loss = 1.17566880\n",
      "Iteration 4982, loss = 1.17565860\n",
      "Iteration 4983, loss = 1.17564807\n",
      "Iteration 4984, loss = 1.17563673\n",
      "Iteration 4985, loss = 1.17562151\n",
      "Iteration 4986, loss = 1.17561710\n",
      "Iteration 4987, loss = 1.17560125\n",
      "Iteration 4988, loss = 1.17559020\n",
      "Iteration 4989, loss = 1.17557814\n",
      "Iteration 4990, loss = 1.17556941\n",
      "Iteration 4991, loss = 1.17555802\n",
      "Iteration 4992, loss = 1.17554660\n",
      "Iteration 4993, loss = 1.17553623\n",
      "Iteration 4994, loss = 1.17552611\n",
      "Iteration 4995, loss = 1.17551989\n",
      "Iteration 4996, loss = 1.17550373\n",
      "Iteration 4997, loss = 1.17549692\n",
      "Iteration 4998, loss = 1.17548172\n",
      "Iteration 4999, loss = 1.17547149\n",
      "Iteration 5000, loss = 1.17546312\n",
      "Iteration 5001, loss = 1.17545502\n",
      "Iteration 5002, loss = 1.17543901\n",
      "Iteration 5003, loss = 1.17542953\n",
      "Iteration 5004, loss = 1.17541925\n",
      "Iteration 5005, loss = 1.17540551\n",
      "Iteration 5006, loss = 1.17539481\n",
      "Iteration 5007, loss = 1.17538464\n",
      "Iteration 5008, loss = 1.17537358\n",
      "Iteration 5009, loss = 1.17536504\n",
      "Iteration 5010, loss = 1.17535322\n",
      "Iteration 5011, loss = 1.17534135\n",
      "Iteration 5012, loss = 1.17533471\n",
      "Iteration 5013, loss = 1.17532030\n",
      "Iteration 5014, loss = 1.17531336\n",
      "Iteration 5015, loss = 1.17530026\n",
      "Iteration 5016, loss = 1.17529067\n",
      "Iteration 5017, loss = 1.17527680\n",
      "Iteration 5018, loss = 1.17526529\n",
      "Iteration 5019, loss = 1.17525399\n",
      "Iteration 5020, loss = 1.17524502\n",
      "Iteration 5021, loss = 1.17523317\n",
      "Iteration 5022, loss = 1.17522348\n",
      "Iteration 5023, loss = 1.17521226\n",
      "Iteration 5024, loss = 1.17520273\n",
      "Iteration 5025, loss = 1.17519256\n",
      "Iteration 5026, loss = 1.17518114\n",
      "Iteration 5027, loss = 1.17517115\n",
      "Iteration 5028, loss = 1.17515962\n",
      "Iteration 5029, loss = 1.17514976\n",
      "Iteration 5030, loss = 1.17513720\n",
      "Iteration 5031, loss = 1.17512613\n",
      "Iteration 5032, loss = 1.17512017\n",
      "Iteration 5033, loss = 1.17510561\n",
      "Iteration 5034, loss = 1.17509631\n",
      "Iteration 5035, loss = 1.17508350\n",
      "Iteration 5036, loss = 1.17507659\n",
      "Iteration 5037, loss = 1.17506388\n",
      "Iteration 5038, loss = 1.17505231\n",
      "Iteration 5039, loss = 1.17504084\n",
      "Iteration 5040, loss = 1.17503099\n",
      "Iteration 5041, loss = 1.17502030\n",
      "Iteration 5042, loss = 1.17501170\n",
      "Iteration 5043, loss = 1.17499801\n",
      "Iteration 5044, loss = 1.17499251\n",
      "Iteration 5045, loss = 1.17497778\n",
      "Iteration 5046, loss = 1.17496935\n",
      "Iteration 5047, loss = 1.17496095\n",
      "Iteration 5048, loss = 1.17494645\n",
      "Iteration 5049, loss = 1.17493665\n",
      "Iteration 5050, loss = 1.17492491\n",
      "Iteration 5051, loss = 1.17491369\n",
      "Iteration 5052, loss = 1.17490391\n",
      "Iteration 5053, loss = 1.17489107\n",
      "Iteration 5054, loss = 1.17488137\n",
      "Iteration 5055, loss = 1.17487078\n",
      "Iteration 5056, loss = 1.17486200\n",
      "Iteration 5057, loss = 1.17485335\n",
      "Iteration 5058, loss = 1.17484157\n",
      "Iteration 5059, loss = 1.17482959\n",
      "Iteration 5060, loss = 1.17481887\n",
      "Iteration 5061, loss = 1.17480826\n",
      "Iteration 5062, loss = 1.17479616\n",
      "Iteration 5063, loss = 1.17478573\n",
      "Iteration 5064, loss = 1.17477780\n",
      "Iteration 5065, loss = 1.17476557\n",
      "Iteration 5066, loss = 1.17475394\n",
      "Iteration 5067, loss = 1.17474700\n",
      "Iteration 5068, loss = 1.17473829\n",
      "Iteration 5069, loss = 1.17472326\n",
      "Iteration 5070, loss = 1.17471800\n",
      "Iteration 5071, loss = 1.17470089\n",
      "Iteration 5072, loss = 1.17468980\n",
      "Iteration 5073, loss = 1.17468422\n",
      "Iteration 5074, loss = 1.17466917\n",
      "Iteration 5075, loss = 1.17466162\n",
      "Iteration 5076, loss = 1.17464881\n",
      "Iteration 5077, loss = 1.17464031\n",
      "Iteration 5078, loss = 1.17462649\n",
      "Iteration 5079, loss = 1.17462914\n",
      "Iteration 5080, loss = 1.17460708\n",
      "Iteration 5081, loss = 1.17459653\n",
      "Iteration 5082, loss = 1.17458568\n",
      "Iteration 5083, loss = 1.17457851\n",
      "Iteration 5084, loss = 1.17456429\n",
      "Iteration 5085, loss = 1.17455354\n",
      "Iteration 5086, loss = 1.17454623\n",
      "Iteration 5087, loss = 1.17453437\n",
      "Iteration 5088, loss = 1.17452723\n",
      "Iteration 5089, loss = 1.17451209\n",
      "Iteration 5090, loss = 1.17450224\n",
      "Iteration 5091, loss = 1.17449620\n",
      "Iteration 5092, loss = 1.17448055\n",
      "Iteration 5093, loss = 1.17447554\n",
      "Iteration 5094, loss = 1.17446040\n",
      "Iteration 5095, loss = 1.17444926\n",
      "Iteration 5096, loss = 1.17444063\n",
      "Iteration 5097, loss = 1.17443049\n",
      "Iteration 5098, loss = 1.17441774\n",
      "Iteration 5099, loss = 1.17440742\n",
      "Iteration 5100, loss = 1.17439525\n",
      "Iteration 5101, loss = 1.17438558\n",
      "Iteration 5102, loss = 1.17437592\n",
      "Iteration 5103, loss = 1.17436621\n",
      "Iteration 5104, loss = 1.17435568\n",
      "Iteration 5105, loss = 1.17434360\n",
      "Iteration 5106, loss = 1.17433328\n",
      "Iteration 5107, loss = 1.17432288\n",
      "Iteration 5108, loss = 1.17431629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5109, loss = 1.17430506\n",
      "Iteration 5110, loss = 1.17429261\n",
      "Iteration 5111, loss = 1.17428212\n",
      "Iteration 5112, loss = 1.17426960\n",
      "Iteration 5113, loss = 1.17425983\n",
      "Iteration 5114, loss = 1.17424945\n",
      "Iteration 5115, loss = 1.17423944\n",
      "Iteration 5116, loss = 1.17422752\n",
      "Iteration 5117, loss = 1.17422026\n",
      "Iteration 5118, loss = 1.17420950\n",
      "Iteration 5119, loss = 1.17420124\n",
      "Iteration 5120, loss = 1.17418732\n",
      "Iteration 5121, loss = 1.17417915\n",
      "Iteration 5122, loss = 1.17416735\n",
      "Iteration 5123, loss = 1.17415480\n",
      "Iteration 5124, loss = 1.17415065\n",
      "Iteration 5125, loss = 1.17413686\n",
      "Iteration 5126, loss = 1.17412421\n",
      "Iteration 5127, loss = 1.17411719\n",
      "Iteration 5128, loss = 1.17410395\n",
      "Iteration 5129, loss = 1.17409312\n",
      "Iteration 5130, loss = 1.17408477\n",
      "Iteration 5131, loss = 1.17407139\n",
      "Iteration 5132, loss = 1.17406197\n",
      "Iteration 5133, loss = 1.17405388\n",
      "Iteration 5134, loss = 1.17404085\n",
      "Iteration 5135, loss = 1.17402946\n",
      "Iteration 5136, loss = 1.17402125\n",
      "Iteration 5137, loss = 1.17401236\n",
      "Iteration 5138, loss = 1.17400123\n",
      "Iteration 5139, loss = 1.17399095\n",
      "Iteration 5140, loss = 1.17398180\n",
      "Iteration 5141, loss = 1.17396908\n",
      "Iteration 5142, loss = 1.17395838\n",
      "Iteration 5143, loss = 1.17394756\n",
      "Iteration 5144, loss = 1.17393744\n",
      "Iteration 5145, loss = 1.17392582\n",
      "Iteration 5146, loss = 1.17391587\n",
      "Iteration 5147, loss = 1.17390567\n",
      "Iteration 5148, loss = 1.17389436\n",
      "Iteration 5149, loss = 1.17388598\n",
      "Iteration 5150, loss = 1.17387776\n",
      "Iteration 5151, loss = 1.17386314\n",
      "Iteration 5152, loss = 1.17385465\n",
      "Iteration 5153, loss = 1.17384249\n",
      "Iteration 5154, loss = 1.17383204\n",
      "Iteration 5155, loss = 1.17382266\n",
      "Iteration 5156, loss = 1.17381382\n",
      "Iteration 5157, loss = 1.17380148\n",
      "Iteration 5158, loss = 1.17379367\n",
      "Iteration 5159, loss = 1.17378265\n",
      "Iteration 5160, loss = 1.17377576\n",
      "Iteration 5161, loss = 1.17375931\n",
      "Iteration 5162, loss = 1.17374889\n",
      "Iteration 5163, loss = 1.17374316\n",
      "Iteration 5164, loss = 1.17373284\n",
      "Iteration 5165, loss = 1.17371929\n",
      "Iteration 5166, loss = 1.17371429\n",
      "Iteration 5167, loss = 1.17370078\n",
      "Iteration 5168, loss = 1.17368882\n",
      "Iteration 5169, loss = 1.17367941\n",
      "Iteration 5170, loss = 1.17366764\n",
      "Iteration 5171, loss = 1.17365606\n",
      "Iteration 5172, loss = 1.17364637\n",
      "Iteration 5173, loss = 1.17363656\n",
      "Iteration 5174, loss = 1.17362551\n",
      "Iteration 5175, loss = 1.17361620\n",
      "Iteration 5176, loss = 1.17360653\n",
      "Iteration 5177, loss = 1.17360373\n",
      "Iteration 5178, loss = 1.17358449\n",
      "Iteration 5179, loss = 1.17357371\n",
      "Iteration 5180, loss = 1.17356407\n",
      "Iteration 5181, loss = 1.17355414\n",
      "Iteration 5182, loss = 1.17354411\n",
      "Iteration 5183, loss = 1.17353914\n",
      "Iteration 5184, loss = 1.17352298\n",
      "Iteration 5185, loss = 1.17351174\n",
      "Iteration 5186, loss = 1.17350292\n",
      "Iteration 5187, loss = 1.17349093\n",
      "Iteration 5188, loss = 1.17348256\n",
      "Iteration 5189, loss = 1.17347152\n",
      "Iteration 5190, loss = 1.17346249\n",
      "Iteration 5191, loss = 1.17345183\n",
      "Iteration 5192, loss = 1.17344090\n",
      "Iteration 5193, loss = 1.17343184\n",
      "Iteration 5194, loss = 1.17341899\n",
      "Iteration 5195, loss = 1.17341174\n",
      "Iteration 5196, loss = 1.17339758\n",
      "Iteration 5197, loss = 1.17338881\n",
      "Iteration 5198, loss = 1.17338001\n",
      "Iteration 5199, loss = 1.17336764\n",
      "Iteration 5200, loss = 1.17335717\n",
      "Iteration 5201, loss = 1.17334761\n",
      "Iteration 5202, loss = 1.17334001\n",
      "Iteration 5203, loss = 1.17332920\n",
      "Iteration 5204, loss = 1.17331695\n",
      "Iteration 5205, loss = 1.17330685\n",
      "Iteration 5206, loss = 1.17329868\n",
      "Iteration 5207, loss = 1.17328783\n",
      "Iteration 5208, loss = 1.17327664\n",
      "Iteration 5209, loss = 1.17326718\n",
      "Iteration 5210, loss = 1.17325412\n",
      "Iteration 5211, loss = 1.17324408\n",
      "Iteration 5212, loss = 1.17323519\n",
      "Iteration 5213, loss = 1.17322562\n",
      "Iteration 5214, loss = 1.17321319\n",
      "Iteration 5215, loss = 1.17320548\n",
      "Iteration 5216, loss = 1.17319840\n",
      "Iteration 5217, loss = 1.17318489\n",
      "Iteration 5218, loss = 1.17317982\n",
      "Iteration 5219, loss = 1.17316442\n",
      "Iteration 5220, loss = 1.17315572\n",
      "Iteration 5221, loss = 1.17314436\n",
      "Iteration 5222, loss = 1.17313480\n",
      "Iteration 5223, loss = 1.17312821\n",
      "Iteration 5224, loss = 1.17311439\n",
      "Iteration 5225, loss = 1.17310418\n",
      "Iteration 5226, loss = 1.17309227\n",
      "Iteration 5227, loss = 1.17308448\n",
      "Iteration 5228, loss = 1.17307107\n",
      "Iteration 5229, loss = 1.17306135\n",
      "Iteration 5230, loss = 1.17305317\n",
      "Iteration 5231, loss = 1.17304114\n",
      "Iteration 5232, loss = 1.17303397\n",
      "Iteration 5233, loss = 1.17302064\n",
      "Iteration 5234, loss = 1.17301154\n",
      "Iteration 5235, loss = 1.17299996\n",
      "Iteration 5236, loss = 1.17299361\n",
      "Iteration 5237, loss = 1.17298200\n",
      "Iteration 5238, loss = 1.17297067\n",
      "Iteration 5239, loss = 1.17295889\n",
      "Iteration 5240, loss = 1.17294885\n",
      "Iteration 5241, loss = 1.17293991\n",
      "Iteration 5242, loss = 1.17292947\n",
      "Iteration 5243, loss = 1.17291928\n",
      "Iteration 5244, loss = 1.17290817\n",
      "Iteration 5245, loss = 1.17289935\n",
      "Iteration 5246, loss = 1.17288955\n",
      "Iteration 5247, loss = 1.17287869\n",
      "Iteration 5248, loss = 1.17286880\n",
      "Iteration 5249, loss = 1.17285743\n",
      "Iteration 5250, loss = 1.17284980\n",
      "Iteration 5251, loss = 1.17284289\n",
      "Iteration 5252, loss = 1.17283114\n",
      "Iteration 5253, loss = 1.17281780\n",
      "Iteration 5254, loss = 1.17281308\n",
      "Iteration 5255, loss = 1.17279779\n",
      "Iteration 5256, loss = 1.17278789\n",
      "Iteration 5257, loss = 1.17277744\n",
      "Iteration 5258, loss = 1.17276888\n",
      "Iteration 5259, loss = 1.17275879\n",
      "Iteration 5260, loss = 1.17274934\n",
      "Iteration 5261, loss = 1.17273838\n",
      "Iteration 5262, loss = 1.17272845\n",
      "Iteration 5263, loss = 1.17271597\n",
      "Iteration 5264, loss = 1.17270533\n",
      "Iteration 5265, loss = 1.17269655\n",
      "Iteration 5266, loss = 1.17268632\n",
      "Iteration 5267, loss = 1.17267545\n",
      "Iteration 5268, loss = 1.17266678\n",
      "Iteration 5269, loss = 1.17265772\n",
      "Iteration 5270, loss = 1.17264611\n",
      "Iteration 5271, loss = 1.17263428\n",
      "Iteration 5272, loss = 1.17262473\n",
      "Iteration 5273, loss = 1.17262122\n",
      "Iteration 5274, loss = 1.17260629\n",
      "Iteration 5275, loss = 1.17259613\n",
      "Iteration 5276, loss = 1.17259152\n",
      "Iteration 5277, loss = 1.17257407\n",
      "Iteration 5278, loss = 1.17256420\n",
      "Iteration 5279, loss = 1.17255492\n",
      "Iteration 5280, loss = 1.17254801\n",
      "Iteration 5281, loss = 1.17253492\n",
      "Iteration 5282, loss = 1.17252743\n",
      "Iteration 5283, loss = 1.17252108\n",
      "Iteration 5284, loss = 1.17250705\n",
      "Iteration 5285, loss = 1.17249473\n",
      "Iteration 5286, loss = 1.17248565\n",
      "Iteration 5287, loss = 1.17247796\n",
      "Iteration 5288, loss = 1.17246638\n",
      "Iteration 5289, loss = 1.17245491\n",
      "Iteration 5290, loss = 1.17245215\n",
      "Iteration 5291, loss = 1.17243643\n",
      "Iteration 5292, loss = 1.17242647\n",
      "Iteration 5293, loss = 1.17241383\n",
      "Iteration 5294, loss = 1.17240546\n",
      "Iteration 5295, loss = 1.17239753\n",
      "Iteration 5296, loss = 1.17238966\n",
      "Iteration 5297, loss = 1.17237789\n",
      "Iteration 5298, loss = 1.17236567\n",
      "Iteration 5299, loss = 1.17235361\n",
      "Iteration 5300, loss = 1.17234492\n",
      "Iteration 5301, loss = 1.17233419\n",
      "Iteration 5302, loss = 1.17232438\n",
      "Iteration 5303, loss = 1.17231666\n",
      "Iteration 5304, loss = 1.17230473\n",
      "Iteration 5305, loss = 1.17229402\n",
      "Iteration 5306, loss = 1.17228331\n",
      "Iteration 5307, loss = 1.17227394\n",
      "Iteration 5308, loss = 1.17226278\n",
      "Iteration 5309, loss = 1.17225296\n",
      "Iteration 5310, loss = 1.17224684\n",
      "Iteration 5311, loss = 1.17223496\n",
      "Iteration 5312, loss = 1.17222994\n",
      "Iteration 5313, loss = 1.17222036\n",
      "Iteration 5314, loss = 1.17220772\n",
      "Iteration 5315, loss = 1.17219389\n",
      "Iteration 5316, loss = 1.17218325\n",
      "Iteration 5317, loss = 1.17217494\n",
      "Iteration 5318, loss = 1.17216802\n",
      "Iteration 5319, loss = 1.17215174\n",
      "Iteration 5320, loss = 1.17215056\n",
      "Iteration 5321, loss = 1.17213420\n",
      "Iteration 5322, loss = 1.17212218\n",
      "Iteration 5323, loss = 1.17211451\n",
      "Iteration 5324, loss = 1.17210394\n",
      "Iteration 5325, loss = 1.17209330\n",
      "Iteration 5326, loss = 1.17208222\n",
      "Iteration 5327, loss = 1.17207497\n",
      "Iteration 5328, loss = 1.17206458\n",
      "Iteration 5329, loss = 1.17205687\n",
      "Iteration 5330, loss = 1.17204376\n",
      "Iteration 5331, loss = 1.17203327\n",
      "Iteration 5332, loss = 1.17202388\n",
      "Iteration 5333, loss = 1.17201307\n",
      "Iteration 5334, loss = 1.17200338\n",
      "Iteration 5335, loss = 1.17199255\n",
      "Iteration 5336, loss = 1.17198418\n",
      "Iteration 5337, loss = 1.17197449\n",
      "Iteration 5338, loss = 1.17196829\n",
      "Iteration 5339, loss = 1.17195379\n",
      "Iteration 5340, loss = 1.17194693\n",
      "Iteration 5341, loss = 1.17193195\n",
      "Iteration 5342, loss = 1.17192275\n",
      "Iteration 5343, loss = 1.17191747\n",
      "Iteration 5344, loss = 1.17190336\n",
      "Iteration 5345, loss = 1.17189400\n",
      "Iteration 5346, loss = 1.17188491\n",
      "Iteration 5347, loss = 1.17187665\n",
      "Iteration 5348, loss = 1.17186605\n",
      "Iteration 5349, loss = 1.17185612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5350, loss = 1.17184326\n",
      "Iteration 5351, loss = 1.17183542\n",
      "Iteration 5352, loss = 1.17182324\n",
      "Iteration 5353, loss = 1.17181408\n",
      "Iteration 5354, loss = 1.17180393\n",
      "Iteration 5355, loss = 1.17179487\n",
      "Iteration 5356, loss = 1.17178975\n",
      "Iteration 5357, loss = 1.17177864\n",
      "Iteration 5358, loss = 1.17176576\n",
      "Iteration 5359, loss = 1.17175566\n",
      "Iteration 5360, loss = 1.17174546\n",
      "Iteration 5361, loss = 1.17173652\n",
      "Iteration 5362, loss = 1.17172488\n",
      "Iteration 5363, loss = 1.17171875\n",
      "Iteration 5364, loss = 1.17170842\n",
      "Iteration 5365, loss = 1.17169564\n",
      "Iteration 5366, loss = 1.17168420\n",
      "Iteration 5367, loss = 1.17167802\n",
      "Iteration 5368, loss = 1.17166714\n",
      "Iteration 5369, loss = 1.17165727\n",
      "Iteration 5370, loss = 1.17164981\n",
      "Iteration 5371, loss = 1.17164051\n",
      "Iteration 5372, loss = 1.17162432\n",
      "Iteration 5373, loss = 1.17161832\n",
      "Iteration 5374, loss = 1.17160772\n",
      "Iteration 5375, loss = 1.17159886\n",
      "Iteration 5376, loss = 1.17159052\n",
      "Iteration 5377, loss = 1.17157671\n",
      "Iteration 5378, loss = 1.17156570\n",
      "Iteration 5379, loss = 1.17155784\n",
      "Iteration 5380, loss = 1.17154959\n",
      "Iteration 5381, loss = 1.17153670\n",
      "Iteration 5382, loss = 1.17152730\n",
      "Iteration 5383, loss = 1.17151627\n",
      "Iteration 5384, loss = 1.17150596\n",
      "Iteration 5385, loss = 1.17149865\n",
      "Iteration 5386, loss = 1.17149313\n",
      "Iteration 5387, loss = 1.17147787\n",
      "Iteration 5388, loss = 1.17147150\n",
      "Iteration 5389, loss = 1.17146343\n",
      "Iteration 5390, loss = 1.17145357\n",
      "Iteration 5391, loss = 1.17143776\n",
      "Iteration 5392, loss = 1.17143032\n",
      "Iteration 5393, loss = 1.17141889\n",
      "Iteration 5394, loss = 1.17141025\n",
      "Iteration 5395, loss = 1.17140439\n",
      "Iteration 5396, loss = 1.17139174\n",
      "Iteration 5397, loss = 1.17138413\n",
      "Iteration 5398, loss = 1.17137342\n",
      "Iteration 5399, loss = 1.17136161\n",
      "Iteration 5400, loss = 1.17135024\n",
      "Iteration 5401, loss = 1.17134148\n",
      "Iteration 5402, loss = 1.17133083\n",
      "Iteration 5403, loss = 1.17132154\n",
      "Iteration 5404, loss = 1.17131067\n",
      "Iteration 5405, loss = 1.17130609\n",
      "Iteration 5406, loss = 1.17129439\n",
      "Iteration 5407, loss = 1.17128601\n",
      "Iteration 5408, loss = 1.17127284\n",
      "Iteration 5409, loss = 1.17126330\n",
      "Iteration 5410, loss = 1.17125353\n",
      "Iteration 5411, loss = 1.17124483\n",
      "Iteration 5412, loss = 1.17124296\n",
      "Iteration 5413, loss = 1.17123159\n",
      "Iteration 5414, loss = 1.17121367\n",
      "Iteration 5415, loss = 1.17120565\n",
      "Iteration 5416, loss = 1.17119284\n",
      "Iteration 5417, loss = 1.17118799\n",
      "Iteration 5418, loss = 1.17117487\n",
      "Iteration 5419, loss = 1.17116614\n",
      "Iteration 5420, loss = 1.17115486\n",
      "Iteration 5421, loss = 1.17116308\n",
      "Iteration 5422, loss = 1.17113545\n",
      "Iteration 5423, loss = 1.17112563\n",
      "Iteration 5424, loss = 1.17111482\n",
      "Iteration 5425, loss = 1.17111019\n",
      "Iteration 5426, loss = 1.17109734\n",
      "Iteration 5427, loss = 1.17108636\n",
      "Iteration 5428, loss = 1.17107532\n",
      "Iteration 5429, loss = 1.17106763\n",
      "Iteration 5430, loss = 1.17105652\n",
      "Iteration 5431, loss = 1.17104938\n",
      "Iteration 5432, loss = 1.17103636\n",
      "Iteration 5433, loss = 1.17103554\n",
      "Iteration 5434, loss = 1.17101968\n",
      "Iteration 5435, loss = 1.17101342\n",
      "Iteration 5436, loss = 1.17100314\n",
      "Iteration 5437, loss = 1.17099216\n",
      "Iteration 5438, loss = 1.17098237\n",
      "Iteration 5439, loss = 1.17097724\n",
      "Iteration 5440, loss = 1.17096023\n",
      "Iteration 5441, loss = 1.17095002\n",
      "Iteration 5442, loss = 1.17094277\n",
      "Iteration 5443, loss = 1.17093305\n",
      "Iteration 5444, loss = 1.17092363\n",
      "Iteration 5445, loss = 1.17091380\n",
      "Iteration 5446, loss = 1.17090360\n",
      "Iteration 5447, loss = 1.17089075\n",
      "Iteration 5448, loss = 1.17088383\n",
      "Iteration 5449, loss = 1.17087412\n",
      "Iteration 5450, loss = 1.17086229\n",
      "Iteration 5451, loss = 1.17085399\n",
      "Iteration 5452, loss = 1.17084323\n",
      "Iteration 5453, loss = 1.17083845\n",
      "Iteration 5454, loss = 1.17082371\n",
      "Iteration 5455, loss = 1.17081423\n",
      "Iteration 5456, loss = 1.17080772\n",
      "Iteration 5457, loss = 1.17079451\n",
      "Iteration 5458, loss = 1.17078465\n",
      "Iteration 5459, loss = 1.17077766\n",
      "Iteration 5460, loss = 1.17076794\n",
      "Iteration 5461, loss = 1.17076148\n",
      "Iteration 5462, loss = 1.17074655\n",
      "Iteration 5463, loss = 1.17073994\n",
      "Iteration 5464, loss = 1.17072753\n",
      "Iteration 5465, loss = 1.17071741\n",
      "Iteration 5466, loss = 1.17071163\n",
      "Iteration 5467, loss = 1.17069983\n",
      "Iteration 5468, loss = 1.17068924\n",
      "Iteration 5469, loss = 1.17067998\n",
      "Iteration 5470, loss = 1.17066906\n",
      "Iteration 5471, loss = 1.17066153\n",
      "Iteration 5472, loss = 1.17064940\n",
      "Iteration 5473, loss = 1.17064381\n",
      "Iteration 5474, loss = 1.17063121\n",
      "Iteration 5475, loss = 1.17062087\n",
      "Iteration 5476, loss = 1.17062224\n",
      "Iteration 5477, loss = 1.17060163\n",
      "Iteration 5478, loss = 1.17059331\n",
      "Iteration 5479, loss = 1.17058259\n",
      "Iteration 5480, loss = 1.17057503\n",
      "Iteration 5481, loss = 1.17056423\n",
      "Iteration 5482, loss = 1.17055855\n",
      "Iteration 5483, loss = 1.17054344\n",
      "Iteration 5484, loss = 1.17053570\n",
      "Iteration 5485, loss = 1.17052300\n",
      "Iteration 5486, loss = 1.17051860\n",
      "Iteration 5487, loss = 1.17051002\n",
      "Iteration 5488, loss = 1.17049784\n",
      "Iteration 5489, loss = 1.17048843\n",
      "Iteration 5490, loss = 1.17048393\n",
      "Iteration 5491, loss = 1.17047012\n",
      "Iteration 5492, loss = 1.17047370\n",
      "Iteration 5493, loss = 1.17045835\n",
      "Iteration 5494, loss = 1.17044022\n",
      "Iteration 5495, loss = 1.17042931\n",
      "Iteration 5496, loss = 1.17041822\n",
      "Iteration 5497, loss = 1.17040783\n",
      "Iteration 5498, loss = 1.17040128\n",
      "Iteration 5499, loss = 1.17038944\n",
      "Iteration 5500, loss = 1.17038152\n",
      "Iteration 5501, loss = 1.17037431\n",
      "Iteration 5502, loss = 1.17036185\n",
      "Iteration 5503, loss = 1.17036065\n",
      "Iteration 5504, loss = 1.17034239\n",
      "Iteration 5505, loss = 1.17033267\n",
      "Iteration 5506, loss = 1.17032492\n",
      "Iteration 5507, loss = 1.17031594\n",
      "Iteration 5508, loss = 1.17030661\n",
      "Iteration 5509, loss = 1.17030135\n",
      "Iteration 5510, loss = 1.17028376\n",
      "Iteration 5511, loss = 1.17027604\n",
      "Iteration 5512, loss = 1.17027501\n",
      "Iteration 5513, loss = 1.17025860\n",
      "Iteration 5514, loss = 1.17024523\n",
      "Iteration 5515, loss = 1.17023636\n",
      "Iteration 5516, loss = 1.17022653\n",
      "Iteration 5517, loss = 1.17021500\n",
      "Iteration 5518, loss = 1.17020767\n",
      "Iteration 5519, loss = 1.17019879\n",
      "Iteration 5520, loss = 1.17018909\n",
      "Iteration 5521, loss = 1.17018434\n",
      "Iteration 5522, loss = 1.17017194\n",
      "Iteration 5523, loss = 1.17016200\n",
      "Iteration 5524, loss = 1.17014939\n",
      "Iteration 5525, loss = 1.17013971\n",
      "Iteration 5526, loss = 1.17013073\n",
      "Iteration 5527, loss = 1.17012110\n",
      "Iteration 5528, loss = 1.17011011\n",
      "Iteration 5529, loss = 1.17010616\n",
      "Iteration 5530, loss = 1.17009265\n",
      "Iteration 5531, loss = 1.17008274\n",
      "Iteration 5532, loss = 1.17007515\n",
      "Iteration 5533, loss = 1.17006403\n",
      "Iteration 5534, loss = 1.17005625\n",
      "Iteration 5535, loss = 1.17004783\n",
      "Iteration 5536, loss = 1.17003818\n",
      "Iteration 5537, loss = 1.17002573\n",
      "Iteration 5538, loss = 1.17001541\n",
      "Iteration 5539, loss = 1.17000589\n",
      "Iteration 5540, loss = 1.16999792\n",
      "Iteration 5541, loss = 1.16998608\n",
      "Iteration 5542, loss = 1.16997708\n",
      "Iteration 5543, loss = 1.16996853\n",
      "Iteration 5544, loss = 1.16996405\n",
      "Iteration 5545, loss = 1.16994881\n",
      "Iteration 5546, loss = 1.16994377\n",
      "Iteration 5547, loss = 1.16993089\n",
      "Iteration 5548, loss = 1.16992086\n",
      "Iteration 5549, loss = 1.16991063\n",
      "Iteration 5550, loss = 1.16990093\n",
      "Iteration 5551, loss = 1.16989341\n",
      "Iteration 5552, loss = 1.16988267\n",
      "Iteration 5553, loss = 1.16987526\n",
      "Iteration 5554, loss = 1.16986195\n",
      "Iteration 5555, loss = 1.16985272\n",
      "Iteration 5556, loss = 1.16984802\n",
      "Iteration 5557, loss = 1.16983797\n",
      "Iteration 5558, loss = 1.16982554\n",
      "Iteration 5559, loss = 1.16981508\n",
      "Iteration 5560, loss = 1.16980548\n",
      "Iteration 5561, loss = 1.16979754\n",
      "Iteration 5562, loss = 1.16978818\n",
      "Iteration 5563, loss = 1.16977800\n",
      "Iteration 5564, loss = 1.16976702\n",
      "Iteration 5565, loss = 1.16976264\n",
      "Iteration 5566, loss = 1.16975272\n",
      "Iteration 5567, loss = 1.16974138\n",
      "Iteration 5568, loss = 1.16973354\n",
      "Iteration 5569, loss = 1.16972032\n",
      "Iteration 5570, loss = 1.16971412\n",
      "Iteration 5571, loss = 1.16970364\n",
      "Iteration 5572, loss = 1.16969127\n",
      "Iteration 5573, loss = 1.16968544\n",
      "Iteration 5574, loss = 1.16967618\n",
      "Iteration 5575, loss = 1.16966565\n",
      "Iteration 5576, loss = 1.16966115\n",
      "Iteration 5577, loss = 1.16964421\n",
      "Iteration 5578, loss = 1.16964321\n",
      "Iteration 5579, loss = 1.16962850\n",
      "Iteration 5580, loss = 1.16961834\n",
      "Iteration 5581, loss = 1.16960973\n",
      "Iteration 5582, loss = 1.16959833\n",
      "Iteration 5583, loss = 1.16959695\n",
      "Iteration 5584, loss = 1.16957845\n",
      "Iteration 5585, loss = 1.16957022\n",
      "Iteration 5586, loss = 1.16955912\n",
      "Iteration 5587, loss = 1.16955056\n",
      "Iteration 5588, loss = 1.16954067\n",
      "Iteration 5589, loss = 1.16953104\n",
      "Iteration 5590, loss = 1.16952243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5591, loss = 1.16951350\n",
      "Iteration 5592, loss = 1.16950196\n",
      "Iteration 5593, loss = 1.16949531\n",
      "Iteration 5594, loss = 1.16948662\n",
      "Iteration 5595, loss = 1.16947430\n",
      "Iteration 5596, loss = 1.16946809\n",
      "Iteration 5597, loss = 1.16945661\n",
      "Iteration 5598, loss = 1.16944557\n",
      "Iteration 5599, loss = 1.16943681\n",
      "Iteration 5600, loss = 1.16942826\n",
      "Iteration 5601, loss = 1.16942667\n",
      "Iteration 5602, loss = 1.16941033\n",
      "Iteration 5603, loss = 1.16939765\n",
      "Iteration 5604, loss = 1.16938918\n",
      "Iteration 5605, loss = 1.16937997\n",
      "Iteration 5606, loss = 1.16937274\n",
      "Iteration 5607, loss = 1.16937078\n",
      "Iteration 5608, loss = 1.16935229\n",
      "Iteration 5609, loss = 1.16934274\n",
      "Iteration 5610, loss = 1.16933236\n",
      "Iteration 5611, loss = 1.16932387\n",
      "Iteration 5612, loss = 1.16931447\n",
      "Iteration 5613, loss = 1.16930936\n",
      "Iteration 5614, loss = 1.16929617\n",
      "Iteration 5615, loss = 1.16929069\n",
      "Iteration 5616, loss = 1.16927881\n",
      "Iteration 5617, loss = 1.16926704\n",
      "Iteration 5618, loss = 1.16925711\n",
      "Iteration 5619, loss = 1.16924778\n",
      "Iteration 5620, loss = 1.16924078\n",
      "Iteration 5621, loss = 1.16923072\n",
      "Iteration 5622, loss = 1.16922153\n",
      "Iteration 5623, loss = 1.16921248\n",
      "Iteration 5624, loss = 1.16920354\n",
      "Iteration 5625, loss = 1.16919188\n",
      "Iteration 5626, loss = 1.16918221\n",
      "Iteration 5627, loss = 1.16917693\n",
      "Iteration 5628, loss = 1.16916549\n",
      "Iteration 5629, loss = 1.16915818\n",
      "Iteration 5630, loss = 1.16914590\n",
      "Iteration 5631, loss = 1.16913606\n",
      "Iteration 5632, loss = 1.16912842\n",
      "Iteration 5633, loss = 1.16911966\n",
      "Iteration 5634, loss = 1.16910985\n",
      "Iteration 5635, loss = 1.16909734\n",
      "Iteration 5636, loss = 1.16909505\n",
      "Iteration 5637, loss = 1.16907968\n",
      "Iteration 5638, loss = 1.16907278\n",
      "Iteration 5639, loss = 1.16906018\n",
      "Iteration 5640, loss = 1.16905688\n",
      "Iteration 5641, loss = 1.16904387\n",
      "Iteration 5642, loss = 1.16903373\n",
      "Iteration 5643, loss = 1.16902371\n",
      "Iteration 5644, loss = 1.16901801\n",
      "Iteration 5645, loss = 1.16900435\n",
      "Iteration 5646, loss = 1.16899974\n",
      "Iteration 5647, loss = 1.16899091\n",
      "Iteration 5648, loss = 1.16897735\n",
      "Iteration 5649, loss = 1.16896886\n",
      "Iteration 5650, loss = 1.16895863\n",
      "Iteration 5651, loss = 1.16894912\n",
      "Iteration 5652, loss = 1.16894005\n",
      "Iteration 5653, loss = 1.16893145\n",
      "Iteration 5654, loss = 1.16892305\n",
      "Iteration 5655, loss = 1.16891377\n",
      "Iteration 5656, loss = 1.16890712\n",
      "Iteration 5657, loss = 1.16889868\n",
      "Iteration 5658, loss = 1.16888521\n",
      "Iteration 5659, loss = 1.16887911\n",
      "Iteration 5660, loss = 1.16886985\n",
      "Iteration 5661, loss = 1.16886331\n",
      "Iteration 5662, loss = 1.16885522\n",
      "Iteration 5663, loss = 1.16883986\n",
      "Iteration 5664, loss = 1.16883008\n",
      "Iteration 5665, loss = 1.16881816\n",
      "Iteration 5666, loss = 1.16881271\n",
      "Iteration 5667, loss = 1.16880435\n",
      "Iteration 5668, loss = 1.16879211\n",
      "Iteration 5669, loss = 1.16878082\n",
      "Iteration 5670, loss = 1.16877762\n",
      "Iteration 5671, loss = 1.16876869\n",
      "Iteration 5672, loss = 1.16875478\n",
      "Iteration 5673, loss = 1.16875991\n",
      "Iteration 5674, loss = 1.16874069\n",
      "Iteration 5675, loss = 1.16872885\n",
      "Iteration 5676, loss = 1.16872140\n",
      "Iteration 5677, loss = 1.16871971\n",
      "Iteration 5678, loss = 1.16870106\n",
      "Iteration 5679, loss = 1.16869364\n",
      "Iteration 5680, loss = 1.16868205\n",
      "Iteration 5681, loss = 1.16867115\n",
      "Iteration 5682, loss = 1.16866184\n",
      "Iteration 5683, loss = 1.16865240\n",
      "Iteration 5684, loss = 1.16864391\n",
      "Iteration 5685, loss = 1.16863264\n",
      "Iteration 5686, loss = 1.16862564\n",
      "Iteration 5687, loss = 1.16861402\n",
      "Iteration 5688, loss = 1.16860702\n",
      "Iteration 5689, loss = 1.16859670\n",
      "Iteration 5690, loss = 1.16860336\n",
      "Iteration 5691, loss = 1.16858128\n",
      "Iteration 5692, loss = 1.16857060\n",
      "Iteration 5693, loss = 1.16856075\n",
      "Iteration 5694, loss = 1.16855282\n",
      "Iteration 5695, loss = 1.16854619\n",
      "Iteration 5696, loss = 1.16853275\n",
      "Iteration 5697, loss = 1.16852458\n",
      "Iteration 5698, loss = 1.16851698\n",
      "Iteration 5699, loss = 1.16850557\n",
      "Iteration 5700, loss = 1.16850152\n",
      "Iteration 5701, loss = 1.16848472\n",
      "Iteration 5702, loss = 1.16847987\n",
      "Iteration 5703, loss = 1.16846869\n",
      "Iteration 5704, loss = 1.16845905\n",
      "Iteration 5705, loss = 1.16845185\n",
      "Iteration 5706, loss = 1.16844207\n",
      "Iteration 5707, loss = 1.16843015\n",
      "Iteration 5708, loss = 1.16842211\n",
      "Iteration 5709, loss = 1.16841542\n",
      "Iteration 5710, loss = 1.16840266\n",
      "Iteration 5711, loss = 1.16839459\n",
      "Iteration 5712, loss = 1.16838487\n",
      "Iteration 5713, loss = 1.16837757\n",
      "Iteration 5714, loss = 1.16836888\n",
      "Iteration 5715, loss = 1.16835914\n",
      "Iteration 5716, loss = 1.16835622\n",
      "Iteration 5717, loss = 1.16834371\n",
      "Iteration 5718, loss = 1.16833067\n",
      "Iteration 5719, loss = 1.16832140\n",
      "Iteration 5720, loss = 1.16831626\n",
      "Iteration 5721, loss = 1.16830402\n",
      "Iteration 5722, loss = 1.16829303\n",
      "Iteration 5723, loss = 1.16828458\n",
      "Iteration 5724, loss = 1.16827458\n",
      "Iteration 5725, loss = 1.16826495\n",
      "Iteration 5726, loss = 1.16826113\n",
      "Iteration 5727, loss = 1.16824549\n",
      "Iteration 5728, loss = 1.16823703\n",
      "Iteration 5729, loss = 1.16823226\n",
      "Iteration 5730, loss = 1.16821850\n",
      "Iteration 5731, loss = 1.16820980\n",
      "Iteration 5732, loss = 1.16820140\n",
      "Iteration 5733, loss = 1.16819102\n",
      "Iteration 5734, loss = 1.16818319\n",
      "Iteration 5735, loss = 1.16817372\n",
      "Iteration 5736, loss = 1.16816454\n",
      "Iteration 5737, loss = 1.16815419\n",
      "Iteration 5738, loss = 1.16814748\n",
      "Iteration 5739, loss = 1.16813984\n",
      "Iteration 5740, loss = 1.16813041\n",
      "Iteration 5741, loss = 1.16812001\n",
      "Iteration 5742, loss = 1.16810927\n",
      "Iteration 5743, loss = 1.16810101\n",
      "Iteration 5744, loss = 1.16809042\n",
      "Iteration 5745, loss = 1.16808351\n",
      "Iteration 5746, loss = 1.16807723\n",
      "Iteration 5747, loss = 1.16807073\n",
      "Iteration 5748, loss = 1.16805420\n",
      "Iteration 5749, loss = 1.16804461\n",
      "Iteration 5750, loss = 1.16803539\n",
      "Iteration 5751, loss = 1.16802912\n",
      "Iteration 5752, loss = 1.16801773\n",
      "Iteration 5753, loss = 1.16800808\n",
      "Iteration 5754, loss = 1.16800398\n",
      "Iteration 5755, loss = 1.16798919\n",
      "Iteration 5756, loss = 1.16798175\n",
      "Iteration 5757, loss = 1.16797247\n",
      "Iteration 5758, loss = 1.16796185\n",
      "Iteration 5759, loss = 1.16795311\n",
      "Iteration 5760, loss = 1.16794423\n",
      "Iteration 5761, loss = 1.16793488\n",
      "Iteration 5762, loss = 1.16792615\n",
      "Iteration 5763, loss = 1.16791626\n",
      "Iteration 5764, loss = 1.16790793\n",
      "Iteration 5765, loss = 1.16790280\n",
      "Iteration 5766, loss = 1.16789027\n",
      "Iteration 5767, loss = 1.16788201\n",
      "Iteration 5768, loss = 1.16787676\n",
      "Iteration 5769, loss = 1.16786393\n",
      "Iteration 5770, loss = 1.16785444\n",
      "Iteration 5771, loss = 1.16784455\n",
      "Iteration 5772, loss = 1.16783762\n",
      "Iteration 5773, loss = 1.16782783\n",
      "Iteration 5774, loss = 1.16782003\n",
      "Iteration 5775, loss = 1.16781042\n",
      "Iteration 5776, loss = 1.16779829\n",
      "Iteration 5777, loss = 1.16778916\n",
      "Iteration 5778, loss = 1.16778560\n",
      "Iteration 5779, loss = 1.16777869\n",
      "Iteration 5780, loss = 1.16776479\n",
      "Iteration 5781, loss = 1.16775452\n",
      "Iteration 5782, loss = 1.16774570\n",
      "Iteration 5783, loss = 1.16774027\n",
      "Iteration 5784, loss = 1.16773511\n",
      "Iteration 5785, loss = 1.16771998\n",
      "Iteration 5786, loss = 1.16770903\n",
      "Iteration 5787, loss = 1.16769898\n",
      "Iteration 5788, loss = 1.16768913\n",
      "Iteration 5789, loss = 1.16769228\n",
      "Iteration 5790, loss = 1.16767233\n",
      "Iteration 5791, loss = 1.16766277\n",
      "Iteration 5792, loss = 1.16765470\n",
      "Iteration 5793, loss = 1.16764706\n",
      "Iteration 5794, loss = 1.16764208\n",
      "Iteration 5795, loss = 1.16762631\n",
      "Iteration 5796, loss = 1.16762045\n",
      "Iteration 5797, loss = 1.16760870\n",
      "Iteration 5798, loss = 1.16760166\n",
      "Iteration 5799, loss = 1.16759383\n",
      "Iteration 5800, loss = 1.16758301\n",
      "Iteration 5801, loss = 1.16757494\n",
      "Iteration 5802, loss = 1.16756103\n",
      "Iteration 5803, loss = 1.16755510\n",
      "Iteration 5804, loss = 1.16754738\n",
      "Iteration 5805, loss = 1.16753925\n",
      "Iteration 5806, loss = 1.16753316\n",
      "Iteration 5807, loss = 1.16752183\n",
      "Iteration 5808, loss = 1.16751397\n",
      "Iteration 5809, loss = 1.16750120\n",
      "Iteration 5810, loss = 1.16749216\n",
      "Iteration 5811, loss = 1.16748258\n",
      "Iteration 5812, loss = 1.16747223\n",
      "Iteration 5813, loss = 1.16746472\n",
      "Iteration 5814, loss = 1.16745432\n",
      "Iteration 5815, loss = 1.16744500\n",
      "Iteration 5816, loss = 1.16743634\n",
      "Iteration 5817, loss = 1.16742893\n",
      "Iteration 5818, loss = 1.16741939\n",
      "Iteration 5819, loss = 1.16741456\n",
      "Iteration 5820, loss = 1.16740019\n",
      "Iteration 5821, loss = 1.16739082\n",
      "Iteration 5822, loss = 1.16739196\n",
      "Iteration 5823, loss = 1.16737597\n",
      "Iteration 5824, loss = 1.16736306\n",
      "Iteration 5825, loss = 1.16735553\n",
      "Iteration 5826, loss = 1.16734767\n",
      "Iteration 5827, loss = 1.16733653\n",
      "Iteration 5828, loss = 1.16732919\n",
      "Iteration 5829, loss = 1.16732303\n",
      "Iteration 5830, loss = 1.16730999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5831, loss = 1.16730795\n",
      "Iteration 5832, loss = 1.16729257\n",
      "Iteration 5833, loss = 1.16728579\n",
      "Iteration 5834, loss = 1.16727582\n",
      "Iteration 5835, loss = 1.16726884\n",
      "Iteration 5836, loss = 1.16725760\n",
      "Iteration 5837, loss = 1.16725129\n",
      "Iteration 5838, loss = 1.16724279\n",
      "Iteration 5839, loss = 1.16723594\n",
      "Iteration 5840, loss = 1.16722113\n",
      "Iteration 5841, loss = 1.16721491\n",
      "Iteration 5842, loss = 1.16720204\n",
      "Iteration 5843, loss = 1.16719625\n",
      "Iteration 5844, loss = 1.16718451\n",
      "Iteration 5845, loss = 1.16717528\n",
      "Iteration 5846, loss = 1.16716994\n",
      "Iteration 5847, loss = 1.16715904\n",
      "Iteration 5848, loss = 1.16715106\n",
      "Iteration 5849, loss = 1.16714004\n",
      "Iteration 5850, loss = 1.16713677\n",
      "Iteration 5851, loss = 1.16712326\n",
      "Iteration 5852, loss = 1.16711451\n",
      "Iteration 5853, loss = 1.16710471\n",
      "Iteration 5854, loss = 1.16709387\n",
      "Iteration 5855, loss = 1.16708677\n",
      "Iteration 5856, loss = 1.16708279\n",
      "Iteration 5857, loss = 1.16707060\n",
      "Iteration 5858, loss = 1.16706041\n",
      "Iteration 5859, loss = 1.16705756\n",
      "Iteration 5860, loss = 1.16704265\n",
      "Iteration 5861, loss = 1.16703195\n",
      "Iteration 5862, loss = 1.16702661\n",
      "Iteration 5863, loss = 1.16701327\n",
      "Iteration 5864, loss = 1.16700616\n",
      "Iteration 5865, loss = 1.16699569\n",
      "Iteration 5866, loss = 1.16698917\n",
      "Iteration 5867, loss = 1.16698320\n",
      "Iteration 5868, loss = 1.16697342\n",
      "Iteration 5869, loss = 1.16696383\n",
      "Iteration 5870, loss = 1.16695194\n",
      "Iteration 5871, loss = 1.16694381\n",
      "Iteration 5872, loss = 1.16693707\n",
      "Iteration 5873, loss = 1.16692860\n",
      "Iteration 5874, loss = 1.16691872\n",
      "Iteration 5875, loss = 1.16691002\n",
      "Iteration 5876, loss = 1.16690005\n",
      "Iteration 5877, loss = 1.16689171\n",
      "Iteration 5878, loss = 1.16688509\n",
      "Iteration 5879, loss = 1.16687350\n",
      "Iteration 5880, loss = 1.16686662\n",
      "Iteration 5881, loss = 1.16685438\n",
      "Iteration 5882, loss = 1.16684786\n",
      "Iteration 5883, loss = 1.16683917\n",
      "Iteration 5884, loss = 1.16682776\n",
      "Iteration 5885, loss = 1.16682195\n",
      "Iteration 5886, loss = 1.16681552\n",
      "Iteration 5887, loss = 1.16680372\n",
      "Iteration 5888, loss = 1.16679315\n",
      "Iteration 5889, loss = 1.16679184\n",
      "Iteration 5890, loss = 1.16677337\n",
      "Iteration 5891, loss = 1.16676460\n",
      "Iteration 5892, loss = 1.16675729\n",
      "Iteration 5893, loss = 1.16674933\n",
      "Iteration 5894, loss = 1.16673948\n",
      "Iteration 5895, loss = 1.16673637\n",
      "Iteration 5896, loss = 1.16672080\n",
      "Iteration 5897, loss = 1.16671224\n",
      "Iteration 5898, loss = 1.16670377\n",
      "Iteration 5899, loss = 1.16669333\n",
      "Iteration 5900, loss = 1.16668498\n",
      "Iteration 5901, loss = 1.16667633\n",
      "Iteration 5902, loss = 1.16667058\n",
      "Iteration 5903, loss = 1.16665774\n",
      "Iteration 5904, loss = 1.16664885\n",
      "Iteration 5905, loss = 1.16664063\n",
      "Iteration 5906, loss = 1.16663332\n",
      "Iteration 5907, loss = 1.16662805\n",
      "Iteration 5908, loss = 1.16661618\n",
      "Iteration 5909, loss = 1.16661054\n",
      "Iteration 5910, loss = 1.16659819\n",
      "Iteration 5911, loss = 1.16658996\n",
      "Iteration 5912, loss = 1.16657934\n",
      "Iteration 5913, loss = 1.16657329\n",
      "Iteration 5914, loss = 1.16656273\n",
      "Iteration 5915, loss = 1.16655158\n",
      "Iteration 5916, loss = 1.16654570\n",
      "Iteration 5917, loss = 1.16654341\n",
      "Iteration 5918, loss = 1.16652669\n",
      "Iteration 5919, loss = 1.16651641\n",
      "Iteration 5920, loss = 1.16650921\n",
      "Iteration 5921, loss = 1.16650097\n",
      "Iteration 5922, loss = 1.16649358\n",
      "Iteration 5923, loss = 1.16648290\n",
      "Iteration 5924, loss = 1.16647277\n",
      "Iteration 5925, loss = 1.16646361\n",
      "Iteration 5926, loss = 1.16645520\n",
      "Iteration 5927, loss = 1.16644913\n",
      "Iteration 5928, loss = 1.16644282\n",
      "Iteration 5929, loss = 1.16642795\n",
      "Iteration 5930, loss = 1.16641952\n",
      "Iteration 5931, loss = 1.16641296\n",
      "Iteration 5932, loss = 1.16641056\n",
      "Iteration 5933, loss = 1.16639684\n",
      "Iteration 5934, loss = 1.16638524\n",
      "Iteration 5935, loss = 1.16637649\n",
      "Iteration 5936, loss = 1.16636701\n",
      "Iteration 5937, loss = 1.16635820\n",
      "Iteration 5938, loss = 1.16634845\n",
      "Iteration 5939, loss = 1.16634187\n",
      "Iteration 5940, loss = 1.16633363\n",
      "Iteration 5941, loss = 1.16632082\n",
      "Iteration 5942, loss = 1.16631438\n",
      "Iteration 5943, loss = 1.16630455\n",
      "Iteration 5944, loss = 1.16629815\n",
      "Iteration 5945, loss = 1.16629140\n",
      "Iteration 5946, loss = 1.16627940\n",
      "Iteration 5947, loss = 1.16627519\n",
      "Iteration 5948, loss = 1.16626414\n",
      "Iteration 5949, loss = 1.16625508\n",
      "Iteration 5950, loss = 1.16624782\n",
      "Iteration 5951, loss = 1.16623616\n",
      "Iteration 5952, loss = 1.16622563\n",
      "Iteration 5953, loss = 1.16621979\n",
      "Iteration 5954, loss = 1.16620743\n",
      "Iteration 5955, loss = 1.16620204\n",
      "Iteration 5956, loss = 1.16619472\n",
      "Iteration 5957, loss = 1.16618571\n",
      "Iteration 5958, loss = 1.16617668\n",
      "Iteration 5959, loss = 1.16616467\n",
      "Iteration 5960, loss = 1.16616347\n",
      "Iteration 5961, loss = 1.16614700\n",
      "Iteration 5962, loss = 1.16613780\n",
      "Iteration 5963, loss = 1.16612972\n",
      "Iteration 5964, loss = 1.16612450\n",
      "Iteration 5965, loss = 1.16611630\n",
      "Iteration 5966, loss = 1.16610262\n",
      "Iteration 5967, loss = 1.16609599\n",
      "Iteration 5968, loss = 1.16608871\n",
      "Iteration 5969, loss = 1.16607768\n",
      "Iteration 5970, loss = 1.16606853\n",
      "Iteration 5971, loss = 1.16606019\n",
      "Iteration 5972, loss = 1.16605071\n",
      "Iteration 5973, loss = 1.16604126\n",
      "Iteration 5974, loss = 1.16603424\n",
      "Iteration 5975, loss = 1.16602709\n",
      "Iteration 5976, loss = 1.16601744\n",
      "Iteration 5977, loss = 1.16600716\n",
      "Iteration 5978, loss = 1.16600191\n",
      "Iteration 5979, loss = 1.16599659\n",
      "Iteration 5980, loss = 1.16598061\n",
      "Iteration 5981, loss = 1.16597201\n",
      "Iteration 5982, loss = 1.16596600\n",
      "Iteration 5983, loss = 1.16595414\n",
      "Iteration 5984, loss = 1.16595982\n",
      "Iteration 5985, loss = 1.16593942\n",
      "Iteration 5986, loss = 1.16592961\n",
      "Iteration 5987, loss = 1.16592651\n",
      "Iteration 5988, loss = 1.16591235\n",
      "Iteration 5989, loss = 1.16590450\n",
      "Iteration 5990, loss = 1.16589374\n",
      "Iteration 5991, loss = 1.16588478\n",
      "Iteration 5992, loss = 1.16587661\n",
      "Iteration 5993, loss = 1.16587597\n",
      "Iteration 5994, loss = 1.16585772\n",
      "Iteration 5995, loss = 1.16585024\n",
      "Iteration 5996, loss = 1.16584355\n",
      "Iteration 5997, loss = 1.16583346\n",
      "Iteration 5998, loss = 1.16582584\n",
      "Iteration 5999, loss = 1.16581660\n",
      "Iteration 6000, loss = 1.16581171\n",
      "Iteration 6001, loss = 1.16579896\n",
      "Iteration 6002, loss = 1.16579316\n",
      "Iteration 6003, loss = 1.16577978\n",
      "Iteration 6004, loss = 1.16578013\n",
      "Iteration 6005, loss = 1.16576595\n",
      "Iteration 6006, loss = 1.16575597\n",
      "Iteration 6007, loss = 1.16574974\n",
      "Iteration 6008, loss = 1.16573899\n",
      "Iteration 6009, loss = 1.16573277\n",
      "Iteration 6010, loss = 1.16572433\n",
      "Iteration 6011, loss = 1.16571100\n",
      "Iteration 6012, loss = 1.16570247\n",
      "Iteration 6013, loss = 1.16570163\n",
      "Iteration 6014, loss = 1.16568516\n",
      "Iteration 6015, loss = 1.16567831\n",
      "Iteration 6016, loss = 1.16566588\n",
      "Iteration 6017, loss = 1.16565842\n",
      "Iteration 6018, loss = 1.16565683\n",
      "Iteration 6019, loss = 1.16564077\n",
      "Iteration 6020, loss = 1.16563734\n",
      "Iteration 6021, loss = 1.16562484\n",
      "Iteration 6022, loss = 1.16561503\n",
      "Iteration 6023, loss = 1.16560831\n",
      "Iteration 6024, loss = 1.16560367\n",
      "Iteration 6025, loss = 1.16559574\n",
      "Iteration 6026, loss = 1.16558122\n",
      "Iteration 6027, loss = 1.16557565\n",
      "Iteration 6028, loss = 1.16556439\n",
      "Iteration 6029, loss = 1.16556143\n",
      "Iteration 6030, loss = 1.16554765\n",
      "Iteration 6031, loss = 1.16554113\n",
      "Iteration 6032, loss = 1.16553461\n",
      "Iteration 6033, loss = 1.16552138\n",
      "Iteration 6034, loss = 1.16551327\n",
      "Iteration 6035, loss = 1.16550343\n",
      "Iteration 6036, loss = 1.16550014\n",
      "Iteration 6037, loss = 1.16548607\n",
      "Iteration 6038, loss = 1.16547844\n",
      "Iteration 6039, loss = 1.16547035\n",
      "Iteration 6040, loss = 1.16546351\n",
      "Iteration 6041, loss = 1.16545004\n",
      "Iteration 6042, loss = 1.16544290\n",
      "Iteration 6043, loss = 1.16543491\n",
      "Iteration 6044, loss = 1.16542432\n",
      "Iteration 6045, loss = 1.16541804\n",
      "Iteration 6046, loss = 1.16541094\n",
      "Iteration 6047, loss = 1.16539921\n",
      "Iteration 6048, loss = 1.16539239\n",
      "Iteration 6049, loss = 1.16538149\n",
      "Iteration 6050, loss = 1.16537588\n",
      "Iteration 6051, loss = 1.16536834\n",
      "Iteration 6052, loss = 1.16535759\n",
      "Iteration 6053, loss = 1.16535551\n",
      "Iteration 6054, loss = 1.16533928\n",
      "Iteration 6055, loss = 1.16533117\n",
      "Iteration 6056, loss = 1.16532282\n",
      "Iteration 6057, loss = 1.16531295\n",
      "Iteration 6058, loss = 1.16530594\n",
      "Iteration 6059, loss = 1.16530581\n",
      "Iteration 6060, loss = 1.16529081\n",
      "Iteration 6061, loss = 1.16528306\n",
      "Iteration 6062, loss = 1.16526970\n",
      "Iteration 6063, loss = 1.16526483\n",
      "Iteration 6064, loss = 1.16525628\n",
      "Iteration 6065, loss = 1.16525002\n",
      "Iteration 6066, loss = 1.16523498\n",
      "Iteration 6067, loss = 1.16522642\n",
      "Iteration 6068, loss = 1.16522342\n",
      "Iteration 6069, loss = 1.16521034\n",
      "Iteration 6070, loss = 1.16520263\n",
      "Iteration 6071, loss = 1.16519279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6072, loss = 1.16518424\n",
      "Iteration 6073, loss = 1.16517579\n",
      "Iteration 6074, loss = 1.16517619\n",
      "Iteration 6075, loss = 1.16516175\n",
      "Iteration 6076, loss = 1.16515069\n",
      "Iteration 6077, loss = 1.16514542\n",
      "Iteration 6078, loss = 1.16513361\n",
      "Iteration 6079, loss = 1.16512571\n",
      "Iteration 6080, loss = 1.16511856\n",
      "Iteration 6081, loss = 1.16511008\n",
      "Iteration 6082, loss = 1.16509996\n",
      "Iteration 6083, loss = 1.16510330\n",
      "Iteration 6084, loss = 1.16508401\n",
      "Iteration 6085, loss = 1.16507617\n",
      "Iteration 6086, loss = 1.16506582\n",
      "Iteration 6087, loss = 1.16505654\n",
      "Iteration 6088, loss = 1.16504719\n",
      "Iteration 6089, loss = 1.16503979\n",
      "Iteration 6090, loss = 1.16503216\n",
      "Iteration 6091, loss = 1.16502213\n",
      "Iteration 6092, loss = 1.16502841\n",
      "Iteration 6093, loss = 1.16500512\n",
      "Iteration 6094, loss = 1.16499827\n",
      "Iteration 6095, loss = 1.16499524\n",
      "Iteration 6096, loss = 1.16497937\n",
      "Iteration 6097, loss = 1.16497255\n",
      "Iteration 6098, loss = 1.16496601\n",
      "Iteration 6099, loss = 1.16495411\n",
      "Iteration 6100, loss = 1.16494644\n",
      "Iteration 6101, loss = 1.16494087\n",
      "Iteration 6102, loss = 1.16493045\n",
      "Iteration 6103, loss = 1.16491928\n",
      "Iteration 6104, loss = 1.16491536\n",
      "Iteration 6105, loss = 1.16491165\n",
      "Iteration 6106, loss = 1.16490512\n",
      "Iteration 6107, loss = 1.16488696\n",
      "Iteration 6108, loss = 1.16487745\n",
      "Iteration 6109, loss = 1.16487077\n",
      "Iteration 6110, loss = 1.16486044\n",
      "Iteration 6111, loss = 1.16485173\n",
      "Iteration 6112, loss = 1.16484693\n",
      "Iteration 6113, loss = 1.16483772\n",
      "Iteration 6114, loss = 1.16482800\n",
      "Iteration 6115, loss = 1.16481871\n",
      "Iteration 6116, loss = 1.16480907\n",
      "Iteration 6117, loss = 1.16480267\n",
      "Iteration 6118, loss = 1.16479394\n",
      "Iteration 6119, loss = 1.16478393\n",
      "Iteration 6120, loss = 1.16477543\n",
      "Iteration 6121, loss = 1.16476632\n",
      "Iteration 6122, loss = 1.16475881\n",
      "Iteration 6123, loss = 1.16475004\n",
      "Iteration 6124, loss = 1.16474083\n",
      "Iteration 6125, loss = 1.16473439\n",
      "Iteration 6126, loss = 1.16472373\n",
      "Iteration 6127, loss = 1.16471608\n",
      "Iteration 6128, loss = 1.16470624\n",
      "Iteration 6129, loss = 1.16469883\n",
      "Iteration 6130, loss = 1.16468957\n",
      "Iteration 6131, loss = 1.16468045\n",
      "Iteration 6132, loss = 1.16467511\n",
      "Iteration 6133, loss = 1.16467005\n",
      "Iteration 6134, loss = 1.16466520\n",
      "Iteration 6135, loss = 1.16464771\n",
      "Iteration 6136, loss = 1.16464442\n",
      "Iteration 6137, loss = 1.16463073\n",
      "Iteration 6138, loss = 1.16462296\n",
      "Iteration 6139, loss = 1.16461405\n",
      "Iteration 6140, loss = 1.16460362\n",
      "Iteration 6141, loss = 1.16459519\n",
      "Iteration 6142, loss = 1.16458740\n",
      "Iteration 6143, loss = 1.16458223\n",
      "Iteration 6144, loss = 1.16457144\n",
      "Iteration 6145, loss = 1.16456165\n",
      "Iteration 6146, loss = 1.16455584\n",
      "Iteration 6147, loss = 1.16454839\n",
      "Iteration 6148, loss = 1.16454188\n",
      "Iteration 6149, loss = 1.16452727\n",
      "Iteration 6150, loss = 1.16452473\n",
      "Iteration 6151, loss = 1.16451241\n",
      "Iteration 6152, loss = 1.16450502\n",
      "Iteration 6153, loss = 1.16449739\n",
      "Iteration 6154, loss = 1.16448914\n",
      "Iteration 6155, loss = 1.16448153\n",
      "Iteration 6156, loss = 1.16446841\n",
      "Iteration 6157, loss = 1.16446014\n",
      "Iteration 6158, loss = 1.16445471\n",
      "Iteration 6159, loss = 1.16444368\n",
      "Iteration 6160, loss = 1.16443480\n",
      "Iteration 6161, loss = 1.16442634\n",
      "Iteration 6162, loss = 1.16441879\n",
      "Iteration 6163, loss = 1.16440906\n",
      "Iteration 6164, loss = 1.16440878\n",
      "Iteration 6165, loss = 1.16439307\n",
      "Iteration 6166, loss = 1.16438415\n",
      "Iteration 6167, loss = 1.16437581\n",
      "Iteration 6168, loss = 1.16437052\n",
      "Iteration 6169, loss = 1.16436024\n",
      "Iteration 6170, loss = 1.16435487\n",
      "Iteration 6171, loss = 1.16434371\n",
      "Iteration 6172, loss = 1.16433408\n",
      "Iteration 6173, loss = 1.16432815\n",
      "Iteration 6174, loss = 1.16431846\n",
      "Iteration 6175, loss = 1.16430701\n",
      "Iteration 6176, loss = 1.16430003\n",
      "Iteration 6177, loss = 1.16429231\n",
      "Iteration 6178, loss = 1.16428211\n",
      "Iteration 6179, loss = 1.16427369\n",
      "Iteration 6180, loss = 1.16427227\n",
      "Iteration 6181, loss = 1.16425847\n",
      "Iteration 6182, loss = 1.16425293\n",
      "Iteration 6183, loss = 1.16424490\n",
      "Iteration 6184, loss = 1.16423216\n",
      "Iteration 6185, loss = 1.16422513\n",
      "Iteration 6186, loss = 1.16422016\n",
      "Iteration 6187, loss = 1.16421745\n",
      "Iteration 6188, loss = 1.16419844\n",
      "Iteration 6189, loss = 1.16419443\n",
      "Iteration 6190, loss = 1.16418464\n",
      "Iteration 6191, loss = 1.16417785\n",
      "Iteration 6192, loss = 1.16416948\n",
      "Iteration 6193, loss = 1.16415755\n",
      "Iteration 6194, loss = 1.16415766\n",
      "Iteration 6195, loss = 1.16413964\n",
      "Iteration 6196, loss = 1.16413285\n",
      "Iteration 6197, loss = 1.16412359\n",
      "Iteration 6198, loss = 1.16411436\n",
      "Iteration 6199, loss = 1.16410794\n",
      "Iteration 6200, loss = 1.16410500\n",
      "Iteration 6201, loss = 1.16409094\n",
      "Iteration 6202, loss = 1.16408500\n",
      "Iteration 6203, loss = 1.16407309\n",
      "Iteration 6204, loss = 1.16406369\n",
      "Iteration 6205, loss = 1.16405869\n",
      "Iteration 6206, loss = 1.16404854\n",
      "Iteration 6207, loss = 1.16403858\n",
      "Iteration 6208, loss = 1.16403090\n",
      "Iteration 6209, loss = 1.16403703\n",
      "Iteration 6210, loss = 1.16401388\n",
      "Iteration 6211, loss = 1.16400491\n",
      "Iteration 6212, loss = 1.16400077\n",
      "Iteration 6213, loss = 1.16398948\n",
      "Iteration 6214, loss = 1.16397954\n",
      "Iteration 6215, loss = 1.16397193\n",
      "Iteration 6216, loss = 1.16396275\n",
      "Iteration 6217, loss = 1.16396478\n",
      "Iteration 6218, loss = 1.16394946\n",
      "Iteration 6219, loss = 1.16394006\n",
      "Iteration 6220, loss = 1.16393232\n",
      "Iteration 6221, loss = 1.16392515\n",
      "Iteration 6222, loss = 1.16391885\n",
      "Iteration 6223, loss = 1.16390775\n",
      "Iteration 6224, loss = 1.16389842\n",
      "Iteration 6225, loss = 1.16389253\n",
      "Iteration 6226, loss = 1.16388200\n",
      "Iteration 6227, loss = 1.16387330\n",
      "Iteration 6228, loss = 1.16386540\n",
      "Iteration 6229, loss = 1.16385769\n",
      "Iteration 6230, loss = 1.16384691\n",
      "Iteration 6231, loss = 1.16384262\n",
      "Iteration 6232, loss = 1.16383128\n",
      "Iteration 6233, loss = 1.16382515\n",
      "Iteration 6234, loss = 1.16381524\n",
      "Iteration 6235, loss = 1.16380693\n",
      "Iteration 6236, loss = 1.16380036\n",
      "Iteration 6237, loss = 1.16379435\n",
      "Iteration 6238, loss = 1.16378268\n",
      "Iteration 6239, loss = 1.16377950\n",
      "Iteration 6240, loss = 1.16376522\n",
      "Iteration 6241, loss = 1.16376266\n",
      "Iteration 6242, loss = 1.16374829\n",
      "Iteration 6243, loss = 1.16374279\n",
      "Iteration 6244, loss = 1.16373233\n",
      "Iteration 6245, loss = 1.16372479\n",
      "Iteration 6246, loss = 1.16371547\n",
      "Iteration 6247, loss = 1.16371163\n",
      "Iteration 6248, loss = 1.16369830\n",
      "Iteration 6249, loss = 1.16368962\n",
      "Iteration 6250, loss = 1.16368317\n",
      "Iteration 6251, loss = 1.16367834\n",
      "Iteration 6252, loss = 1.16366746\n",
      "Iteration 6253, loss = 1.16366687\n",
      "Iteration 6254, loss = 1.16364966\n",
      "Iteration 6255, loss = 1.16364183\n",
      "Iteration 6256, loss = 1.16363362\n",
      "Iteration 6257, loss = 1.16362464\n",
      "Iteration 6258, loss = 1.16361444\n",
      "Iteration 6259, loss = 1.16360821\n",
      "Iteration 6260, loss = 1.16360058\n",
      "Iteration 6261, loss = 1.16359785\n",
      "Iteration 6262, loss = 1.16358172\n",
      "Iteration 6263, loss = 1.16357784\n",
      "Iteration 6264, loss = 1.16356833\n",
      "Iteration 6265, loss = 1.16356575\n",
      "Iteration 6266, loss = 1.16355830\n",
      "Iteration 6267, loss = 1.16355149\n",
      "Iteration 6268, loss = 1.16353527\n",
      "Iteration 6269, loss = 1.16352687\n",
      "Iteration 6270, loss = 1.16352183\n",
      "Iteration 6271, loss = 1.16351331\n",
      "Iteration 6272, loss = 1.16350391\n",
      "Iteration 6273, loss = 1.16349296\n",
      "Iteration 6274, loss = 1.16349047\n",
      "Iteration 6275, loss = 1.16347942\n",
      "Iteration 6276, loss = 1.16346995\n",
      "Iteration 6277, loss = 1.16346038\n",
      "Iteration 6278, loss = 1.16345490\n",
      "Iteration 6279, loss = 1.16345040\n",
      "Iteration 6280, loss = 1.16343489\n",
      "Iteration 6281, loss = 1.16342708\n",
      "Iteration 6282, loss = 1.16342001\n",
      "Iteration 6283, loss = 1.16341942\n",
      "Iteration 6284, loss = 1.16340523\n",
      "Iteration 6285, loss = 1.16339934\n",
      "Iteration 6286, loss = 1.16338644\n",
      "Iteration 6287, loss = 1.16337691\n",
      "Iteration 6288, loss = 1.16337211\n",
      "Iteration 6289, loss = 1.16336088\n",
      "Iteration 6290, loss = 1.16335487\n",
      "Iteration 6291, loss = 1.16334439\n",
      "Iteration 6292, loss = 1.16333659\n",
      "Iteration 6293, loss = 1.16332785\n",
      "Iteration 6294, loss = 1.16331817\n",
      "Iteration 6295, loss = 1.16330908\n",
      "Iteration 6296, loss = 1.16330280\n",
      "Iteration 6297, loss = 1.16330126\n",
      "Iteration 6298, loss = 1.16328585\n",
      "Iteration 6299, loss = 1.16327817\n",
      "Iteration 6300, loss = 1.16327175\n",
      "Iteration 6301, loss = 1.16326251\n",
      "Iteration 6302, loss = 1.16325470\n",
      "Iteration 6303, loss = 1.16325392\n",
      "Iteration 6304, loss = 1.16324144\n",
      "Iteration 6305, loss = 1.16322884\n",
      "Iteration 6306, loss = 1.16322620\n",
      "Iteration 6307, loss = 1.16321503\n",
      "Iteration 6308, loss = 1.16320324\n",
      "Iteration 6309, loss = 1.16319657\n",
      "Iteration 6310, loss = 1.16318756\n",
      "Iteration 6311, loss = 1.16317993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6312, loss = 1.16317375\n",
      "Iteration 6313, loss = 1.16316312\n",
      "Iteration 6314, loss = 1.16315792\n",
      "Iteration 6315, loss = 1.16315321\n",
      "Iteration 6316, loss = 1.16314462\n",
      "Iteration 6317, loss = 1.16313024\n",
      "Iteration 6318, loss = 1.16312383\n",
      "Iteration 6319, loss = 1.16311464\n",
      "Iteration 6320, loss = 1.16310796\n",
      "Iteration 6321, loss = 1.16309799\n",
      "Iteration 6322, loss = 1.16309441\n",
      "Iteration 6323, loss = 1.16308052\n",
      "Iteration 6324, loss = 1.16307292\n",
      "Iteration 6325, loss = 1.16306460\n",
      "Iteration 6326, loss = 1.16305789\n",
      "Iteration 6327, loss = 1.16305019\n",
      "Iteration 6328, loss = 1.16304156\n",
      "Iteration 6329, loss = 1.16303117\n",
      "Iteration 6330, loss = 1.16302681\n",
      "Iteration 6331, loss = 1.16301950\n",
      "Iteration 6332, loss = 1.16301471\n",
      "Iteration 6333, loss = 1.16300167\n",
      "Iteration 6334, loss = 1.16299150\n",
      "Iteration 6335, loss = 1.16298724\n",
      "Iteration 6336, loss = 1.16297425\n",
      "Iteration 6337, loss = 1.16296714\n",
      "Iteration 6338, loss = 1.16295831\n",
      "Iteration 6339, loss = 1.16295162\n",
      "Iteration 6340, loss = 1.16295058\n",
      "Iteration 6341, loss = 1.16293398\n",
      "Iteration 6342, loss = 1.16292768\n",
      "Iteration 6343, loss = 1.16291769\n",
      "Iteration 6344, loss = 1.16291229\n",
      "Iteration 6345, loss = 1.16290272\n",
      "Iteration 6346, loss = 1.16289283\n",
      "Iteration 6347, loss = 1.16288641\n",
      "Iteration 6348, loss = 1.16287803\n",
      "Iteration 6349, loss = 1.16287333\n",
      "Iteration 6350, loss = 1.16286011\n",
      "Iteration 6351, loss = 1.16285189\n",
      "Iteration 6352, loss = 1.16284382\n",
      "Iteration 6353, loss = 1.16283944\n",
      "Iteration 6354, loss = 1.16283187\n",
      "Iteration 6355, loss = 1.16282456\n",
      "Iteration 6356, loss = 1.16281626\n",
      "Iteration 6357, loss = 1.16280359\n",
      "Iteration 6358, loss = 1.16279787\n",
      "Iteration 6359, loss = 1.16278939\n",
      "Iteration 6360, loss = 1.16277981\n",
      "Iteration 6361, loss = 1.16278497\n",
      "Iteration 6362, loss = 1.16276479\n",
      "Iteration 6363, loss = 1.16275605\n",
      "Iteration 6364, loss = 1.16274814\n",
      "Iteration 6365, loss = 1.16274337\n",
      "Iteration 6366, loss = 1.16273189\n",
      "Iteration 6367, loss = 1.16272976\n",
      "Iteration 6368, loss = 1.16271356\n",
      "Iteration 6369, loss = 1.16270591\n",
      "Iteration 6370, loss = 1.16270371\n",
      "Iteration 6371, loss = 1.16269167\n",
      "Iteration 6372, loss = 1.16268266\n",
      "Iteration 6373, loss = 1.16267670\n",
      "Iteration 6374, loss = 1.16266629\n",
      "Iteration 6375, loss = 1.16265744\n",
      "Iteration 6376, loss = 1.16265146\n",
      "Iteration 6377, loss = 1.16264153\n",
      "Iteration 6378, loss = 1.16263380\n",
      "Iteration 6379, loss = 1.16262663\n",
      "Iteration 6380, loss = 1.16262159\n",
      "Iteration 6381, loss = 1.16261030\n",
      "Iteration 6382, loss = 1.16260783\n",
      "Iteration 6383, loss = 1.16259254\n",
      "Iteration 6384, loss = 1.16259078\n",
      "Iteration 6385, loss = 1.16257604\n",
      "Iteration 6386, loss = 1.16257041\n",
      "Iteration 6387, loss = 1.16256210\n",
      "Iteration 6388, loss = 1.16255462\n",
      "Iteration 6389, loss = 1.16254407\n",
      "Iteration 6390, loss = 1.16254563\n",
      "Iteration 6391, loss = 1.16252666\n",
      "Iteration 6392, loss = 1.16252484\n",
      "Iteration 6393, loss = 1.16251851\n",
      "Iteration 6394, loss = 1.16250687\n",
      "Iteration 6395, loss = 1.16249621\n",
      "Iteration 6396, loss = 1.16248702\n",
      "Iteration 6397, loss = 1.16248082\n",
      "Iteration 6398, loss = 1.16247073\n",
      "Iteration 6399, loss = 1.16246285\n",
      "Iteration 6400, loss = 1.16246277\n",
      "Iteration 6401, loss = 1.16244964\n",
      "Iteration 6402, loss = 1.16244225\n",
      "Iteration 6403, loss = 1.16243580\n",
      "Iteration 6404, loss = 1.16242581\n",
      "Iteration 6405, loss = 1.16241614\n",
      "Iteration 6406, loss = 1.16241036\n",
      "Iteration 6407, loss = 1.16239839\n",
      "Iteration 6408, loss = 1.16239152\n",
      "Iteration 6409, loss = 1.16238460\n",
      "Iteration 6410, loss = 1.16237498\n",
      "Iteration 6411, loss = 1.16236773\n",
      "Iteration 6412, loss = 1.16236125\n",
      "Iteration 6413, loss = 1.16235046\n",
      "Iteration 6414, loss = 1.16234328\n",
      "Iteration 6415, loss = 1.16233675\n",
      "Iteration 6416, loss = 1.16232760\n",
      "Iteration 6417, loss = 1.16231760\n",
      "Iteration 6418, loss = 1.16231189\n",
      "Iteration 6419, loss = 1.16230248\n",
      "Iteration 6420, loss = 1.16229468\n",
      "Iteration 6421, loss = 1.16228644\n",
      "Iteration 6422, loss = 1.16228287\n",
      "Iteration 6423, loss = 1.16227491\n",
      "Iteration 6424, loss = 1.16226169\n",
      "Iteration 6425, loss = 1.16225373\n",
      "Iteration 6426, loss = 1.16225145\n",
      "Iteration 6427, loss = 1.16223862\n",
      "Iteration 6428, loss = 1.16223155\n",
      "Iteration 6429, loss = 1.16222754\n",
      "Iteration 6430, loss = 1.16221541\n",
      "Iteration 6431, loss = 1.16220637\n",
      "Iteration 6432, loss = 1.16219791\n",
      "Iteration 6433, loss = 1.16219160\n",
      "Iteration 6434, loss = 1.16218303\n",
      "Iteration 6435, loss = 1.16217755\n",
      "Iteration 6436, loss = 1.16217183\n",
      "Iteration 6437, loss = 1.16216099\n",
      "Iteration 6438, loss = 1.16215007\n",
      "Iteration 6439, loss = 1.16214403\n",
      "Iteration 6440, loss = 1.16214230\n",
      "Iteration 6441, loss = 1.16213170\n",
      "Iteration 6442, loss = 1.16212156\n",
      "Iteration 6443, loss = 1.16211686\n",
      "Iteration 6444, loss = 1.16210965\n",
      "Iteration 6445, loss = 1.16209544\n",
      "Iteration 6446, loss = 1.16209076\n",
      "Iteration 6447, loss = 1.16208311\n",
      "Iteration 6448, loss = 1.16207216\n",
      "Iteration 6449, loss = 1.16206196\n",
      "Iteration 6450, loss = 1.16205621\n",
      "Iteration 6451, loss = 1.16204648\n",
      "Iteration 6452, loss = 1.16204411\n",
      "Iteration 6453, loss = 1.16204164\n",
      "Iteration 6454, loss = 1.16202190\n",
      "Iteration 6455, loss = 1.16201354\n",
      "Iteration 6456, loss = 1.16200955\n",
      "Iteration 6457, loss = 1.16199881\n",
      "Iteration 6458, loss = 1.16199550\n",
      "Iteration 6459, loss = 1.16198391\n",
      "Iteration 6460, loss = 1.16198384\n",
      "Iteration 6461, loss = 1.16197270\n",
      "Iteration 6462, loss = 1.16195892\n",
      "Iteration 6463, loss = 1.16196095\n",
      "Iteration 6464, loss = 1.16194336\n",
      "Iteration 6465, loss = 1.16193671\n",
      "Iteration 6466, loss = 1.16193163\n",
      "Iteration 6467, loss = 1.16192416\n",
      "Iteration 6468, loss = 1.16191443\n",
      "Iteration 6469, loss = 1.16190242\n",
      "Iteration 6470, loss = 1.16190207\n",
      "Iteration 6471, loss = 1.16189074\n",
      "Iteration 6472, loss = 1.16188198\n",
      "Iteration 6473, loss = 1.16187839\n",
      "Iteration 6474, loss = 1.16186415\n",
      "Iteration 6475, loss = 1.16185596\n",
      "Iteration 6476, loss = 1.16185613\n",
      "Iteration 6477, loss = 1.16183716\n",
      "Iteration 6478, loss = 1.16183146\n",
      "Iteration 6479, loss = 1.16182426\n",
      "Iteration 6480, loss = 1.16181702\n",
      "Iteration 6481, loss = 1.16180871\n",
      "Iteration 6482, loss = 1.16180349\n",
      "Iteration 6483, loss = 1.16179183\n",
      "Iteration 6484, loss = 1.16178513\n",
      "Iteration 6485, loss = 1.16177476\n",
      "Iteration 6486, loss = 1.16177186\n",
      "Iteration 6487, loss = 1.16175993\n",
      "Iteration 6488, loss = 1.16175427\n",
      "Iteration 6489, loss = 1.16174570\n",
      "Iteration 6490, loss = 1.16173976\n",
      "Iteration 6491, loss = 1.16173083\n",
      "Iteration 6492, loss = 1.16172228\n",
      "Iteration 6493, loss = 1.16171629\n",
      "Iteration 6494, loss = 1.16170576\n",
      "Iteration 6495, loss = 1.16169691\n",
      "Iteration 6496, loss = 1.16169500\n",
      "Iteration 6497, loss = 1.16168223\n",
      "Iteration 6498, loss = 1.16167313\n",
      "Iteration 6499, loss = 1.16167578\n",
      "Iteration 6500, loss = 1.16166091\n",
      "Iteration 6501, loss = 1.16165017\n",
      "Iteration 6502, loss = 1.16164454\n",
      "Iteration 6503, loss = 1.16163585\n",
      "Iteration 6504, loss = 1.16162669\n",
      "Iteration 6505, loss = 1.16161819\n",
      "Iteration 6506, loss = 1.16161196\n",
      "Iteration 6507, loss = 1.16160315\n",
      "Iteration 6508, loss = 1.16159514\n",
      "Iteration 6509, loss = 1.16158903\n",
      "Iteration 6510, loss = 1.16158259\n",
      "Iteration 6511, loss = 1.16157343\n",
      "Iteration 6512, loss = 1.16156525\n",
      "Iteration 6513, loss = 1.16155594\n",
      "Iteration 6514, loss = 1.16155197\n",
      "Iteration 6515, loss = 1.16154614\n",
      "Iteration 6516, loss = 1.16153481\n",
      "Iteration 6517, loss = 1.16152588\n",
      "Iteration 6518, loss = 1.16153295\n",
      "Iteration 6519, loss = 1.16150856\n",
      "Iteration 6520, loss = 1.16150092\n",
      "Iteration 6521, loss = 1.16149149\n",
      "Iteration 6522, loss = 1.16148640\n",
      "Iteration 6523, loss = 1.16147770\n",
      "Iteration 6524, loss = 1.16147003\n",
      "Iteration 6525, loss = 1.16146068\n",
      "Iteration 6526, loss = 1.16145442\n",
      "Iteration 6527, loss = 1.16144505\n",
      "Iteration 6528, loss = 1.16144049\n",
      "Iteration 6529, loss = 1.16142780\n",
      "Iteration 6530, loss = 1.16142160\n",
      "Iteration 6531, loss = 1.16141577\n",
      "Iteration 6532, loss = 1.16140910\n",
      "Iteration 6533, loss = 1.16140683\n",
      "Iteration 6534, loss = 1.16139065\n",
      "Iteration 6535, loss = 1.16138759\n",
      "Iteration 6536, loss = 1.16137365\n",
      "Iteration 6537, loss = 1.16137036\n",
      "Iteration 6538, loss = 1.16136521\n",
      "Iteration 6539, loss = 1.16135326\n",
      "Iteration 6540, loss = 1.16135254\n",
      "Iteration 6541, loss = 1.16133821\n",
      "Iteration 6542, loss = 1.16132994\n",
      "Iteration 6543, loss = 1.16133006\n",
      "Iteration 6544, loss = 1.16131086\n",
      "Iteration 6545, loss = 1.16130625\n",
      "Iteration 6546, loss = 1.16129893\n",
      "Iteration 6547, loss = 1.16128791\n",
      "Iteration 6548, loss = 1.16128415\n",
      "Iteration 6549, loss = 1.16127422\n",
      "Iteration 6550, loss = 1.16126551\n",
      "Iteration 6551, loss = 1.16125775\n",
      "Iteration 6552, loss = 1.16125730\n",
      "Iteration 6553, loss = 1.16124549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6554, loss = 1.16123906\n",
      "Iteration 6555, loss = 1.16122835\n",
      "Iteration 6556, loss = 1.16121812\n",
      "Iteration 6557, loss = 1.16120810\n",
      "Iteration 6558, loss = 1.16120284\n",
      "Iteration 6559, loss = 1.16120078\n",
      "Iteration 6560, loss = 1.16119165\n",
      "Iteration 6561, loss = 1.16117946\n",
      "Iteration 6562, loss = 1.16117646\n",
      "Iteration 6563, loss = 1.16116726\n",
      "Iteration 6564, loss = 1.16115635\n",
      "Iteration 6565, loss = 1.16114655\n",
      "Iteration 6566, loss = 1.16114221\n",
      "Iteration 6567, loss = 1.16113477\n",
      "Iteration 6568, loss = 1.16112251\n",
      "Iteration 6569, loss = 1.16111352\n",
      "Iteration 6570, loss = 1.16110763\n",
      "Iteration 6571, loss = 1.16110332\n",
      "Iteration 6572, loss = 1.16109537\n",
      "Iteration 6573, loss = 1.16108716\n",
      "Iteration 6574, loss = 1.16108478\n",
      "Iteration 6575, loss = 1.16107343\n",
      "Iteration 6576, loss = 1.16106209\n",
      "Iteration 6577, loss = 1.16105394\n",
      "Iteration 6578, loss = 1.16104617\n",
      "Iteration 6579, loss = 1.16104242\n",
      "Iteration 6580, loss = 1.16102847\n",
      "Iteration 6581, loss = 1.16102258\n",
      "Iteration 6582, loss = 1.16101673\n",
      "Iteration 6583, loss = 1.16100869\n",
      "Iteration 6584, loss = 1.16099779\n",
      "Iteration 6585, loss = 1.16099107\n",
      "Iteration 6586, loss = 1.16098453\n",
      "Iteration 6587, loss = 1.16097733\n",
      "Iteration 6588, loss = 1.16096535\n",
      "Iteration 6589, loss = 1.16096009\n",
      "Iteration 6590, loss = 1.16095103\n",
      "Iteration 6591, loss = 1.16094972\n",
      "Iteration 6592, loss = 1.16093466\n",
      "Iteration 6593, loss = 1.16092685\n",
      "Iteration 6594, loss = 1.16092037\n",
      "Iteration 6595, loss = 1.16091307\n",
      "Iteration 6596, loss = 1.16090391\n",
      "Iteration 6597, loss = 1.16089894\n",
      "Iteration 6598, loss = 1.16089176\n",
      "Iteration 6599, loss = 1.16088688\n",
      "Iteration 6600, loss = 1.16087834\n",
      "Iteration 6601, loss = 1.16086578\n",
      "Iteration 6602, loss = 1.16086131\n",
      "Iteration 6603, loss = 1.16085179\n",
      "Iteration 6604, loss = 1.16085138\n",
      "Iteration 6605, loss = 1.16083644\n",
      "Iteration 6606, loss = 1.16082667\n",
      "Iteration 6607, loss = 1.16083018\n",
      "Iteration 6608, loss = 1.16081270\n",
      "Iteration 6609, loss = 1.16080463\n",
      "Iteration 6610, loss = 1.16079912\n",
      "Iteration 6611, loss = 1.16078989\n",
      "Iteration 6612, loss = 1.16078211\n",
      "Iteration 6613, loss = 1.16077709\n",
      "Iteration 6614, loss = 1.16077286\n",
      "Iteration 6615, loss = 1.16075927\n",
      "Iteration 6616, loss = 1.16074816\n",
      "Iteration 6617, loss = 1.16074219\n",
      "Iteration 6618, loss = 1.16073471\n",
      "Iteration 6619, loss = 1.16072977\n",
      "Iteration 6620, loss = 1.16071869\n",
      "Iteration 6621, loss = 1.16071600\n",
      "Iteration 6622, loss = 1.16070282\n",
      "Iteration 6623, loss = 1.16070334\n",
      "Iteration 6624, loss = 1.16069249\n",
      "Iteration 6625, loss = 1.16068293\n",
      "Iteration 6626, loss = 1.16067530\n",
      "Iteration 6627, loss = 1.16066990\n",
      "Iteration 6628, loss = 1.16066086\n",
      "Iteration 6629, loss = 1.16065185\n",
      "Iteration 6630, loss = 1.16064351\n",
      "Iteration 6631, loss = 1.16063308\n",
      "Iteration 6632, loss = 1.16062416\n",
      "Iteration 6633, loss = 1.16061847\n",
      "Iteration 6634, loss = 1.16061154\n",
      "Iteration 6635, loss = 1.16060197\n",
      "Iteration 6636, loss = 1.16059668\n",
      "Iteration 6637, loss = 1.16059265\n",
      "Iteration 6638, loss = 1.16059176\n",
      "Iteration 6639, loss = 1.16057564\n",
      "Iteration 6640, loss = 1.16056289\n",
      "Iteration 6641, loss = 1.16056557\n",
      "Iteration 6642, loss = 1.16055367\n",
      "Iteration 6643, loss = 1.16054423\n",
      "Iteration 6644, loss = 1.16053373\n",
      "Iteration 6645, loss = 1.16052578\n",
      "Iteration 6646, loss = 1.16051809\n",
      "Iteration 6647, loss = 1.16050853\n",
      "Iteration 6648, loss = 1.16050867\n",
      "Iteration 6649, loss = 1.16049408\n",
      "Iteration 6650, loss = 1.16048559\n",
      "Iteration 6651, loss = 1.16047885\n",
      "Iteration 6652, loss = 1.16047261\n",
      "Iteration 6653, loss = 1.16046740\n",
      "Iteration 6654, loss = 1.16045812\n",
      "Iteration 6655, loss = 1.16044612\n",
      "Iteration 6656, loss = 1.16043827\n",
      "Iteration 6657, loss = 1.16043862\n",
      "Iteration 6658, loss = 1.16043136\n",
      "Iteration 6659, loss = 1.16042146\n",
      "Iteration 6660, loss = 1.16041170\n",
      "Iteration 6661, loss = 1.16040528\n",
      "Iteration 6662, loss = 1.16039543\n",
      "Iteration 6663, loss = 1.16039197\n",
      "Iteration 6664, loss = 1.16038459\n",
      "Iteration 6665, loss = 1.16036932\n",
      "Iteration 6666, loss = 1.16036157\n",
      "Iteration 6667, loss = 1.16035994\n",
      "Iteration 6668, loss = 1.16034923\n",
      "Iteration 6669, loss = 1.16034338\n",
      "Iteration 6670, loss = 1.16033005\n",
      "Iteration 6671, loss = 1.16032441\n",
      "Iteration 6672, loss = 1.16031690\n",
      "Iteration 6673, loss = 1.16031408\n",
      "Iteration 6674, loss = 1.16030423\n",
      "Iteration 6675, loss = 1.16030026\n",
      "Iteration 6676, loss = 1.16028726\n",
      "Iteration 6677, loss = 1.16027823\n",
      "Iteration 6678, loss = 1.16028140\n",
      "Iteration 6679, loss = 1.16027120\n",
      "Iteration 6680, loss = 1.16026596\n",
      "Iteration 6681, loss = 1.16024867\n",
      "Iteration 6682, loss = 1.16024301\n",
      "Iteration 6683, loss = 1.16023480\n",
      "Iteration 6684, loss = 1.16023777\n",
      "Iteration 6685, loss = 1.16022085\n",
      "Iteration 6686, loss = 1.16021380\n",
      "Iteration 6687, loss = 1.16020385\n",
      "Iteration 6688, loss = 1.16020146\n",
      "Iteration 6689, loss = 1.16018628\n",
      "Iteration 6690, loss = 1.16018230\n",
      "Iteration 6691, loss = 1.16017033\n",
      "Iteration 6692, loss = 1.16016422\n",
      "Iteration 6693, loss = 1.16015592\n",
      "Iteration 6694, loss = 1.16014990\n",
      "Iteration 6695, loss = 1.16014042\n",
      "Iteration 6696, loss = 1.16013347\n",
      "Iteration 6697, loss = 1.16012758\n",
      "Iteration 6698, loss = 1.16011955\n",
      "Iteration 6699, loss = 1.16011116\n",
      "Iteration 6700, loss = 1.16010764\n",
      "Iteration 6701, loss = 1.16009660\n",
      "Iteration 6702, loss = 1.16009261\n",
      "Iteration 6703, loss = 1.16008335\n",
      "Iteration 6704, loss = 1.16007591\n",
      "Iteration 6705, loss = 1.16006454\n",
      "Iteration 6706, loss = 1.16005740\n",
      "Iteration 6707, loss = 1.16004834\n",
      "Iteration 6708, loss = 1.16004111\n",
      "Iteration 6709, loss = 1.16003369\n",
      "Iteration 6710, loss = 1.16002555\n",
      "Iteration 6711, loss = 1.16001896\n",
      "Iteration 6712, loss = 1.16001946\n",
      "Iteration 6713, loss = 1.16000648\n",
      "Iteration 6714, loss = 1.15999681\n",
      "Iteration 6715, loss = 1.15998997\n",
      "Iteration 6716, loss = 1.15998323\n",
      "Iteration 6717, loss = 1.15997271\n",
      "Iteration 6718, loss = 1.15996540\n",
      "Iteration 6719, loss = 1.15995712\n",
      "Iteration 6720, loss = 1.15995009\n",
      "Iteration 6721, loss = 1.15994570\n",
      "Iteration 6722, loss = 1.15993763\n",
      "Iteration 6723, loss = 1.15992791\n",
      "Iteration 6724, loss = 1.15992039\n",
      "Iteration 6725, loss = 1.15991556\n",
      "Iteration 6726, loss = 1.15990972\n",
      "Iteration 6727, loss = 1.15989694\n",
      "Iteration 6728, loss = 1.15989303\n",
      "Iteration 6729, loss = 1.15988686\n",
      "Iteration 6730, loss = 1.15987806\n",
      "Iteration 6731, loss = 1.15987032\n",
      "Iteration 6732, loss = 1.15985955\n",
      "Iteration 6733, loss = 1.15985380\n",
      "Iteration 6734, loss = 1.15984635\n",
      "Iteration 6735, loss = 1.15983543\n",
      "Iteration 6736, loss = 1.15983324\n",
      "Iteration 6737, loss = 1.15983477\n",
      "Iteration 6738, loss = 1.15981710\n",
      "Iteration 6739, loss = 1.15980439\n",
      "Iteration 6740, loss = 1.15979866\n",
      "Iteration 6741, loss = 1.15979738\n",
      "Iteration 6742, loss = 1.15978352\n",
      "Iteration 6743, loss = 1.15977441\n",
      "Iteration 6744, loss = 1.15977021\n",
      "Iteration 6745, loss = 1.15976475\n",
      "Iteration 6746, loss = 1.15975405\n",
      "Iteration 6747, loss = 1.15974458\n",
      "Iteration 6748, loss = 1.15974211\n",
      "Iteration 6749, loss = 1.15973214\n",
      "Iteration 6750, loss = 1.15972526\n",
      "Iteration 6751, loss = 1.15972137\n",
      "Iteration 6752, loss = 1.15971516\n",
      "Iteration 6753, loss = 1.15970400\n",
      "Iteration 6754, loss = 1.15969259\n",
      "Iteration 6755, loss = 1.15968519\n",
      "Iteration 6756, loss = 1.15967788\n",
      "Iteration 6757, loss = 1.15966787\n",
      "Iteration 6758, loss = 1.15966125\n",
      "Iteration 6759, loss = 1.15965474\n",
      "Iteration 6760, loss = 1.15964564\n",
      "Iteration 6761, loss = 1.15963886\n",
      "Iteration 6762, loss = 1.15963554\n",
      "Iteration 6763, loss = 1.15962212\n",
      "Iteration 6764, loss = 1.15961496\n",
      "Iteration 6765, loss = 1.15960966\n",
      "Iteration 6766, loss = 1.15960582\n",
      "Iteration 6767, loss = 1.15959650\n",
      "Iteration 6768, loss = 1.15958500\n",
      "Iteration 6769, loss = 1.15957671\n",
      "Iteration 6770, loss = 1.15957465\n",
      "Iteration 6771, loss = 1.15956718\n",
      "Iteration 6772, loss = 1.15955491\n",
      "Iteration 6773, loss = 1.15956020\n",
      "Iteration 6774, loss = 1.15954254\n",
      "Iteration 6775, loss = 1.15953426\n",
      "Iteration 6776, loss = 1.15952313\n",
      "Iteration 6777, loss = 1.15952760\n",
      "Iteration 6778, loss = 1.15950960\n",
      "Iteration 6779, loss = 1.15950613\n",
      "Iteration 6780, loss = 1.15949904\n",
      "Iteration 6781, loss = 1.15948788\n",
      "Iteration 6782, loss = 1.15948194\n",
      "Iteration 6783, loss = 1.15947586\n",
      "Iteration 6784, loss = 1.15946649\n",
      "Iteration 6785, loss = 1.15945704\n",
      "Iteration 6786, loss = 1.15945057\n",
      "Iteration 6787, loss = 1.15944207\n",
      "Iteration 6788, loss = 1.15943363\n",
      "Iteration 6789, loss = 1.15942583\n",
      "Iteration 6790, loss = 1.15942110\n",
      "Iteration 6791, loss = 1.15942089\n",
      "Iteration 6792, loss = 1.15940183\n",
      "Iteration 6793, loss = 1.15939957\n",
      "Iteration 6794, loss = 1.15939801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6795, loss = 1.15938614\n",
      "Iteration 6796, loss = 1.15938031\n",
      "Iteration 6797, loss = 1.15937000\n",
      "Iteration 6798, loss = 1.15936002\n",
      "Iteration 6799, loss = 1.15935223\n",
      "Iteration 6800, loss = 1.15934348\n",
      "Iteration 6801, loss = 1.15933725\n",
      "Iteration 6802, loss = 1.15932946\n",
      "Iteration 6803, loss = 1.15932107\n",
      "Iteration 6804, loss = 1.15931308\n",
      "Iteration 6805, loss = 1.15930556\n",
      "Iteration 6806, loss = 1.15930047\n",
      "Iteration 6807, loss = 1.15929161\n",
      "Iteration 6808, loss = 1.15928609\n",
      "Iteration 6809, loss = 1.15927915\n",
      "Iteration 6810, loss = 1.15926963\n",
      "Iteration 6811, loss = 1.15926114\n",
      "Iteration 6812, loss = 1.15925448\n",
      "Iteration 6813, loss = 1.15924758\n",
      "Iteration 6814, loss = 1.15923809\n",
      "Iteration 6815, loss = 1.15923553\n",
      "Iteration 6816, loss = 1.15922246\n",
      "Iteration 6817, loss = 1.15921657\n",
      "Iteration 6818, loss = 1.15922052\n",
      "Iteration 6819, loss = 1.15920971\n",
      "Iteration 6820, loss = 1.15919765\n",
      "Iteration 6821, loss = 1.15918623\n",
      "Iteration 6822, loss = 1.15918118\n",
      "Iteration 6823, loss = 1.15917061\n",
      "Iteration 6824, loss = 1.15916558\n",
      "Iteration 6825, loss = 1.15915868\n",
      "Iteration 6826, loss = 1.15915048\n",
      "Iteration 6827, loss = 1.15914425\n",
      "Iteration 6828, loss = 1.15913749\n",
      "Iteration 6829, loss = 1.15913144\n",
      "Iteration 6830, loss = 1.15912133\n",
      "Iteration 6831, loss = 1.15911252\n",
      "Iteration 6832, loss = 1.15910378\n",
      "Iteration 6833, loss = 1.15910087\n",
      "Iteration 6834, loss = 1.15908848\n",
      "Iteration 6835, loss = 1.15908341\n",
      "Iteration 6836, loss = 1.15907643\n",
      "Iteration 6837, loss = 1.15907068\n",
      "Iteration 6838, loss = 1.15905861\n",
      "Iteration 6839, loss = 1.15905056\n",
      "Iteration 6840, loss = 1.15904293\n",
      "Iteration 6841, loss = 1.15903792\n",
      "Iteration 6842, loss = 1.15902655\n",
      "Iteration 6843, loss = 1.15902092\n",
      "Iteration 6844, loss = 1.15901531\n",
      "Iteration 6845, loss = 1.15900599\n",
      "Iteration 6846, loss = 1.15899923\n",
      "Iteration 6847, loss = 1.15899062\n",
      "Iteration 6848, loss = 1.15898684\n",
      "Iteration 6849, loss = 1.15898036\n",
      "Iteration 6850, loss = 1.15897162\n",
      "Iteration 6851, loss = 1.15896224\n",
      "Iteration 6852, loss = 1.15895998\n",
      "Iteration 6853, loss = 1.15895489\n",
      "Iteration 6854, loss = 1.15893940\n",
      "Iteration 6855, loss = 1.15893804\n",
      "Iteration 6856, loss = 1.15892587\n",
      "Iteration 6857, loss = 1.15892157\n",
      "Iteration 6858, loss = 1.15890999\n",
      "Iteration 6859, loss = 1.15890413\n",
      "Iteration 6860, loss = 1.15889711\n",
      "Iteration 6861, loss = 1.15889326\n",
      "Iteration 6862, loss = 1.15887859\n",
      "Iteration 6863, loss = 1.15887331\n",
      "Iteration 6864, loss = 1.15887064\n",
      "Iteration 6865, loss = 1.15886111\n",
      "Iteration 6866, loss = 1.15886267\n",
      "Iteration 6867, loss = 1.15884812\n",
      "Iteration 6868, loss = 1.15883412\n",
      "Iteration 6869, loss = 1.15883054\n",
      "Iteration 6870, loss = 1.15882199\n",
      "Iteration 6871, loss = 1.15881574\n",
      "Iteration 6872, loss = 1.15880442\n",
      "Iteration 6873, loss = 1.15879679\n",
      "Iteration 6874, loss = 1.15878890\n",
      "Iteration 6875, loss = 1.15878660\n",
      "Iteration 6876, loss = 1.15877644\n",
      "Iteration 6877, loss = 1.15876966\n",
      "Iteration 6878, loss = 1.15875997\n",
      "Iteration 6879, loss = 1.15875675\n",
      "Iteration 6880, loss = 1.15876051\n",
      "Iteration 6881, loss = 1.15873991\n",
      "Iteration 6882, loss = 1.15873704\n",
      "Iteration 6883, loss = 1.15872535\n",
      "Iteration 6884, loss = 1.15871756\n",
      "Iteration 6885, loss = 1.15871071\n",
      "Iteration 6886, loss = 1.15870506\n",
      "Iteration 6887, loss = 1.15869567\n",
      "Iteration 6888, loss = 1.15868909\n",
      "Iteration 6889, loss = 1.15867904\n",
      "Iteration 6890, loss = 1.15867109\n",
      "Iteration 6891, loss = 1.15866368\n",
      "Iteration 6892, loss = 1.15865574\n",
      "Iteration 6893, loss = 1.15864970\n",
      "Iteration 6894, loss = 1.15864712\n",
      "Iteration 6895, loss = 1.15863567\n",
      "Iteration 6896, loss = 1.15862755\n",
      "Iteration 6897, loss = 1.15861922\n",
      "Iteration 6898, loss = 1.15861320\n",
      "Iteration 6899, loss = 1.15860987\n",
      "Iteration 6900, loss = 1.15859835\n",
      "Iteration 6901, loss = 1.15859385\n",
      "Iteration 6902, loss = 1.15858364\n",
      "Iteration 6903, loss = 1.15857525\n",
      "Iteration 6904, loss = 1.15857208\n",
      "Iteration 6905, loss = 1.15856146\n",
      "Iteration 6906, loss = 1.15855331\n",
      "Iteration 6907, loss = 1.15854631\n",
      "Iteration 6908, loss = 1.15854011\n",
      "Iteration 6909, loss = 1.15853386\n",
      "Iteration 6910, loss = 1.15852710\n",
      "Iteration 6911, loss = 1.15852419\n",
      "Iteration 6912, loss = 1.15850943\n",
      "Iteration 6913, loss = 1.15850827\n",
      "Iteration 6914, loss = 1.15850547\n",
      "Iteration 6915, loss = 1.15849181\n",
      "Iteration 6916, loss = 1.15848166\n",
      "Iteration 6917, loss = 1.15847503\n",
      "Iteration 6918, loss = 1.15847549\n",
      "Iteration 6919, loss = 1.15846049\n",
      "Iteration 6920, loss = 1.15845626\n",
      "Iteration 6921, loss = 1.15844212\n",
      "Iteration 6922, loss = 1.15843797\n",
      "Iteration 6923, loss = 1.15843176\n",
      "Iteration 6924, loss = 1.15843058\n",
      "Iteration 6925, loss = 1.15841213\n",
      "Iteration 6926, loss = 1.15840635\n",
      "Iteration 6927, loss = 1.15839874\n",
      "Iteration 6928, loss = 1.15839219\n",
      "Iteration 6929, loss = 1.15838497\n",
      "Iteration 6930, loss = 1.15837727\n",
      "Iteration 6931, loss = 1.15837319\n",
      "Iteration 6932, loss = 1.15836777\n",
      "Iteration 6933, loss = 1.15835621\n",
      "Iteration 6934, loss = 1.15834621\n",
      "Iteration 6935, loss = 1.15833840\n",
      "Iteration 6936, loss = 1.15833232\n",
      "Iteration 6937, loss = 1.15832690\n",
      "Iteration 6938, loss = 1.15831588\n",
      "Iteration 6939, loss = 1.15831263\n",
      "Iteration 6940, loss = 1.15830445\n",
      "Iteration 6941, loss = 1.15829502\n",
      "Iteration 6942, loss = 1.15829180\n",
      "Iteration 6943, loss = 1.15827965\n",
      "Iteration 6944, loss = 1.15827363\n",
      "Iteration 6945, loss = 1.15826785\n",
      "Iteration 6946, loss = 1.15826036\n",
      "Iteration 6947, loss = 1.15825188\n",
      "Iteration 6948, loss = 1.15824759\n",
      "Iteration 6949, loss = 1.15823736\n",
      "Iteration 6950, loss = 1.15823468\n",
      "Iteration 6951, loss = 1.15822837\n",
      "Iteration 6952, loss = 1.15821660\n",
      "Iteration 6953, loss = 1.15821405\n",
      "Iteration 6954, loss = 1.15820099\n",
      "Iteration 6955, loss = 1.15819770\n",
      "Iteration 6956, loss = 1.15818701\n",
      "Iteration 6957, loss = 1.15819030\n",
      "Iteration 6958, loss = 1.15817932\n",
      "Iteration 6959, loss = 1.15816876\n",
      "Iteration 6960, loss = 1.15815513\n",
      "Iteration 6961, loss = 1.15815083\n",
      "Iteration 6962, loss = 1.15813983\n",
      "Iteration 6963, loss = 1.15813863\n",
      "Iteration 6964, loss = 1.15812849\n",
      "Iteration 6965, loss = 1.15812677\n",
      "Iteration 6966, loss = 1.15811408\n",
      "Iteration 6967, loss = 1.15811027\n",
      "Iteration 6968, loss = 1.15809789\n",
      "Iteration 6969, loss = 1.15809087\n",
      "Iteration 6970, loss = 1.15808923\n",
      "Iteration 6971, loss = 1.15807715\n",
      "Iteration 6972, loss = 1.15807014\n",
      "Iteration 6973, loss = 1.15806403\n",
      "Iteration 6974, loss = 1.15805890\n",
      "Iteration 6975, loss = 1.15804519\n",
      "Iteration 6976, loss = 1.15804269\n",
      "Iteration 6977, loss = 1.15803418\n",
      "Iteration 6978, loss = 1.15803491\n",
      "Iteration 6979, loss = 1.15802247\n",
      "Iteration 6980, loss = 1.15800836\n",
      "Iteration 6981, loss = 1.15800732\n",
      "Iteration 6982, loss = 1.15799619\n",
      "Iteration 6983, loss = 1.15799045\n",
      "Iteration 6984, loss = 1.15798102\n",
      "Iteration 6985, loss = 1.15797398\n",
      "Iteration 6986, loss = 1.15797073\n",
      "Iteration 6987, loss = 1.15796348\n",
      "Iteration 6988, loss = 1.15795276\n",
      "Iteration 6989, loss = 1.15795620\n",
      "Iteration 6990, loss = 1.15793646\n",
      "Iteration 6991, loss = 1.15793755\n",
      "Iteration 6992, loss = 1.15792365\n",
      "Iteration 6993, loss = 1.15791713\n",
      "Iteration 6994, loss = 1.15791205\n",
      "Iteration 6995, loss = 1.15790235\n",
      "Iteration 6996, loss = 1.15789741\n",
      "Iteration 6997, loss = 1.15788651\n",
      "Iteration 6998, loss = 1.15788031\n",
      "Iteration 6999, loss = 1.15787235\n",
      "Iteration 7000, loss = 1.15786282\n",
      "Iteration 7001, loss = 1.15785829\n",
      "Iteration 7002, loss = 1.15785206\n",
      "Iteration 7003, loss = 1.15784388\n",
      "Iteration 7004, loss = 1.15783682\n",
      "Iteration 7005, loss = 1.15782885\n",
      "Iteration 7006, loss = 1.15781935\n",
      "Iteration 7007, loss = 1.15781287\n",
      "Iteration 7008, loss = 1.15781095\n",
      "Iteration 7009, loss = 1.15780010\n",
      "Iteration 7010, loss = 1.15779580\n",
      "Iteration 7011, loss = 1.15778932\n",
      "Iteration 7012, loss = 1.15778477\n",
      "Iteration 7013, loss = 1.15777143\n",
      "Iteration 7014, loss = 1.15776704\n",
      "Iteration 7015, loss = 1.15775759\n",
      "Iteration 7016, loss = 1.15776023\n",
      "Iteration 7017, loss = 1.15774308\n",
      "Iteration 7018, loss = 1.15773589\n",
      "Iteration 7019, loss = 1.15772670\n",
      "Iteration 7020, loss = 1.15771960\n",
      "Iteration 7021, loss = 1.15771558\n",
      "Iteration 7022, loss = 1.15770427\n",
      "Iteration 7023, loss = 1.15769972\n",
      "Iteration 7024, loss = 1.15769261\n",
      "Iteration 7025, loss = 1.15768640\n",
      "Iteration 7026, loss = 1.15768385\n",
      "Iteration 7027, loss = 1.15767403\n",
      "Iteration 7028, loss = 1.15766372\n",
      "Iteration 7029, loss = 1.15765205\n",
      "Iteration 7030, loss = 1.15764553\n",
      "Iteration 7031, loss = 1.15763947\n",
      "Iteration 7032, loss = 1.15763582\n",
      "Iteration 7033, loss = 1.15762444\n",
      "Iteration 7034, loss = 1.15761662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7035, loss = 1.15761031\n",
      "Iteration 7036, loss = 1.15760165\n",
      "Iteration 7037, loss = 1.15759580\n",
      "Iteration 7038, loss = 1.15758911\n",
      "Iteration 7039, loss = 1.15758203\n",
      "Iteration 7040, loss = 1.15757413\n",
      "Iteration 7041, loss = 1.15757306\n",
      "Iteration 7042, loss = 1.15755975\n",
      "Iteration 7043, loss = 1.15755658\n",
      "Iteration 7044, loss = 1.15754390\n",
      "Iteration 7045, loss = 1.15753804\n",
      "Iteration 7046, loss = 1.15753947\n",
      "Iteration 7047, loss = 1.15752412\n",
      "Iteration 7048, loss = 1.15751604\n",
      "Iteration 7049, loss = 1.15751146\n",
      "Iteration 7050, loss = 1.15750359\n",
      "Iteration 7051, loss = 1.15749493\n",
      "Iteration 7052, loss = 1.15750030\n",
      "Iteration 7053, loss = 1.15748075\n",
      "Iteration 7054, loss = 1.15747191\n",
      "Iteration 7055, loss = 1.15746483\n",
      "Iteration 7056, loss = 1.15746179\n",
      "Iteration 7057, loss = 1.15745222\n",
      "Iteration 7058, loss = 1.15744548\n",
      "Iteration 7059, loss = 1.15743564\n",
      "Iteration 7060, loss = 1.15743195\n",
      "Iteration 7061, loss = 1.15742351\n",
      "Iteration 7062, loss = 1.15741685\n",
      "Iteration 7063, loss = 1.15740656\n",
      "Iteration 7064, loss = 1.15739909\n",
      "Iteration 7065, loss = 1.15739848\n",
      "Iteration 7066, loss = 1.15739723\n",
      "Iteration 7067, loss = 1.15738376\n",
      "Iteration 7068, loss = 1.15737714\n",
      "Iteration 7069, loss = 1.15736926\n",
      "Iteration 7070, loss = 1.15736001\n",
      "Iteration 7071, loss = 1.15735208\n",
      "Iteration 7072, loss = 1.15734129\n",
      "Iteration 7073, loss = 1.15733643\n",
      "Iteration 7074, loss = 1.15733196\n",
      "Iteration 7075, loss = 1.15732942\n",
      "Iteration 7076, loss = 1.15731365\n",
      "Iteration 7077, loss = 1.15730859\n",
      "Iteration 7078, loss = 1.15730436\n",
      "Iteration 7079, loss = 1.15729697\n",
      "Iteration 7080, loss = 1.15728925\n",
      "Iteration 7081, loss = 1.15727710\n",
      "Iteration 7082, loss = 1.15727220\n",
      "Iteration 7083, loss = 1.15726658\n",
      "Iteration 7084, loss = 1.15725762\n",
      "Iteration 7085, loss = 1.15725165\n",
      "Iteration 7086, loss = 1.15724300\n",
      "Iteration 7087, loss = 1.15723782\n",
      "Iteration 7088, loss = 1.15722955\n",
      "Iteration 7089, loss = 1.15722146\n",
      "Iteration 7090, loss = 1.15721506\n",
      "Iteration 7091, loss = 1.15720686\n",
      "Iteration 7092, loss = 1.15720015\n",
      "Iteration 7093, loss = 1.15719231\n",
      "Iteration 7094, loss = 1.15719266\n",
      "Iteration 7095, loss = 1.15717928\n",
      "Iteration 7096, loss = 1.15716911\n",
      "Iteration 7097, loss = 1.15716293\n",
      "Iteration 7098, loss = 1.15715626\n",
      "Iteration 7099, loss = 1.15714929\n",
      "Iteration 7100, loss = 1.15715451\n",
      "Iteration 7101, loss = 1.15713271\n",
      "Iteration 7102, loss = 1.15712895\n",
      "Iteration 7103, loss = 1.15712743\n",
      "Iteration 7104, loss = 1.15711123\n",
      "Iteration 7105, loss = 1.15711049\n",
      "Iteration 7106, loss = 1.15710311\n",
      "Iteration 7107, loss = 1.15709219\n",
      "Iteration 7108, loss = 1.15709614\n",
      "Iteration 7109, loss = 1.15707854\n",
      "Iteration 7110, loss = 1.15706908\n",
      "Iteration 7111, loss = 1.15706960\n",
      "Iteration 7112, loss = 1.15705449\n",
      "Iteration 7113, loss = 1.15706030\n",
      "Iteration 7114, loss = 1.15704168\n",
      "Iteration 7115, loss = 1.15703833\n",
      "Iteration 7116, loss = 1.15702837\n",
      "Iteration 7117, loss = 1.15702697\n",
      "Iteration 7118, loss = 1.15701446\n",
      "Iteration 7119, loss = 1.15701457\n",
      "Iteration 7120, loss = 1.15699971\n",
      "Iteration 7121, loss = 1.15699348\n",
      "Iteration 7122, loss = 1.15698458\n",
      "Iteration 7123, loss = 1.15697913\n",
      "Iteration 7124, loss = 1.15696781\n",
      "Iteration 7125, loss = 1.15696471\n",
      "Iteration 7126, loss = 1.15695761\n",
      "Iteration 7127, loss = 1.15695472\n",
      "Iteration 7128, loss = 1.15695277\n",
      "Iteration 7129, loss = 1.15693619\n",
      "Iteration 7130, loss = 1.15692595\n",
      "Iteration 7131, loss = 1.15692022\n",
      "Iteration 7132, loss = 1.15691530\n",
      "Iteration 7133, loss = 1.15690679\n",
      "Iteration 7134, loss = 1.15690030\n",
      "Iteration 7135, loss = 1.15689111\n",
      "Iteration 7136, loss = 1.15688293\n",
      "Iteration 7137, loss = 1.15687838\n",
      "Iteration 7138, loss = 1.15686868\n",
      "Iteration 7139, loss = 1.15686435\n",
      "Iteration 7140, loss = 1.15685452\n",
      "Iteration 7141, loss = 1.15685027\n",
      "Iteration 7142, loss = 1.15684567\n",
      "Iteration 7143, loss = 1.15683973\n",
      "Iteration 7144, loss = 1.15682578\n",
      "Iteration 7145, loss = 1.15682257\n",
      "Iteration 7146, loss = 1.15681345\n",
      "Iteration 7147, loss = 1.15680515\n",
      "Iteration 7148, loss = 1.15679864\n",
      "Iteration 7149, loss = 1.15679308\n",
      "Iteration 7150, loss = 1.15678738\n",
      "Iteration 7151, loss = 1.15677583\n",
      "Iteration 7152, loss = 1.15677086\n",
      "Iteration 7153, loss = 1.15676184\n",
      "Iteration 7154, loss = 1.15675597\n",
      "Iteration 7155, loss = 1.15675370\n",
      "Iteration 7156, loss = 1.15674734\n",
      "Iteration 7157, loss = 1.15673739\n",
      "Iteration 7158, loss = 1.15672681\n",
      "Iteration 7159, loss = 1.15671929\n",
      "Iteration 7160, loss = 1.15672006\n",
      "Iteration 7161, loss = 1.15672030\n",
      "Iteration 7162, loss = 1.15669865\n",
      "Iteration 7163, loss = 1.15669560\n",
      "Iteration 7164, loss = 1.15668558\n",
      "Iteration 7165, loss = 1.15667647\n",
      "Iteration 7166, loss = 1.15668097\n",
      "Iteration 7167, loss = 1.15666168\n",
      "Iteration 7168, loss = 1.15665966\n",
      "Iteration 7169, loss = 1.15664768\n",
      "Iteration 7170, loss = 1.15664351\n",
      "Iteration 7171, loss = 1.15663643\n",
      "Iteration 7172, loss = 1.15662537\n",
      "Iteration 7173, loss = 1.15661987\n",
      "Iteration 7174, loss = 1.15662333\n",
      "Iteration 7175, loss = 1.15661276\n",
      "Iteration 7176, loss = 1.15660250\n",
      "Iteration 7177, loss = 1.15660113\n",
      "Iteration 7178, loss = 1.15658570\n",
      "Iteration 7179, loss = 1.15657629\n",
      "Iteration 7180, loss = 1.15656809\n",
      "Iteration 7181, loss = 1.15657176\n",
      "Iteration 7182, loss = 1.15655666\n",
      "Iteration 7183, loss = 1.15655139\n",
      "Iteration 7184, loss = 1.15654393\n",
      "Iteration 7185, loss = 1.15654318\n",
      "Iteration 7186, loss = 1.15652584\n",
      "Iteration 7187, loss = 1.15652515\n",
      "Iteration 7188, loss = 1.15651742\n",
      "Iteration 7189, loss = 1.15650612\n",
      "Iteration 7190, loss = 1.15651032\n",
      "Iteration 7191, loss = 1.15649341\n",
      "Iteration 7192, loss = 1.15648428\n",
      "Iteration 7193, loss = 1.15647957\n",
      "Iteration 7194, loss = 1.15646947\n",
      "Iteration 7195, loss = 1.15646442\n",
      "Iteration 7196, loss = 1.15645593\n",
      "Iteration 7197, loss = 1.15645282\n",
      "Iteration 7198, loss = 1.15644425\n",
      "Iteration 7199, loss = 1.15643957\n",
      "Iteration 7200, loss = 1.15642733\n",
      "Iteration 7201, loss = 1.15642898\n",
      "Iteration 7202, loss = 1.15641554\n",
      "Iteration 7203, loss = 1.15640733\n",
      "Iteration 7204, loss = 1.15639944\n",
      "Iteration 7205, loss = 1.15639669\n",
      "Iteration 7206, loss = 1.15638747\n",
      "Iteration 7207, loss = 1.15637835\n",
      "Iteration 7208, loss = 1.15637105\n",
      "Iteration 7209, loss = 1.15636513\n",
      "Iteration 7210, loss = 1.15636072\n",
      "Iteration 7211, loss = 1.15634952\n",
      "Iteration 7212, loss = 1.15634507\n",
      "Iteration 7213, loss = 1.15633681\n",
      "Iteration 7214, loss = 1.15632820\n",
      "Iteration 7215, loss = 1.15632043\n",
      "Iteration 7216, loss = 1.15631810\n",
      "Iteration 7217, loss = 1.15631319\n",
      "Iteration 7218, loss = 1.15630048\n",
      "Iteration 7219, loss = 1.15629156\n",
      "Iteration 7220, loss = 1.15629012\n",
      "Iteration 7221, loss = 1.15627827\n",
      "Iteration 7222, loss = 1.15627110\n",
      "Iteration 7223, loss = 1.15629604\n",
      "Iteration 7224, loss = 1.15625552\n",
      "Iteration 7225, loss = 1.15626697\n",
      "Iteration 7226, loss = 1.15624512\n",
      "Iteration 7227, loss = 1.15623922\n",
      "Iteration 7228, loss = 1.15623084\n",
      "Iteration 7229, loss = 1.15622912\n",
      "Iteration 7230, loss = 1.15621596\n",
      "Iteration 7231, loss = 1.15621174\n",
      "Iteration 7232, loss = 1.15620057\n",
      "Iteration 7233, loss = 1.15619281\n",
      "Iteration 7234, loss = 1.15618959\n",
      "Iteration 7235, loss = 1.15618296\n",
      "Iteration 7236, loss = 1.15617562\n",
      "Iteration 7237, loss = 1.15618094\n",
      "Iteration 7238, loss = 1.15616557\n",
      "Iteration 7239, loss = 1.15615336\n",
      "Iteration 7240, loss = 1.15614738\n",
      "Iteration 7241, loss = 1.15613829\n",
      "Iteration 7242, loss = 1.15613190\n",
      "Iteration 7243, loss = 1.15612285\n",
      "Iteration 7244, loss = 1.15612066\n",
      "Iteration 7245, loss = 1.15611127\n",
      "Iteration 7246, loss = 1.15610293\n",
      "Iteration 7247, loss = 1.15609821\n",
      "Iteration 7248, loss = 1.15609208\n",
      "Iteration 7249, loss = 1.15609019\n",
      "Iteration 7250, loss = 1.15607363\n",
      "Iteration 7251, loss = 1.15607987\n",
      "Iteration 7252, loss = 1.15605910\n",
      "Iteration 7253, loss = 1.15605948\n",
      "Iteration 7254, loss = 1.15605230\n",
      "Iteration 7255, loss = 1.15605831\n",
      "Iteration 7256, loss = 1.15603205\n",
      "Iteration 7257, loss = 1.15603679\n",
      "Iteration 7258, loss = 1.15601682\n",
      "Iteration 7259, loss = 1.15601537\n",
      "Iteration 7260, loss = 1.15601323\n",
      "Iteration 7261, loss = 1.15599927\n",
      "Iteration 7262, loss = 1.15600099\n",
      "Iteration 7263, loss = 1.15598529\n",
      "Iteration 7264, loss = 1.15597502\n",
      "Iteration 7265, loss = 1.15597883\n",
      "Iteration 7266, loss = 1.15596132\n",
      "Iteration 7267, loss = 1.15595798\n",
      "Iteration 7268, loss = 1.15594822\n",
      "Iteration 7269, loss = 1.15594390\n",
      "Iteration 7270, loss = 1.15593474\n",
      "Iteration 7271, loss = 1.15592736\n",
      "Iteration 7272, loss = 1.15591868\n",
      "Iteration 7273, loss = 1.15592597\n",
      "Iteration 7274, loss = 1.15592101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7275, loss = 1.15589771\n",
      "Iteration 7276, loss = 1.15589847\n",
      "Iteration 7277, loss = 1.15588591\n",
      "Iteration 7278, loss = 1.15588451\n",
      "Iteration 7279, loss = 1.15586932\n",
      "Iteration 7280, loss = 1.15586265\n",
      "Iteration 7281, loss = 1.15585925\n",
      "Iteration 7282, loss = 1.15584891\n",
      "Iteration 7283, loss = 1.15584566\n",
      "Iteration 7284, loss = 1.15584035\n",
      "Iteration 7285, loss = 1.15582922\n",
      "Iteration 7286, loss = 1.15582806\n",
      "Iteration 7287, loss = 1.15582003\n",
      "Iteration 7288, loss = 1.15581509\n",
      "Iteration 7289, loss = 1.15580908\n",
      "Iteration 7290, loss = 1.15579550\n",
      "Iteration 7291, loss = 1.15579299\n",
      "Iteration 7292, loss = 1.15578012\n",
      "Iteration 7293, loss = 1.15577894\n",
      "Iteration 7294, loss = 1.15577145\n",
      "Iteration 7295, loss = 1.15576149\n",
      "Iteration 7296, loss = 1.15575136\n",
      "Iteration 7297, loss = 1.15574351\n",
      "Iteration 7298, loss = 1.15573819\n",
      "Iteration 7299, loss = 1.15573329\n",
      "Iteration 7300, loss = 1.15572456\n",
      "Iteration 7301, loss = 1.15571740\n",
      "Iteration 7302, loss = 1.15571049\n",
      "Iteration 7303, loss = 1.15570871\n",
      "Iteration 7304, loss = 1.15569530\n",
      "Iteration 7305, loss = 1.15568988\n",
      "Iteration 7306, loss = 1.15568040\n",
      "Iteration 7307, loss = 1.15568879\n",
      "Iteration 7308, loss = 1.15566814\n",
      "Iteration 7309, loss = 1.15566076\n",
      "Iteration 7310, loss = 1.15567027\n",
      "Iteration 7311, loss = 1.15564853\n",
      "Iteration 7312, loss = 1.15564352\n",
      "Iteration 7313, loss = 1.15563956\n",
      "Iteration 7314, loss = 1.15564357\n",
      "Iteration 7315, loss = 1.15561979\n",
      "Iteration 7316, loss = 1.15561286\n",
      "Iteration 7317, loss = 1.15560809\n",
      "Iteration 7318, loss = 1.15561054\n",
      "Iteration 7319, loss = 1.15559998\n",
      "Iteration 7320, loss = 1.15558445\n",
      "Iteration 7321, loss = 1.15557646\n",
      "Iteration 7322, loss = 1.15557313\n",
      "Iteration 7323, loss = 1.15556114\n",
      "Iteration 7324, loss = 1.15556171\n",
      "Iteration 7325, loss = 1.15555417\n",
      "Iteration 7326, loss = 1.15554074\n",
      "Iteration 7327, loss = 1.15553738\n",
      "Iteration 7328, loss = 1.15552868\n",
      "Iteration 7329, loss = 1.15553755\n",
      "Iteration 7330, loss = 1.15551353\n",
      "Iteration 7331, loss = 1.15550631\n",
      "Iteration 7332, loss = 1.15550669\n",
      "Iteration 7333, loss = 1.15550325\n",
      "Iteration 7334, loss = 1.15548975\n",
      "Iteration 7335, loss = 1.15549226\n",
      "Iteration 7336, loss = 1.15547926\n",
      "Iteration 7337, loss = 1.15546832\n",
      "Iteration 7338, loss = 1.15545824\n",
      "Iteration 7339, loss = 1.15545305\n",
      "Iteration 7340, loss = 1.15544379\n",
      "Iteration 7341, loss = 1.15543847\n",
      "Iteration 7342, loss = 1.15543104\n",
      "Iteration 7343, loss = 1.15542437\n",
      "Iteration 7344, loss = 1.15543021\n",
      "Iteration 7345, loss = 1.15540969\n",
      "Iteration 7346, loss = 1.15540397\n",
      "Iteration 7347, loss = 1.15539680\n",
      "Iteration 7348, loss = 1.15538975\n",
      "Iteration 7349, loss = 1.15538068\n",
      "Iteration 7350, loss = 1.15537246\n",
      "Iteration 7351, loss = 1.15536868\n",
      "Iteration 7352, loss = 1.15536002\n",
      "Iteration 7353, loss = 1.15535973\n",
      "Iteration 7354, loss = 1.15534829\n",
      "Iteration 7355, loss = 1.15534238\n",
      "Iteration 7356, loss = 1.15533464\n",
      "Iteration 7357, loss = 1.15532977\n",
      "Iteration 7358, loss = 1.15532355\n",
      "Iteration 7359, loss = 1.15531140\n",
      "Iteration 7360, loss = 1.15530794\n",
      "Iteration 7361, loss = 1.15531576\n",
      "Iteration 7362, loss = 1.15529054\n",
      "Iteration 7363, loss = 1.15528515\n",
      "Iteration 7364, loss = 1.15527681\n",
      "Iteration 7365, loss = 1.15527036\n",
      "Iteration 7366, loss = 1.15526475\n",
      "Iteration 7367, loss = 1.15525610\n",
      "Iteration 7368, loss = 1.15525153\n",
      "Iteration 7369, loss = 1.15524515\n",
      "Iteration 7370, loss = 1.15523777\n",
      "Iteration 7371, loss = 1.15522945\n",
      "Iteration 7372, loss = 1.15522738\n",
      "Iteration 7373, loss = 1.15522262\n",
      "Iteration 7374, loss = 1.15520806\n",
      "Iteration 7375, loss = 1.15520356\n",
      "Iteration 7376, loss = 1.15519160\n",
      "Iteration 7377, loss = 1.15519139\n",
      "Iteration 7378, loss = 1.15518174\n",
      "Iteration 7379, loss = 1.15518165\n",
      "Iteration 7380, loss = 1.15516987\n",
      "Iteration 7381, loss = 1.15515962\n",
      "Iteration 7382, loss = 1.15516064\n",
      "Iteration 7383, loss = 1.15514721\n",
      "Iteration 7384, loss = 1.15514564\n",
      "Iteration 7385, loss = 1.15513735\n",
      "Iteration 7386, loss = 1.15512877\n",
      "Iteration 7387, loss = 1.15511839\n",
      "Iteration 7388, loss = 1.15511344\n",
      "Iteration 7389, loss = 1.15512905\n",
      "Iteration 7390, loss = 1.15510111\n",
      "Iteration 7391, loss = 1.15509125\n",
      "Iteration 7392, loss = 1.15508716\n",
      "Iteration 7393, loss = 1.15507822\n",
      "Iteration 7394, loss = 1.15507403\n",
      "Iteration 7395, loss = 1.15506560\n",
      "Iteration 7396, loss = 1.15505719\n",
      "Iteration 7397, loss = 1.15504973\n",
      "Iteration 7398, loss = 1.15505467\n",
      "Iteration 7399, loss = 1.15503561\n",
      "Iteration 7400, loss = 1.15502866\n",
      "Iteration 7401, loss = 1.15502657\n",
      "Iteration 7402, loss = 1.15501372\n",
      "Iteration 7403, loss = 1.15500779\n",
      "Iteration 7404, loss = 1.15499986\n",
      "Iteration 7405, loss = 1.15499974\n",
      "Iteration 7406, loss = 1.15499148\n",
      "Iteration 7407, loss = 1.15497964\n",
      "Iteration 7408, loss = 1.15497192\n",
      "Iteration 7409, loss = 1.15496581\n",
      "Iteration 7410, loss = 1.15495920\n",
      "Iteration 7411, loss = 1.15495318\n",
      "Iteration 7412, loss = 1.15494895\n",
      "Iteration 7413, loss = 1.15494538\n",
      "Iteration 7414, loss = 1.15493722\n",
      "Iteration 7415, loss = 1.15492714\n",
      "Iteration 7416, loss = 1.15492117\n",
      "Iteration 7417, loss = 1.15491762\n",
      "Iteration 7418, loss = 1.15490270\n",
      "Iteration 7419, loss = 1.15490058\n",
      "Iteration 7420, loss = 1.15490592\n",
      "Iteration 7421, loss = 1.15488675\n",
      "Iteration 7422, loss = 1.15487961\n",
      "Iteration 7423, loss = 1.15487118\n",
      "Iteration 7424, loss = 1.15486383\n",
      "Iteration 7425, loss = 1.15485434\n",
      "Iteration 7426, loss = 1.15485032\n",
      "Iteration 7427, loss = 1.15484252\n",
      "Iteration 7428, loss = 1.15484264\n",
      "Iteration 7429, loss = 1.15483557\n",
      "Iteration 7430, loss = 1.15482320\n",
      "Iteration 7431, loss = 1.15482374\n",
      "Iteration 7432, loss = 1.15481044\n",
      "Iteration 7433, loss = 1.15480262\n",
      "Iteration 7434, loss = 1.15479274\n",
      "Iteration 7435, loss = 1.15478728\n",
      "Iteration 7436, loss = 1.15477934\n",
      "Iteration 7437, loss = 1.15477485\n",
      "Iteration 7438, loss = 1.15476707\n",
      "Iteration 7439, loss = 1.15475805\n",
      "Iteration 7440, loss = 1.15475194\n",
      "Iteration 7441, loss = 1.15474412\n",
      "Iteration 7442, loss = 1.15474417\n",
      "Iteration 7443, loss = 1.15473140\n",
      "Iteration 7444, loss = 1.15472528\n",
      "Iteration 7445, loss = 1.15472611\n",
      "Iteration 7446, loss = 1.15471497\n",
      "Iteration 7447, loss = 1.15471545\n",
      "Iteration 7448, loss = 1.15469654\n",
      "Iteration 7449, loss = 1.15469853\n",
      "Iteration 7450, loss = 1.15468305\n",
      "Iteration 7451, loss = 1.15467596\n",
      "Iteration 7452, loss = 1.15467279\n",
      "Iteration 7453, loss = 1.15466522\n",
      "Iteration 7454, loss = 1.15466103\n",
      "Iteration 7455, loss = 1.15464810\n",
      "Iteration 7456, loss = 1.15465032\n",
      "Iteration 7457, loss = 1.15463528\n",
      "Iteration 7458, loss = 1.15463643\n",
      "Iteration 7459, loss = 1.15463623\n",
      "Iteration 7460, loss = 1.15461630\n",
      "Iteration 7461, loss = 1.15461111\n",
      "Iteration 7462, loss = 1.15460423\n",
      "Iteration 7463, loss = 1.15459960\n",
      "Iteration 7464, loss = 1.15459230\n",
      "Iteration 7465, loss = 1.15458549\n",
      "Iteration 7466, loss = 1.15458460\n",
      "Iteration 7467, loss = 1.15456715\n",
      "Iteration 7468, loss = 1.15458570\n",
      "Iteration 7469, loss = 1.15455979\n",
      "Iteration 7470, loss = 1.15454788\n",
      "Iteration 7471, loss = 1.15454104\n",
      "Iteration 7472, loss = 1.15453748\n",
      "Iteration 7473, loss = 1.15452581\n",
      "Iteration 7474, loss = 1.15451831\n",
      "Iteration 7475, loss = 1.15452419\n",
      "Iteration 7476, loss = 1.15451201\n",
      "Iteration 7477, loss = 1.15449785\n",
      "Iteration 7478, loss = 1.15449116\n",
      "Iteration 7479, loss = 1.15448455\n",
      "Iteration 7480, loss = 1.15448350\n",
      "Iteration 7481, loss = 1.15447342\n",
      "Iteration 7482, loss = 1.15446504\n",
      "Iteration 7483, loss = 1.15445976\n",
      "Iteration 7484, loss = 1.15445820\n",
      "Iteration 7485, loss = 1.15444941\n",
      "Iteration 7486, loss = 1.15444198\n",
      "Iteration 7487, loss = 1.15443636\n",
      "Iteration 7488, loss = 1.15442633\n",
      "Iteration 7489, loss = 1.15442025\n",
      "Iteration 7490, loss = 1.15441041\n",
      "Iteration 7491, loss = 1.15440874\n",
      "Iteration 7492, loss = 1.15440782\n",
      "Iteration 7493, loss = 1.15438859\n",
      "Iteration 7494, loss = 1.15438236\n",
      "Iteration 7495, loss = 1.15437837\n",
      "Iteration 7496, loss = 1.15436879\n",
      "Iteration 7497, loss = 1.15436404\n",
      "Iteration 7498, loss = 1.15435489\n",
      "Iteration 7499, loss = 1.15434896\n",
      "Iteration 7500, loss = 1.15434818\n",
      "Iteration 7501, loss = 1.15433835\n",
      "Iteration 7502, loss = 1.15433391\n",
      "Iteration 7503, loss = 1.15432904\n",
      "Iteration 7504, loss = 1.15431775\n",
      "Iteration 7505, loss = 1.15431330\n",
      "Iteration 7506, loss = 1.15430181\n",
      "Iteration 7507, loss = 1.15429584\n",
      "Iteration 7508, loss = 1.15429032\n",
      "Iteration 7509, loss = 1.15428697\n",
      "Iteration 7510, loss = 1.15427312\n",
      "Iteration 7511, loss = 1.15426496\n",
      "Iteration 7512, loss = 1.15426071\n",
      "Iteration 7513, loss = 1.15425858\n",
      "Iteration 7514, loss = 1.15425060\n",
      "Iteration 7515, loss = 1.15423819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7516, loss = 1.15423367\n",
      "Iteration 7517, loss = 1.15422612\n",
      "Iteration 7518, loss = 1.15422518\n",
      "Iteration 7519, loss = 1.15421472\n",
      "Iteration 7520, loss = 1.15420426\n",
      "Iteration 7521, loss = 1.15420519\n",
      "Iteration 7522, loss = 1.15420305\n",
      "Iteration 7523, loss = 1.15418814\n",
      "Iteration 7524, loss = 1.15418263\n",
      "Iteration 7525, loss = 1.15417069\n",
      "Iteration 7526, loss = 1.15418238\n",
      "Iteration 7527, loss = 1.15415819\n",
      "Iteration 7528, loss = 1.15415175\n",
      "Iteration 7529, loss = 1.15414353\n",
      "Iteration 7530, loss = 1.15413823\n",
      "Iteration 7531, loss = 1.15413217\n",
      "Iteration 7532, loss = 1.15412704\n",
      "Iteration 7533, loss = 1.15412131\n",
      "Iteration 7534, loss = 1.15411511\n",
      "Iteration 7535, loss = 1.15410182\n",
      "Iteration 7536, loss = 1.15410245\n",
      "Iteration 7537, loss = 1.15408855\n",
      "Iteration 7538, loss = 1.15408150\n",
      "Iteration 7539, loss = 1.15408645\n",
      "Iteration 7540, loss = 1.15408271\n",
      "Iteration 7541, loss = 1.15406414\n",
      "Iteration 7542, loss = 1.15405446\n",
      "Iteration 7543, loss = 1.15404757\n",
      "Iteration 7544, loss = 1.15404352\n",
      "Iteration 7545, loss = 1.15403715\n",
      "Iteration 7546, loss = 1.15403918\n",
      "Iteration 7547, loss = 1.15402128\n",
      "Iteration 7548, loss = 1.15401521\n",
      "Iteration 7549, loss = 1.15400985\n",
      "Iteration 7550, loss = 1.15400570\n",
      "Iteration 7551, loss = 1.15399490\n",
      "Iteration 7552, loss = 1.15399955\n",
      "Iteration 7553, loss = 1.15398082\n",
      "Iteration 7554, loss = 1.15397270\n",
      "Iteration 7555, loss = 1.15398325\n",
      "Iteration 7556, loss = 1.15396001\n",
      "Iteration 7557, loss = 1.15395462\n",
      "Iteration 7558, loss = 1.15395056\n",
      "Iteration 7559, loss = 1.15394711\n",
      "Iteration 7560, loss = 1.15393429\n",
      "Iteration 7561, loss = 1.15392736\n",
      "Iteration 7562, loss = 1.15391939\n",
      "Iteration 7563, loss = 1.15391677\n",
      "Iteration 7564, loss = 1.15390956\n",
      "Iteration 7565, loss = 1.15389814\n",
      "Iteration 7566, loss = 1.15390292\n",
      "Iteration 7567, loss = 1.15388476\n",
      "Iteration 7568, loss = 1.15388139\n",
      "Iteration 7569, loss = 1.15389780\n",
      "Iteration 7570, loss = 1.15386716\n",
      "Iteration 7571, loss = 1.15386022\n",
      "Iteration 7572, loss = 1.15386067\n",
      "Iteration 7573, loss = 1.15384762\n",
      "Iteration 7574, loss = 1.15384110\n",
      "Iteration 7575, loss = 1.15383131\n",
      "Iteration 7576, loss = 1.15382714\n",
      "Iteration 7577, loss = 1.15382010\n",
      "Iteration 7578, loss = 1.15382785\n",
      "Iteration 7579, loss = 1.15380447\n",
      "Iteration 7580, loss = 1.15380046\n",
      "Iteration 7581, loss = 1.15379516\n",
      "Iteration 7582, loss = 1.15378939\n",
      "Iteration 7583, loss = 1.15378142\n",
      "Iteration 7584, loss = 1.15377148\n",
      "Iteration 7585, loss = 1.15376634\n",
      "Iteration 7586, loss = 1.15376061\n",
      "Iteration 7587, loss = 1.15375046\n",
      "Iteration 7588, loss = 1.15374290\n",
      "Iteration 7589, loss = 1.15373601\n",
      "Iteration 7590, loss = 1.15373191\n",
      "Iteration 7591, loss = 1.15372515\n",
      "Iteration 7592, loss = 1.15372457\n",
      "Iteration 7593, loss = 1.15372102\n",
      "Iteration 7594, loss = 1.15370785\n",
      "Iteration 7595, loss = 1.15370159\n",
      "Iteration 7596, loss = 1.15369678\n",
      "Iteration 7597, loss = 1.15368286\n",
      "Iteration 7598, loss = 1.15367537\n",
      "Iteration 7599, loss = 1.15367526\n",
      "Iteration 7600, loss = 1.15366371\n",
      "Iteration 7601, loss = 1.15365887\n",
      "Iteration 7602, loss = 1.15365755\n",
      "Iteration 7603, loss = 1.15364448\n",
      "Iteration 7604, loss = 1.15364036\n",
      "Iteration 7605, loss = 1.15365347\n",
      "Iteration 7606, loss = 1.15362182\n",
      "Iteration 7607, loss = 1.15362641\n",
      "Iteration 7608, loss = 1.15361480\n",
      "Iteration 7609, loss = 1.15361571\n",
      "Iteration 7610, loss = 1.15360691\n",
      "Iteration 7611, loss = 1.15359683\n",
      "Iteration 7612, loss = 1.15358945\n",
      "Iteration 7613, loss = 1.15357928\n",
      "Iteration 7614, loss = 1.15356704\n",
      "Iteration 7615, loss = 1.15357058\n",
      "Iteration 7616, loss = 1.15355304\n",
      "Iteration 7617, loss = 1.15354954\n",
      "Iteration 7618, loss = 1.15355063\n",
      "Iteration 7619, loss = 1.15355228\n",
      "Iteration 7620, loss = 1.15354283\n",
      "Iteration 7621, loss = 1.15352038\n",
      "Iteration 7622, loss = 1.15351619\n",
      "Iteration 7623, loss = 1.15351573\n",
      "Iteration 7624, loss = 1.15350098\n",
      "Iteration 7625, loss = 1.15349589\n",
      "Iteration 7626, loss = 1.15348599\n",
      "Iteration 7627, loss = 1.15348190\n",
      "Iteration 7628, loss = 1.15348829\n",
      "Iteration 7629, loss = 1.15346785\n",
      "Iteration 7630, loss = 1.15346074\n",
      "Iteration 7631, loss = 1.15345511\n",
      "Iteration 7632, loss = 1.15345090\n",
      "Iteration 7633, loss = 1.15344503\n",
      "Iteration 7634, loss = 1.15343341\n",
      "Iteration 7635, loss = 1.15343088\n",
      "Iteration 7636, loss = 1.15342138\n",
      "Iteration 7637, loss = 1.15342273\n",
      "Iteration 7638, loss = 1.15341268\n",
      "Iteration 7639, loss = 1.15339871\n",
      "Iteration 7640, loss = 1.15339808\n",
      "Iteration 7641, loss = 1.15338448\n",
      "Iteration 7642, loss = 1.15338318\n",
      "Iteration 7643, loss = 1.15337574\n",
      "Iteration 7644, loss = 1.15336902\n",
      "Iteration 7645, loss = 1.15336296\n",
      "Iteration 7646, loss = 1.15335609\n",
      "Iteration 7647, loss = 1.15334590\n",
      "Iteration 7648, loss = 1.15334141\n",
      "Iteration 7649, loss = 1.15333245\n",
      "Iteration 7650, loss = 1.15332422\n",
      "Iteration 7651, loss = 1.15333271\n",
      "Iteration 7652, loss = 1.15331510\n",
      "Iteration 7653, loss = 1.15330822\n",
      "Iteration 7654, loss = 1.15330680\n",
      "Iteration 7655, loss = 1.15329415\n",
      "Iteration 7656, loss = 1.15328661\n",
      "Iteration 7657, loss = 1.15328237\n",
      "Iteration 7658, loss = 1.15327926\n",
      "Iteration 7659, loss = 1.15326806\n",
      "Iteration 7660, loss = 1.15326061\n",
      "Iteration 7661, loss = 1.15325323\n",
      "Iteration 7662, loss = 1.15325412\n",
      "Iteration 7663, loss = 1.15324857\n",
      "Iteration 7664, loss = 1.15323427\n",
      "Iteration 7665, loss = 1.15322450\n",
      "Iteration 7666, loss = 1.15322919\n",
      "Iteration 7667, loss = 1.15322731\n",
      "Iteration 7668, loss = 1.15321029\n",
      "Iteration 7669, loss = 1.15320201\n",
      "Iteration 7670, loss = 1.15319620\n",
      "Iteration 7671, loss = 1.15318551\n",
      "Iteration 7672, loss = 1.15318368\n",
      "Iteration 7673, loss = 1.15317137\n",
      "Iteration 7674, loss = 1.15317524\n",
      "Iteration 7675, loss = 1.15316280\n",
      "Iteration 7676, loss = 1.15315418\n",
      "Iteration 7677, loss = 1.15314748\n",
      "Iteration 7678, loss = 1.15314109\n",
      "Iteration 7679, loss = 1.15313505\n",
      "Iteration 7680, loss = 1.15313773\n",
      "Iteration 7681, loss = 1.15312076\n",
      "Iteration 7682, loss = 1.15311863\n",
      "Iteration 7683, loss = 1.15310892\n",
      "Iteration 7684, loss = 1.15310704\n",
      "Iteration 7685, loss = 1.15309086\n",
      "Iteration 7686, loss = 1.15308550\n",
      "Iteration 7687, loss = 1.15308097\n",
      "Iteration 7688, loss = 1.15308202\n",
      "Iteration 7689, loss = 1.15306502\n",
      "Iteration 7690, loss = 1.15305710\n",
      "Iteration 7691, loss = 1.15305156\n",
      "Iteration 7692, loss = 1.15304574\n",
      "Iteration 7693, loss = 1.15304220\n",
      "Iteration 7694, loss = 1.15303192\n",
      "Iteration 7695, loss = 1.15303241\n",
      "Iteration 7696, loss = 1.15302106\n",
      "Iteration 7697, loss = 1.15300958\n",
      "Iteration 7698, loss = 1.15300856\n",
      "Iteration 7699, loss = 1.15300744\n",
      "Iteration 7700, loss = 1.15299017\n",
      "Iteration 7701, loss = 1.15298433\n",
      "Iteration 7702, loss = 1.15298167\n",
      "Iteration 7703, loss = 1.15297173\n",
      "Iteration 7704, loss = 1.15296653\n",
      "Iteration 7705, loss = 1.15295775\n",
      "Iteration 7706, loss = 1.15295899\n",
      "Iteration 7707, loss = 1.15294729\n",
      "Iteration 7708, loss = 1.15294291\n",
      "Iteration 7709, loss = 1.15293547\n",
      "Iteration 7710, loss = 1.15292889\n",
      "Iteration 7711, loss = 1.15292389\n",
      "Iteration 7712, loss = 1.15292524\n",
      "Iteration 7713, loss = 1.15290621\n",
      "Iteration 7714, loss = 1.15289626\n",
      "Iteration 7715, loss = 1.15289449\n",
      "Iteration 7716, loss = 1.15289087\n",
      "Iteration 7717, loss = 1.15287863\n",
      "Iteration 7718, loss = 1.15287092\n",
      "Iteration 7719, loss = 1.15286525\n",
      "Iteration 7720, loss = 1.15285880\n",
      "Iteration 7721, loss = 1.15285471\n",
      "Iteration 7722, loss = 1.15285022\n",
      "Iteration 7723, loss = 1.15285105\n",
      "Iteration 7724, loss = 1.15283147\n",
      "Iteration 7725, loss = 1.15282953\n",
      "Iteration 7726, loss = 1.15282239\n",
      "Iteration 7727, loss = 1.15281878\n",
      "Iteration 7728, loss = 1.15280892\n",
      "Iteration 7729, loss = 1.15280364\n",
      "Iteration 7730, loss = 1.15279504\n",
      "Iteration 7731, loss = 1.15278582\n",
      "Iteration 7732, loss = 1.15277861\n",
      "Iteration 7733, loss = 1.15277033\n",
      "Iteration 7734, loss = 1.15276324\n",
      "Iteration 7735, loss = 1.15275651\n",
      "Iteration 7736, loss = 1.15276375\n",
      "Iteration 7737, loss = 1.15274783\n",
      "Iteration 7738, loss = 1.15273740\n",
      "Iteration 7739, loss = 1.15273587\n",
      "Iteration 7740, loss = 1.15273143\n",
      "Iteration 7741, loss = 1.15271963\n",
      "Iteration 7742, loss = 1.15271706\n",
      "Iteration 7743, loss = 1.15270336\n",
      "Iteration 7744, loss = 1.15269700\n",
      "Iteration 7745, loss = 1.15269163\n",
      "Iteration 7746, loss = 1.15268408\n",
      "Iteration 7747, loss = 1.15267687\n",
      "Iteration 7748, loss = 1.15267543\n",
      "Iteration 7749, loss = 1.15267290\n",
      "Iteration 7750, loss = 1.15266308\n",
      "Iteration 7751, loss = 1.15265419\n",
      "Iteration 7752, loss = 1.15265561\n",
      "Iteration 7753, loss = 1.15264195\n",
      "Iteration 7754, loss = 1.15263646\n",
      "Iteration 7755, loss = 1.15262350\n",
      "Iteration 7756, loss = 1.15261701\n",
      "Iteration 7757, loss = 1.15261697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7758, loss = 1.15261665\n",
      "Iteration 7759, loss = 1.15259800\n",
      "Iteration 7760, loss = 1.15259630\n",
      "Iteration 7761, loss = 1.15259942\n",
      "Iteration 7762, loss = 1.15257890\n",
      "Iteration 7763, loss = 1.15256929\n",
      "Iteration 7764, loss = 1.15256467\n",
      "Iteration 7765, loss = 1.15256548\n",
      "Iteration 7766, loss = 1.15255619\n",
      "Iteration 7767, loss = 1.15255563\n",
      "Iteration 7768, loss = 1.15253989\n",
      "Iteration 7769, loss = 1.15253251\n",
      "Iteration 7770, loss = 1.15252858\n",
      "Iteration 7771, loss = 1.15252311\n",
      "Iteration 7772, loss = 1.15252147\n",
      "Iteration 7773, loss = 1.15250989\n",
      "Iteration 7774, loss = 1.15249848\n",
      "Iteration 7775, loss = 1.15250101\n",
      "Iteration 7776, loss = 1.15248601\n",
      "Iteration 7777, loss = 1.15249622\n",
      "Iteration 7778, loss = 1.15247415\n",
      "Iteration 7779, loss = 1.15247300\n",
      "Iteration 7780, loss = 1.15245838\n",
      "Iteration 7781, loss = 1.15245608\n",
      "Iteration 7782, loss = 1.15245010\n",
      "Iteration 7783, loss = 1.15244529\n",
      "Iteration 7784, loss = 1.15243916\n",
      "Iteration 7785, loss = 1.15243225\n",
      "Iteration 7786, loss = 1.15243668\n",
      "Iteration 7787, loss = 1.15241458\n",
      "Iteration 7788, loss = 1.15240802\n",
      "Iteration 7789, loss = 1.15239758\n",
      "Iteration 7790, loss = 1.15239661\n",
      "Iteration 7791, loss = 1.15238478\n",
      "Iteration 7792, loss = 1.15238047\n",
      "Iteration 7793, loss = 1.15237583\n",
      "Iteration 7794, loss = 1.15237002\n",
      "Iteration 7795, loss = 1.15235893\n",
      "Iteration 7796, loss = 1.15235224\n",
      "Iteration 7797, loss = 1.15235443\n",
      "Iteration 7798, loss = 1.15234005\n",
      "Iteration 7799, loss = 1.15233849\n",
      "Iteration 7800, loss = 1.15232595\n",
      "Iteration 7801, loss = 1.15232028\n",
      "Iteration 7802, loss = 1.15232194\n",
      "Iteration 7803, loss = 1.15230801\n",
      "Iteration 7804, loss = 1.15230055\n",
      "Iteration 7805, loss = 1.15229345\n",
      "Iteration 7806, loss = 1.15228975\n",
      "Iteration 7807, loss = 1.15227892\n",
      "Iteration 7808, loss = 1.15227189\n",
      "Iteration 7809, loss = 1.15227656\n",
      "Iteration 7810, loss = 1.15226781\n",
      "Iteration 7811, loss = 1.15225242\n",
      "Iteration 7812, loss = 1.15225531\n",
      "Iteration 7813, loss = 1.15226431\n",
      "Iteration 7814, loss = 1.15223777\n",
      "Iteration 7815, loss = 1.15223707\n",
      "Iteration 7816, loss = 1.15223254\n",
      "Iteration 7817, loss = 1.15221553\n",
      "Iteration 7818, loss = 1.15220918\n",
      "Iteration 7819, loss = 1.15220240\n",
      "Iteration 7820, loss = 1.15219491\n",
      "Iteration 7821, loss = 1.15220239\n",
      "Iteration 7822, loss = 1.15218036\n",
      "Iteration 7823, loss = 1.15217538\n",
      "Iteration 7824, loss = 1.15217211\n",
      "Iteration 7825, loss = 1.15216723\n",
      "Iteration 7826, loss = 1.15215601\n",
      "Iteration 7827, loss = 1.15215925\n",
      "Iteration 7828, loss = 1.15214179\n",
      "Iteration 7829, loss = 1.15213527\n",
      "Iteration 7830, loss = 1.15213040\n",
      "Iteration 7831, loss = 1.15212709\n",
      "Iteration 7832, loss = 1.15211508\n",
      "Iteration 7833, loss = 1.15210832\n",
      "Iteration 7834, loss = 1.15210509\n",
      "Iteration 7835, loss = 1.15209595\n",
      "Iteration 7836, loss = 1.15208928\n",
      "Iteration 7837, loss = 1.15208637\n",
      "Iteration 7838, loss = 1.15207320\n",
      "Iteration 7839, loss = 1.15207027\n",
      "Iteration 7840, loss = 1.15207373\n",
      "Iteration 7841, loss = 1.15205814\n",
      "Iteration 7842, loss = 1.15205650\n",
      "Iteration 7843, loss = 1.15204743\n",
      "Iteration 7844, loss = 1.15203634\n",
      "Iteration 7845, loss = 1.15203572\n",
      "Iteration 7846, loss = 1.15203088\n",
      "Iteration 7847, loss = 1.15202241\n",
      "Iteration 7848, loss = 1.15201609\n",
      "Iteration 7849, loss = 1.15200245\n",
      "Iteration 7850, loss = 1.15199529\n",
      "Iteration 7851, loss = 1.15200125\n",
      "Iteration 7852, loss = 1.15198524\n",
      "Iteration 7853, loss = 1.15198856\n",
      "Iteration 7854, loss = 1.15196815\n",
      "Iteration 7855, loss = 1.15196256\n",
      "Iteration 7856, loss = 1.15195528\n",
      "Iteration 7857, loss = 1.15195402\n",
      "Iteration 7858, loss = 1.15195623\n",
      "Iteration 7859, loss = 1.15193823\n",
      "Iteration 7860, loss = 1.15193596\n",
      "Iteration 7861, loss = 1.15192856\n",
      "Iteration 7862, loss = 1.15191750\n",
      "Iteration 7863, loss = 1.15192000\n",
      "Iteration 7864, loss = 1.15190542\n",
      "Iteration 7865, loss = 1.15189753\n",
      "Iteration 7866, loss = 1.15188831\n",
      "Iteration 7867, loss = 1.15188204\n",
      "Iteration 7868, loss = 1.15188565\n",
      "Iteration 7869, loss = 1.15186884\n",
      "Iteration 7870, loss = 1.15186686\n",
      "Iteration 7871, loss = 1.15186649\n",
      "Iteration 7872, loss = 1.15186138\n",
      "Iteration 7873, loss = 1.15185441\n",
      "Iteration 7874, loss = 1.15184791\n",
      "Iteration 7875, loss = 1.15184170\n",
      "Iteration 7876, loss = 1.15183454\n",
      "Iteration 7877, loss = 1.15182983\n",
      "Iteration 7878, loss = 1.15182505\n",
      "Iteration 7879, loss = 1.15180437\n",
      "Iteration 7880, loss = 1.15180511\n",
      "Iteration 7881, loss = 1.15180013\n",
      "Iteration 7882, loss = 1.15178728\n",
      "Iteration 7883, loss = 1.15178290\n",
      "Iteration 7884, loss = 1.15177593\n",
      "Iteration 7885, loss = 1.15178720\n",
      "Iteration 7886, loss = 1.15176742\n",
      "Iteration 7887, loss = 1.15175251\n",
      "Iteration 7888, loss = 1.15174402\n",
      "Iteration 7889, loss = 1.15173875\n",
      "Iteration 7890, loss = 1.15173314\n",
      "Iteration 7891, loss = 1.15172641\n",
      "Iteration 7892, loss = 1.15172336\n",
      "Iteration 7893, loss = 1.15172161\n",
      "Iteration 7894, loss = 1.15171250\n",
      "Iteration 7895, loss = 1.15170084\n",
      "Iteration 7896, loss = 1.15169321\n",
      "Iteration 7897, loss = 1.15169486\n",
      "Iteration 7898, loss = 1.15168064\n",
      "Iteration 7899, loss = 1.15167662\n",
      "Iteration 7900, loss = 1.15167069\n",
      "Iteration 7901, loss = 1.15166219\n",
      "Iteration 7902, loss = 1.15165992\n",
      "Iteration 7903, loss = 1.15165107\n",
      "Iteration 7904, loss = 1.15164246\n",
      "Iteration 7905, loss = 1.15164096\n",
      "Iteration 7906, loss = 1.15163406\n",
      "Iteration 7907, loss = 1.15162727\n",
      "Iteration 7908, loss = 1.15161525\n",
      "Iteration 7909, loss = 1.15162161\n",
      "Iteration 7910, loss = 1.15160368\n",
      "Iteration 7911, loss = 1.15159497\n",
      "Iteration 7912, loss = 1.15158948\n",
      "Iteration 7913, loss = 1.15158619\n",
      "Iteration 7914, loss = 1.15158280\n",
      "Iteration 7915, loss = 1.15156975\n",
      "Iteration 7916, loss = 1.15156276\n",
      "Iteration 7917, loss = 1.15155440\n",
      "Iteration 7918, loss = 1.15154835\n",
      "Iteration 7919, loss = 1.15154682\n",
      "Iteration 7920, loss = 1.15153716\n",
      "Iteration 7921, loss = 1.15153249\n",
      "Iteration 7922, loss = 1.15152349\n",
      "Iteration 7923, loss = 1.15152057\n",
      "Iteration 7924, loss = 1.15151449\n",
      "Iteration 7925, loss = 1.15150479\n",
      "Iteration 7926, loss = 1.15151878\n",
      "Iteration 7927, loss = 1.15148940\n",
      "Iteration 7928, loss = 1.15148516\n",
      "Iteration 7929, loss = 1.15147911\n",
      "Iteration 7930, loss = 1.15148641\n",
      "Iteration 7931, loss = 1.15146613\n",
      "Iteration 7932, loss = 1.15147459\n",
      "Iteration 7933, loss = 1.15145305\n",
      "Iteration 7934, loss = 1.15145575\n",
      "Iteration 7935, loss = 1.15143939\n",
      "Iteration 7936, loss = 1.15143865\n",
      "Iteration 7937, loss = 1.15142847\n",
      "Iteration 7938, loss = 1.15142520\n",
      "Iteration 7939, loss = 1.15141313\n",
      "Iteration 7940, loss = 1.15140518\n",
      "Iteration 7941, loss = 1.15140046\n",
      "Iteration 7942, loss = 1.15139749\n",
      "Iteration 7943, loss = 1.15138481\n",
      "Iteration 7944, loss = 1.15138038\n",
      "Iteration 7945, loss = 1.15137617\n",
      "Iteration 7946, loss = 1.15136849\n",
      "Iteration 7947, loss = 1.15136025\n",
      "Iteration 7948, loss = 1.15135612\n",
      "Iteration 7949, loss = 1.15135800\n",
      "Iteration 7950, loss = 1.15134074\n",
      "Iteration 7951, loss = 1.15134248\n",
      "Iteration 7952, loss = 1.15133723\n",
      "Iteration 7953, loss = 1.15132081\n",
      "Iteration 7954, loss = 1.15131912\n",
      "Iteration 7955, loss = 1.15131575\n",
      "Iteration 7956, loss = 1.15130237\n",
      "Iteration 7957, loss = 1.15129662\n",
      "Iteration 7958, loss = 1.15129895\n",
      "Iteration 7959, loss = 1.15128810\n",
      "Iteration 7960, loss = 1.15129186\n",
      "Iteration 7961, loss = 1.15126889\n",
      "Iteration 7962, loss = 1.15127832\n",
      "Iteration 7963, loss = 1.15126001\n",
      "Iteration 7964, loss = 1.15129397\n",
      "Iteration 7965, loss = 1.15124508\n",
      "Iteration 7966, loss = 1.15123705\n",
      "Iteration 7967, loss = 1.15123004\n",
      "Iteration 7968, loss = 1.15122540\n",
      "Iteration 7969, loss = 1.15122370\n",
      "Iteration 7970, loss = 1.15121419\n",
      "Iteration 7971, loss = 1.15120647\n",
      "Iteration 7972, loss = 1.15120118\n",
      "Iteration 7973, loss = 1.15119871\n",
      "Iteration 7974, loss = 1.15118379\n",
      "Iteration 7975, loss = 1.15118171\n",
      "Iteration 7976, loss = 1.15117650\n",
      "Iteration 7977, loss = 1.15116368\n",
      "Iteration 7978, loss = 1.15116376\n",
      "Iteration 7979, loss = 1.15115144\n",
      "Iteration 7980, loss = 1.15114579\n",
      "Iteration 7981, loss = 1.15114311\n",
      "Iteration 7982, loss = 1.15113791\n",
      "Iteration 7983, loss = 1.15112701\n",
      "Iteration 7984, loss = 1.15112385\n",
      "Iteration 7985, loss = 1.15111243\n",
      "Iteration 7986, loss = 1.15110474\n",
      "Iteration 7987, loss = 1.15110917\n",
      "Iteration 7988, loss = 1.15110286\n",
      "Iteration 7989, loss = 1.15108689\n",
      "Iteration 7990, loss = 1.15107965\n",
      "Iteration 7991, loss = 1.15107572\n",
      "Iteration 7992, loss = 1.15107777\n",
      "Iteration 7993, loss = 1.15105941\n",
      "Iteration 7994, loss = 1.15105784\n",
      "Iteration 7995, loss = 1.15105083\n",
      "Iteration 7996, loss = 1.15104605\n",
      "Iteration 7997, loss = 1.15104184\n",
      "Iteration 7998, loss = 1.15103731\n",
      "Iteration 7999, loss = 1.15101867\n",
      "Iteration 8000, loss = 1.15101598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8001, loss = 1.15100851\n",
      "Iteration 8002, loss = 1.15100745\n",
      "Iteration 8003, loss = 1.15100519\n",
      "Iteration 8004, loss = 1.15099150\n",
      "Iteration 8005, loss = 1.15099336\n",
      "Iteration 8006, loss = 1.15097918\n",
      "Iteration 8007, loss = 1.15097208\n",
      "Iteration 8008, loss = 1.15096848\n",
      "Iteration 8009, loss = 1.15096257\n",
      "Iteration 8010, loss = 1.15094986\n",
      "Iteration 8011, loss = 1.15094219\n",
      "Iteration 8012, loss = 1.15094293\n",
      "Iteration 8013, loss = 1.15093401\n",
      "Iteration 8014, loss = 1.15092786\n",
      "Iteration 8015, loss = 1.15091706\n",
      "Iteration 8016, loss = 1.15091698\n",
      "Iteration 8017, loss = 1.15091217\n",
      "Iteration 8018, loss = 1.15089968\n",
      "Iteration 8019, loss = 1.15089136\n",
      "Iteration 8020, loss = 1.15088672\n",
      "Iteration 8021, loss = 1.15088030\n",
      "Iteration 8022, loss = 1.15087050\n",
      "Iteration 8023, loss = 1.15087122\n",
      "Iteration 8024, loss = 1.15086398\n",
      "Iteration 8025, loss = 1.15085399\n",
      "Iteration 8026, loss = 1.15084447\n",
      "Iteration 8027, loss = 1.15084697\n",
      "Iteration 8028, loss = 1.15083587\n",
      "Iteration 8029, loss = 1.15083125\n",
      "Iteration 8030, loss = 1.15081872\n",
      "Iteration 8031, loss = 1.15082003\n",
      "Iteration 8032, loss = 1.15080997\n",
      "Iteration 8033, loss = 1.15081957\n",
      "Iteration 8034, loss = 1.15079788\n",
      "Iteration 8035, loss = 1.15078587\n",
      "Iteration 8036, loss = 1.15079166\n",
      "Iteration 8037, loss = 1.15077402\n",
      "Iteration 8038, loss = 1.15077315\n",
      "Iteration 8039, loss = 1.15076968\n",
      "Iteration 8040, loss = 1.15075916\n",
      "Iteration 8041, loss = 1.15075575\n",
      "Iteration 8042, loss = 1.15074062\n",
      "Iteration 8043, loss = 1.15073862\n",
      "Iteration 8044, loss = 1.15073267\n",
      "Iteration 8045, loss = 1.15073389\n",
      "Iteration 8046, loss = 1.15071844\n",
      "Iteration 8047, loss = 1.15071462\n",
      "Iteration 8048, loss = 1.15070353\n",
      "Iteration 8049, loss = 1.15069593\n",
      "Iteration 8050, loss = 1.15069069\n",
      "Iteration 8051, loss = 1.15068594\n",
      "Iteration 8052, loss = 1.15067533\n",
      "Iteration 8053, loss = 1.15067135\n",
      "Iteration 8054, loss = 1.15067108\n",
      "Iteration 8055, loss = 1.15066167\n",
      "Iteration 8056, loss = 1.15065635\n",
      "Iteration 8057, loss = 1.15064529\n",
      "Iteration 8058, loss = 1.15063882\n",
      "Iteration 8059, loss = 1.15063266\n",
      "Iteration 8060, loss = 1.15062482\n",
      "Iteration 8061, loss = 1.15062604\n",
      "Iteration 8062, loss = 1.15061787\n",
      "Iteration 8063, loss = 1.15062404\n",
      "Iteration 8064, loss = 1.15060335\n",
      "Iteration 8065, loss = 1.15059561\n",
      "Iteration 8066, loss = 1.15059211\n",
      "Iteration 8067, loss = 1.15059361\n",
      "Iteration 8068, loss = 1.15057859\n",
      "Iteration 8069, loss = 1.15057030\n",
      "Iteration 8070, loss = 1.15056655\n",
      "Iteration 8071, loss = 1.15055325\n",
      "Iteration 8072, loss = 1.15055773\n",
      "Iteration 8073, loss = 1.15054936\n",
      "Iteration 8074, loss = 1.15053777\n",
      "Iteration 8075, loss = 1.15053161\n",
      "Iteration 8076, loss = 1.15052331\n",
      "Iteration 8077, loss = 1.15051605\n",
      "Iteration 8078, loss = 1.15052993\n",
      "Iteration 8079, loss = 1.15050963\n",
      "Iteration 8080, loss = 1.15049932\n",
      "Iteration 8081, loss = 1.15049601\n",
      "Iteration 8082, loss = 1.15048455\n",
      "Iteration 8083, loss = 1.15048259\n",
      "Iteration 8084, loss = 1.15047211\n",
      "Iteration 8085, loss = 1.15046676\n",
      "Iteration 8086, loss = 1.15046090\n",
      "Iteration 8087, loss = 1.15045232\n",
      "Iteration 8088, loss = 1.15044759\n",
      "Iteration 8089, loss = 1.15045713\n",
      "Iteration 8090, loss = 1.15043726\n",
      "Iteration 8091, loss = 1.15042591\n",
      "Iteration 8092, loss = 1.15042217\n",
      "Iteration 8093, loss = 1.15041183\n",
      "Iteration 8094, loss = 1.15041292\n",
      "Iteration 8095, loss = 1.15039938\n",
      "Iteration 8096, loss = 1.15039393\n",
      "Iteration 8097, loss = 1.15038737\n",
      "Iteration 8098, loss = 1.15038307\n",
      "Iteration 8099, loss = 1.15038568\n",
      "Iteration 8100, loss = 1.15036875\n",
      "Iteration 8101, loss = 1.15035965\n",
      "Iteration 8102, loss = 1.15035527\n",
      "Iteration 8103, loss = 1.15034931\n",
      "Iteration 8104, loss = 1.15034908\n",
      "Iteration 8105, loss = 1.15033498\n",
      "Iteration 8106, loss = 1.15033474\n",
      "Iteration 8107, loss = 1.15032604\n",
      "Iteration 8108, loss = 1.15032526\n",
      "Iteration 8109, loss = 1.15031163\n",
      "Iteration 8110, loss = 1.15031176\n",
      "Iteration 8111, loss = 1.15030829\n",
      "Iteration 8112, loss = 1.15030391\n",
      "Iteration 8113, loss = 1.15029036\n",
      "Iteration 8114, loss = 1.15027722\n",
      "Iteration 8115, loss = 1.15028192\n",
      "Iteration 8116, loss = 1.15028493\n",
      "Iteration 8117, loss = 1.15025992\n",
      "Iteration 8118, loss = 1.15026489\n",
      "Iteration 8119, loss = 1.15025239\n",
      "Iteration 8120, loss = 1.15024438\n",
      "Iteration 8121, loss = 1.15024124\n",
      "Iteration 8122, loss = 1.15022758\n",
      "Iteration 8123, loss = 1.15022110\n",
      "Iteration 8124, loss = 1.15022051\n",
      "Iteration 8125, loss = 1.15020752\n",
      "Iteration 8126, loss = 1.15020540\n",
      "Iteration 8127, loss = 1.15019804\n",
      "Iteration 8128, loss = 1.15019097\n",
      "Iteration 8129, loss = 1.15018979\n",
      "Iteration 8130, loss = 1.15017443\n",
      "Iteration 8131, loss = 1.15017369\n",
      "Iteration 8132, loss = 1.15017103\n",
      "Iteration 8133, loss = 1.15018202\n",
      "Iteration 8134, loss = 1.15014981\n",
      "Iteration 8135, loss = 1.15014949\n",
      "Iteration 8136, loss = 1.15014353\n",
      "Iteration 8137, loss = 1.15014188\n",
      "Iteration 8138, loss = 1.15012468\n",
      "Iteration 8139, loss = 1.15012006\n",
      "Iteration 8140, loss = 1.15011332\n",
      "Iteration 8141, loss = 1.15010604\n",
      "Iteration 8142, loss = 1.15011176\n",
      "Iteration 8143, loss = 1.15009117\n",
      "Iteration 8144, loss = 1.15008516\n",
      "Iteration 8145, loss = 1.15008367\n",
      "Iteration 8146, loss = 1.15007500\n",
      "Iteration 8147, loss = 1.15006522\n",
      "Iteration 8148, loss = 1.15006862\n",
      "Iteration 8149, loss = 1.15005310\n",
      "Iteration 8150, loss = 1.15004612\n",
      "Iteration 8151, loss = 1.15004109\n",
      "Iteration 8152, loss = 1.15003440\n",
      "Iteration 8153, loss = 1.15003071\n",
      "Iteration 8154, loss = 1.15001986\n",
      "Iteration 8155, loss = 1.15001972\n",
      "Iteration 8156, loss = 1.15001362\n",
      "Iteration 8157, loss = 1.15001912\n",
      "Iteration 8158, loss = 1.14999805\n",
      "Iteration 8159, loss = 1.14999253\n",
      "Iteration 8160, loss = 1.14998706\n",
      "Iteration 8161, loss = 1.14998621\n",
      "Iteration 8162, loss = 1.14998933\n",
      "Iteration 8163, loss = 1.14996740\n",
      "Iteration 8164, loss = 1.14996739\n",
      "Iteration 8165, loss = 1.14996091\n",
      "Iteration 8166, loss = 1.14994855\n",
      "Iteration 8167, loss = 1.14994051\n",
      "Iteration 8168, loss = 1.14994078\n",
      "Iteration 8169, loss = 1.14992359\n",
      "Iteration 8170, loss = 1.14992097\n",
      "Iteration 8171, loss = 1.14991806\n",
      "Iteration 8172, loss = 1.14990937\n",
      "Iteration 8173, loss = 1.14989743\n",
      "Iteration 8174, loss = 1.14989090\n",
      "Iteration 8175, loss = 1.14988726\n",
      "Iteration 8176, loss = 1.14987920\n",
      "Iteration 8177, loss = 1.14987306\n",
      "Iteration 8178, loss = 1.14987029\n",
      "Iteration 8179, loss = 1.14986987\n",
      "Iteration 8180, loss = 1.14986815\n",
      "Iteration 8181, loss = 1.14984929\n",
      "Iteration 8182, loss = 1.14985086\n",
      "Iteration 8183, loss = 1.14985130\n",
      "Iteration 8184, loss = 1.14983497\n",
      "Iteration 8185, loss = 1.14985556\n",
      "Iteration 8186, loss = 1.14982879\n",
      "Iteration 8187, loss = 1.14981163\n",
      "Iteration 8188, loss = 1.14980436\n",
      "Iteration 8189, loss = 1.14979466\n",
      "Iteration 8190, loss = 1.14979526\n",
      "Iteration 8191, loss = 1.14979191\n",
      "Iteration 8192, loss = 1.14978384\n",
      "Iteration 8193, loss = 1.14977268\n",
      "Iteration 8194, loss = 1.14976882\n",
      "Iteration 8195, loss = 1.14976626\n",
      "Iteration 8196, loss = 1.14975344\n",
      "Iteration 8197, loss = 1.14975275\n",
      "Iteration 8198, loss = 1.14974983\n",
      "Iteration 8199, loss = 1.14973066\n",
      "Iteration 8200, loss = 1.14972880\n",
      "Iteration 8201, loss = 1.14971930\n",
      "Iteration 8202, loss = 1.14971218\n",
      "Iteration 8203, loss = 1.14970713\n",
      "Iteration 8204, loss = 1.14969832\n",
      "Iteration 8205, loss = 1.14969293\n",
      "Iteration 8206, loss = 1.14969838\n",
      "Iteration 8207, loss = 1.14968190\n",
      "Iteration 8208, loss = 1.14967713\n",
      "Iteration 8209, loss = 1.14966925\n",
      "Iteration 8210, loss = 1.14966116\n",
      "Iteration 8211, loss = 1.14965507\n",
      "Iteration 8212, loss = 1.14965132\n",
      "Iteration 8213, loss = 1.14964959\n",
      "Iteration 8214, loss = 1.14964625\n",
      "Iteration 8215, loss = 1.14963671\n",
      "Iteration 8216, loss = 1.14963423\n",
      "Iteration 8217, loss = 1.14961942\n",
      "Iteration 8218, loss = 1.14961522\n",
      "Iteration 8219, loss = 1.14960846\n",
      "Iteration 8220, loss = 1.14960387\n",
      "Iteration 8221, loss = 1.14959780\n",
      "Iteration 8222, loss = 1.14959086\n",
      "Iteration 8223, loss = 1.14957913\n",
      "Iteration 8224, loss = 1.14958213\n",
      "Iteration 8225, loss = 1.14956767\n",
      "Iteration 8226, loss = 1.14956453\n",
      "Iteration 8227, loss = 1.14955639\n",
      "Iteration 8228, loss = 1.14956014\n",
      "Iteration 8229, loss = 1.14954596\n",
      "Iteration 8230, loss = 1.14953958\n",
      "Iteration 8231, loss = 1.14952783\n",
      "Iteration 8232, loss = 1.14952359\n",
      "Iteration 8233, loss = 1.14951417\n",
      "Iteration 8234, loss = 1.14951065\n",
      "Iteration 8235, loss = 1.14950573\n",
      "Iteration 8236, loss = 1.14950471\n",
      "Iteration 8237, loss = 1.14948995\n",
      "Iteration 8238, loss = 1.14950124\n",
      "Iteration 8239, loss = 1.14948486\n",
      "Iteration 8240, loss = 1.14947541\n",
      "Iteration 8241, loss = 1.14946339\n",
      "Iteration 8242, loss = 1.14945929\n",
      "Iteration 8243, loss = 1.14945612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8244, loss = 1.14944394\n",
      "Iteration 8245, loss = 1.14944081\n",
      "Iteration 8246, loss = 1.14943070\n",
      "Iteration 8247, loss = 1.14942538\n",
      "Iteration 8248, loss = 1.14941937\n",
      "Iteration 8249, loss = 1.14941386\n",
      "Iteration 8250, loss = 1.14940591\n",
      "Iteration 8251, loss = 1.14940670\n",
      "Iteration 8252, loss = 1.14939824\n",
      "Iteration 8253, loss = 1.14939696\n",
      "Iteration 8254, loss = 1.14938718\n",
      "Iteration 8255, loss = 1.14937950\n",
      "Iteration 8256, loss = 1.14936920\n",
      "Iteration 8257, loss = 1.14935977\n",
      "Iteration 8258, loss = 1.14935419\n",
      "Iteration 8259, loss = 1.14935337\n",
      "Iteration 8260, loss = 1.14934712\n",
      "Iteration 8261, loss = 1.14933709\n",
      "Iteration 8262, loss = 1.14933029\n",
      "Iteration 8263, loss = 1.14932925\n",
      "Iteration 8264, loss = 1.14931615\n",
      "Iteration 8265, loss = 1.14931032\n",
      "Iteration 8266, loss = 1.14930765\n",
      "Iteration 8267, loss = 1.14930535\n",
      "Iteration 8268, loss = 1.14929392\n",
      "Iteration 8269, loss = 1.14928997\n",
      "Iteration 8270, loss = 1.14929275\n",
      "Iteration 8271, loss = 1.14927343\n",
      "Iteration 8272, loss = 1.14927468\n",
      "Iteration 8273, loss = 1.14926680\n",
      "Iteration 8274, loss = 1.14925885\n",
      "Iteration 8275, loss = 1.14924749\n",
      "Iteration 8276, loss = 1.14924397\n",
      "Iteration 8277, loss = 1.14923699\n",
      "Iteration 8278, loss = 1.14923022\n",
      "Iteration 8279, loss = 1.14922379\n",
      "Iteration 8280, loss = 1.14921735\n",
      "Iteration 8281, loss = 1.14921803\n",
      "Iteration 8282, loss = 1.14921439\n",
      "Iteration 8283, loss = 1.14920086\n",
      "Iteration 8284, loss = 1.14919634\n",
      "Iteration 8285, loss = 1.14919174\n",
      "Iteration 8286, loss = 1.14918163\n",
      "Iteration 8287, loss = 1.14917170\n",
      "Iteration 8288, loss = 1.14916569\n",
      "Iteration 8289, loss = 1.14917883\n",
      "Iteration 8290, loss = 1.14914926\n",
      "Iteration 8291, loss = 1.14914805\n",
      "Iteration 8292, loss = 1.14914094\n",
      "Iteration 8293, loss = 1.14914086\n",
      "Iteration 8294, loss = 1.14913239\n",
      "Iteration 8295, loss = 1.14911909\n",
      "Iteration 8296, loss = 1.14911659\n",
      "Iteration 8297, loss = 1.14910780\n",
      "Iteration 8298, loss = 1.14910070\n",
      "Iteration 8299, loss = 1.14909869\n",
      "Iteration 8300, loss = 1.14909547\n",
      "Iteration 8301, loss = 1.14908428\n",
      "Iteration 8302, loss = 1.14907539\n",
      "Iteration 8303, loss = 1.14908619\n",
      "Iteration 8304, loss = 1.14906785\n",
      "Iteration 8305, loss = 1.14906000\n",
      "Iteration 8306, loss = 1.14906805\n",
      "Iteration 8307, loss = 1.14904686\n",
      "Iteration 8308, loss = 1.14904155\n",
      "Iteration 8309, loss = 1.14903392\n",
      "Iteration 8310, loss = 1.14902506\n",
      "Iteration 8311, loss = 1.14902125\n",
      "Iteration 8312, loss = 1.14901869\n",
      "Iteration 8313, loss = 1.14901322\n",
      "Iteration 8314, loss = 1.14901208\n",
      "Iteration 8315, loss = 1.14899952\n",
      "Iteration 8316, loss = 1.14900073\n",
      "Iteration 8317, loss = 1.14898229\n",
      "Iteration 8318, loss = 1.14897399\n",
      "Iteration 8319, loss = 1.14897256\n",
      "Iteration 8320, loss = 1.14896403\n",
      "Iteration 8321, loss = 1.14895833\n",
      "Iteration 8322, loss = 1.14895093\n",
      "Iteration 8323, loss = 1.14894492\n",
      "Iteration 8324, loss = 1.14893658\n",
      "Iteration 8325, loss = 1.14893340\n",
      "Iteration 8326, loss = 1.14892220\n",
      "Iteration 8327, loss = 1.14891655\n",
      "Iteration 8328, loss = 1.14891211\n",
      "Iteration 8329, loss = 1.14890993\n",
      "Iteration 8330, loss = 1.14890781\n",
      "Iteration 8331, loss = 1.14889696\n",
      "Iteration 8332, loss = 1.14888750\n",
      "Iteration 8333, loss = 1.14888873\n",
      "Iteration 8334, loss = 1.14887526\n",
      "Iteration 8335, loss = 1.14886797\n",
      "Iteration 8336, loss = 1.14886103\n",
      "Iteration 8337, loss = 1.14885342\n",
      "Iteration 8338, loss = 1.14884719\n",
      "Iteration 8339, loss = 1.14885540\n",
      "Iteration 8340, loss = 1.14884041\n",
      "Iteration 8341, loss = 1.14883519\n",
      "Iteration 8342, loss = 1.14882603\n",
      "Iteration 8343, loss = 1.14881850\n",
      "Iteration 8344, loss = 1.14881040\n",
      "Iteration 8345, loss = 1.14880269\n",
      "Iteration 8346, loss = 1.14879759\n",
      "Iteration 8347, loss = 1.14880181\n",
      "Iteration 8348, loss = 1.14879530\n",
      "Iteration 8349, loss = 1.14879991\n",
      "Iteration 8350, loss = 1.14877511\n",
      "Iteration 8351, loss = 1.14876761\n",
      "Iteration 8352, loss = 1.14876394\n",
      "Iteration 8353, loss = 1.14878968\n",
      "Iteration 8354, loss = 1.14878452\n",
      "Iteration 8355, loss = 1.14874827\n",
      "Iteration 8356, loss = 1.14875725\n",
      "Iteration 8357, loss = 1.14872767\n",
      "Iteration 8358, loss = 1.14874791\n",
      "Iteration 8359, loss = 1.14871531\n",
      "Iteration 8360, loss = 1.14872214\n",
      "Iteration 8361, loss = 1.14871082\n",
      "Iteration 8362, loss = 1.14869864\n",
      "Iteration 8363, loss = 1.14869306\n",
      "Iteration 8364, loss = 1.14869134\n",
      "Iteration 8365, loss = 1.14868453\n",
      "Iteration 8366, loss = 1.14866920\n",
      "Iteration 8367, loss = 1.14867648\n",
      "Iteration 8368, loss = 1.14866236\n",
      "Iteration 8369, loss = 1.14865673\n",
      "Iteration 8370, loss = 1.14864942\n",
      "Iteration 8371, loss = 1.14865046\n",
      "Iteration 8372, loss = 1.14864254\n",
      "Iteration 8373, loss = 1.14863515\n",
      "Iteration 8374, loss = 1.14862740\n",
      "Iteration 8375, loss = 1.14861762\n",
      "Iteration 8376, loss = 1.14861084\n",
      "Iteration 8377, loss = 1.14860454\n",
      "Iteration 8378, loss = 1.14859552\n",
      "Iteration 8379, loss = 1.14859714\n",
      "Iteration 8380, loss = 1.14858588\n",
      "Iteration 8381, loss = 1.14858555\n",
      "Iteration 8382, loss = 1.14857178\n",
      "Iteration 8383, loss = 1.14858360\n",
      "Iteration 8384, loss = 1.14856053\n",
      "Iteration 8385, loss = 1.14855624\n",
      "Iteration 8386, loss = 1.14855220\n",
      "Iteration 8387, loss = 1.14853954\n",
      "Iteration 8388, loss = 1.14853578\n",
      "Iteration 8389, loss = 1.14852613\n",
      "Iteration 8390, loss = 1.14852032\n",
      "Iteration 8391, loss = 1.14852641\n",
      "Iteration 8392, loss = 1.14851416\n",
      "Iteration 8393, loss = 1.14851096\n",
      "Iteration 8394, loss = 1.14850883\n",
      "Iteration 8395, loss = 1.14848998\n",
      "Iteration 8396, loss = 1.14848861\n",
      "Iteration 8397, loss = 1.14849456\n",
      "Iteration 8398, loss = 1.14847973\n",
      "Iteration 8399, loss = 1.14847038\n",
      "Iteration 8400, loss = 1.14845723\n",
      "Iteration 8401, loss = 1.14846277\n",
      "Iteration 8402, loss = 1.14846326\n",
      "Iteration 8403, loss = 1.14844451\n",
      "Iteration 8404, loss = 1.14843371\n",
      "Iteration 8405, loss = 1.14843026\n",
      "Iteration 8406, loss = 1.14842048\n",
      "Iteration 8407, loss = 1.14841427\n",
      "Iteration 8408, loss = 1.14841208\n",
      "Iteration 8409, loss = 1.14840282\n",
      "Iteration 8410, loss = 1.14840366\n",
      "Iteration 8411, loss = 1.14839021\n",
      "Iteration 8412, loss = 1.14838197\n",
      "Iteration 8413, loss = 1.14837620\n",
      "Iteration 8414, loss = 1.14837281\n",
      "Iteration 8415, loss = 1.14836345\n",
      "Iteration 8416, loss = 1.14835858\n",
      "Iteration 8417, loss = 1.14834927\n",
      "Iteration 8418, loss = 1.14834582\n",
      "Iteration 8419, loss = 1.14834121\n",
      "Iteration 8420, loss = 1.14833852\n",
      "Iteration 8421, loss = 1.14832847\n",
      "Iteration 8422, loss = 1.14832437\n",
      "Iteration 8423, loss = 1.14831159\n",
      "Iteration 8424, loss = 1.14830507\n",
      "Iteration 8425, loss = 1.14830502\n",
      "Iteration 8426, loss = 1.14829370\n",
      "Iteration 8427, loss = 1.14829783\n",
      "Iteration 8428, loss = 1.14828271\n",
      "Iteration 8429, loss = 1.14827622\n",
      "Iteration 8430, loss = 1.14827884\n",
      "Iteration 8431, loss = 1.14826876\n",
      "Iteration 8432, loss = 1.14825857\n",
      "Iteration 8433, loss = 1.14826155\n",
      "Iteration 8434, loss = 1.14824724\n",
      "Iteration 8435, loss = 1.14824781\n",
      "Iteration 8436, loss = 1.14823593\n",
      "Iteration 8437, loss = 1.14823431\n",
      "Iteration 8438, loss = 1.14822355\n",
      "Iteration 8439, loss = 1.14822137\n",
      "Iteration 8440, loss = 1.14822066\n",
      "Iteration 8441, loss = 1.14820832\n",
      "Iteration 8442, loss = 1.14819762\n",
      "Iteration 8443, loss = 1.14818621\n",
      "Iteration 8444, loss = 1.14819020\n",
      "Iteration 8445, loss = 1.14821219\n",
      "Iteration 8446, loss = 1.14817053\n",
      "Iteration 8447, loss = 1.14817259\n",
      "Iteration 8448, loss = 1.14815730\n",
      "Iteration 8449, loss = 1.14814923\n",
      "Iteration 8450, loss = 1.14814532\n",
      "Iteration 8451, loss = 1.14815849\n",
      "Iteration 8452, loss = 1.14816932\n",
      "Iteration 8453, loss = 1.14812327\n",
      "Iteration 8454, loss = 1.14811737\n",
      "Iteration 8455, loss = 1.14811728\n",
      "Iteration 8456, loss = 1.14810633\n",
      "Iteration 8457, loss = 1.14809955\n",
      "Iteration 8458, loss = 1.14809456\n",
      "Iteration 8459, loss = 1.14808788\n",
      "Iteration 8460, loss = 1.14808305\n",
      "Iteration 8461, loss = 1.14807277\n",
      "Iteration 8462, loss = 1.14807423\n",
      "Iteration 8463, loss = 1.14806881\n",
      "Iteration 8464, loss = 1.14806064\n",
      "Iteration 8465, loss = 1.14805162\n",
      "Iteration 8466, loss = 1.14804266\n",
      "Iteration 8467, loss = 1.14804181\n",
      "Iteration 8468, loss = 1.14803391\n",
      "Iteration 8469, loss = 1.14802930\n",
      "Iteration 8470, loss = 1.14802433\n",
      "Iteration 8471, loss = 1.14801387\n",
      "Iteration 8472, loss = 1.14801097\n",
      "Iteration 8473, loss = 1.14800697\n",
      "Iteration 8474, loss = 1.14799887\n",
      "Iteration 8475, loss = 1.14799217\n",
      "Iteration 8476, loss = 1.14798031\n",
      "Iteration 8477, loss = 1.14797599\n",
      "Iteration 8478, loss = 1.14797226\n",
      "Iteration 8479, loss = 1.14797063\n",
      "Iteration 8480, loss = 1.14795836\n",
      "Iteration 8481, loss = 1.14795362\n",
      "Iteration 8482, loss = 1.14794605\n",
      "Iteration 8483, loss = 1.14796341\n",
      "Iteration 8484, loss = 1.14793088\n",
      "Iteration 8485, loss = 1.14792720\n",
      "Iteration 8486, loss = 1.14792667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8487, loss = 1.14791882\n",
      "Iteration 8488, loss = 1.14790564\n",
      "Iteration 8489, loss = 1.14790742\n",
      "Iteration 8490, loss = 1.14789510\n",
      "Iteration 8491, loss = 1.14788865\n",
      "Iteration 8492, loss = 1.14788532\n",
      "Iteration 8493, loss = 1.14787494\n",
      "Iteration 8494, loss = 1.14786718\n",
      "Iteration 8495, loss = 1.14786545\n",
      "Iteration 8496, loss = 1.14787177\n",
      "Iteration 8497, loss = 1.14785019\n",
      "Iteration 8498, loss = 1.14784939\n",
      "Iteration 8499, loss = 1.14785009\n",
      "Iteration 8500, loss = 1.14784000\n",
      "Iteration 8501, loss = 1.14782402\n",
      "Iteration 8502, loss = 1.14783821\n",
      "Iteration 8503, loss = 1.14781210\n",
      "Iteration 8504, loss = 1.14780915\n",
      "Iteration 8505, loss = 1.14779998\n",
      "Iteration 8506, loss = 1.14779666\n",
      "Iteration 8507, loss = 1.14778977\n",
      "Iteration 8508, loss = 1.14778473\n",
      "Iteration 8509, loss = 1.14777612\n",
      "Iteration 8510, loss = 1.14777112\n",
      "Iteration 8511, loss = 1.14776925\n",
      "Iteration 8512, loss = 1.14776693\n",
      "Iteration 8513, loss = 1.14774935\n",
      "Iteration 8514, loss = 1.14774401\n",
      "Iteration 8515, loss = 1.14774269\n",
      "Iteration 8516, loss = 1.14774504\n",
      "Iteration 8517, loss = 1.14772704\n",
      "Iteration 8518, loss = 1.14771811\n",
      "Iteration 8519, loss = 1.14772110\n",
      "Iteration 8520, loss = 1.14770850\n",
      "Iteration 8521, loss = 1.14770549\n",
      "Iteration 8522, loss = 1.14769376\n",
      "Iteration 8523, loss = 1.14771038\n",
      "Iteration 8524, loss = 1.14768061\n",
      "Iteration 8525, loss = 1.14767372\n",
      "Iteration 8526, loss = 1.14767707\n",
      "Iteration 8527, loss = 1.14766253\n",
      "Iteration 8528, loss = 1.14765490\n",
      "Iteration 8529, loss = 1.14765089\n",
      "Iteration 8530, loss = 1.14765145\n",
      "Iteration 8531, loss = 1.14764422\n",
      "Iteration 8532, loss = 1.14763319\n",
      "Iteration 8533, loss = 1.14764250\n",
      "Iteration 8534, loss = 1.14762011\n",
      "Iteration 8535, loss = 1.14761927\n",
      "Iteration 8536, loss = 1.14760561\n",
      "Iteration 8537, loss = 1.14759803\n",
      "Iteration 8538, loss = 1.14759303\n",
      "Iteration 8539, loss = 1.14758824\n",
      "Iteration 8540, loss = 1.14759342\n",
      "Iteration 8541, loss = 1.14758191\n",
      "Iteration 8542, loss = 1.14757499\n",
      "Iteration 8543, loss = 1.14756747\n",
      "Iteration 8544, loss = 1.14756358\n",
      "Iteration 8545, loss = 1.14755515\n",
      "Iteration 8546, loss = 1.14755278\n",
      "Iteration 8547, loss = 1.14753812\n",
      "Iteration 8548, loss = 1.14754354\n",
      "Iteration 8549, loss = 1.14752581\n",
      "Iteration 8550, loss = 1.14751844\n",
      "Iteration 8551, loss = 1.14753234\n",
      "Iteration 8552, loss = 1.14750531\n",
      "Iteration 8553, loss = 1.14750147\n",
      "Iteration 8554, loss = 1.14751122\n",
      "Iteration 8555, loss = 1.14750090\n",
      "Iteration 8556, loss = 1.14748570\n",
      "Iteration 8557, loss = 1.14747914\n",
      "Iteration 8558, loss = 1.14746929\n",
      "Iteration 8559, loss = 1.14746925\n",
      "Iteration 8560, loss = 1.14746064\n",
      "Iteration 8561, loss = 1.14744943\n",
      "Iteration 8562, loss = 1.14744314\n",
      "Iteration 8563, loss = 1.14744063\n",
      "Iteration 8564, loss = 1.14743478\n",
      "Iteration 8565, loss = 1.14742331\n",
      "Iteration 8566, loss = 1.14741909\n",
      "Iteration 8567, loss = 1.14741550\n",
      "Iteration 8568, loss = 1.14740820\n",
      "Iteration 8569, loss = 1.14740347\n",
      "Iteration 8570, loss = 1.14740447\n",
      "Iteration 8571, loss = 1.14739327\n",
      "Iteration 8572, loss = 1.14738424\n",
      "Iteration 8573, loss = 1.14737706\n",
      "Iteration 8574, loss = 1.14740054\n",
      "Iteration 8575, loss = 1.14736617\n",
      "Iteration 8576, loss = 1.14736434\n",
      "Iteration 8577, loss = 1.14735092\n",
      "Iteration 8578, loss = 1.14735700\n",
      "Iteration 8579, loss = 1.14734758\n",
      "Iteration 8580, loss = 1.14734490\n",
      "Iteration 8581, loss = 1.14732468\n",
      "Iteration 8582, loss = 1.14732022\n",
      "Iteration 8583, loss = 1.14732353\n",
      "Iteration 8584, loss = 1.14731615\n",
      "Iteration 8585, loss = 1.14730750\n",
      "Iteration 8586, loss = 1.14729563\n",
      "Iteration 8587, loss = 1.14729133\n",
      "Iteration 8588, loss = 1.14728084\n",
      "Iteration 8589, loss = 1.14727670\n",
      "Iteration 8590, loss = 1.14728665\n",
      "Iteration 8591, loss = 1.14727754\n",
      "Iteration 8592, loss = 1.14725901\n",
      "Iteration 8593, loss = 1.14725914\n",
      "Iteration 8594, loss = 1.14724641\n",
      "Iteration 8595, loss = 1.14724040\n",
      "Iteration 8596, loss = 1.14725568\n",
      "Iteration 8597, loss = 1.14722590\n",
      "Iteration 8598, loss = 1.14723019\n",
      "Iteration 8599, loss = 1.14721452\n",
      "Iteration 8600, loss = 1.14720910\n",
      "Iteration 8601, loss = 1.14720144\n",
      "Iteration 8602, loss = 1.14720632\n",
      "Iteration 8603, loss = 1.14721646\n",
      "Iteration 8604, loss = 1.14718340\n",
      "Iteration 8605, loss = 1.14718821\n",
      "Iteration 8606, loss = 1.14718286\n",
      "Iteration 8607, loss = 1.14717039\n",
      "Iteration 8608, loss = 1.14715756\n",
      "Iteration 8609, loss = 1.14715174\n",
      "Iteration 8610, loss = 1.14715061\n",
      "Iteration 8611, loss = 1.14716188\n",
      "Iteration 8612, loss = 1.14716008\n",
      "Iteration 8613, loss = 1.14713577\n",
      "Iteration 8614, loss = 1.14712012\n",
      "Iteration 8615, loss = 1.14711248\n",
      "Iteration 8616, loss = 1.14711055\n",
      "Iteration 8617, loss = 1.14710688\n",
      "Iteration 8618, loss = 1.14709632\n",
      "Iteration 8619, loss = 1.14709944\n",
      "Iteration 8620, loss = 1.14709063\n",
      "Iteration 8621, loss = 1.14708427\n",
      "Iteration 8622, loss = 1.14707720\n",
      "Iteration 8623, loss = 1.14706927\n",
      "Iteration 8624, loss = 1.14706183\n",
      "Iteration 8625, loss = 1.14705278\n",
      "Iteration 8626, loss = 1.14704526\n",
      "Iteration 8627, loss = 1.14703830\n",
      "Iteration 8628, loss = 1.14703307\n",
      "Iteration 8629, loss = 1.14702845\n",
      "Iteration 8630, loss = 1.14701986\n",
      "Iteration 8631, loss = 1.14701526\n",
      "Iteration 8632, loss = 1.14702563\n",
      "Iteration 8633, loss = 1.14700334\n",
      "Iteration 8634, loss = 1.14699846\n",
      "Iteration 8635, loss = 1.14702103\n",
      "Iteration 8636, loss = 1.14699435\n",
      "Iteration 8637, loss = 1.14698445\n",
      "Iteration 8638, loss = 1.14697663\n",
      "Iteration 8639, loss = 1.14697493\n",
      "Iteration 8640, loss = 1.14696254\n",
      "Iteration 8641, loss = 1.14695794\n",
      "Iteration 8642, loss = 1.14696270\n",
      "Iteration 8643, loss = 1.14694100\n",
      "Iteration 8644, loss = 1.14694001\n",
      "Iteration 8645, loss = 1.14692837\n",
      "Iteration 8646, loss = 1.14693876\n",
      "Iteration 8647, loss = 1.14691532\n",
      "Iteration 8648, loss = 1.14691725\n",
      "Iteration 8649, loss = 1.14691702\n",
      "Iteration 8650, loss = 1.14690910\n",
      "Iteration 8651, loss = 1.14689712\n",
      "Iteration 8652, loss = 1.14688458\n",
      "Iteration 8653, loss = 1.14689791\n",
      "Iteration 8654, loss = 1.14687613\n",
      "Iteration 8655, loss = 1.14686591\n",
      "Iteration 8656, loss = 1.14686437\n",
      "Iteration 8657, loss = 1.14686884\n",
      "Iteration 8658, loss = 1.14686573\n",
      "Iteration 8659, loss = 1.14684370\n",
      "Iteration 8660, loss = 1.14683738\n",
      "Iteration 8661, loss = 1.14683482\n",
      "Iteration 8662, loss = 1.14682576\n",
      "Iteration 8663, loss = 1.14681736\n",
      "Iteration 8664, loss = 1.14681715\n",
      "Iteration 8665, loss = 1.14681197\n",
      "Iteration 8666, loss = 1.14679759\n",
      "Iteration 8667, loss = 1.14682232\n",
      "Iteration 8668, loss = 1.14678509\n",
      "Iteration 8669, loss = 1.14678275\n",
      "Iteration 8670, loss = 1.14677421\n",
      "Iteration 8671, loss = 1.14676669\n",
      "Iteration 8672, loss = 1.14676342\n",
      "Iteration 8673, loss = 1.14675667\n",
      "Iteration 8674, loss = 1.14675344\n",
      "Iteration 8675, loss = 1.14674834\n",
      "Iteration 8676, loss = 1.14674052\n",
      "Iteration 8677, loss = 1.14673377\n",
      "Iteration 8678, loss = 1.14673014\n",
      "Iteration 8679, loss = 1.14671807\n",
      "Iteration 8680, loss = 1.14672606\n",
      "Iteration 8681, loss = 1.14670838\n",
      "Iteration 8682, loss = 1.14670250\n",
      "Iteration 8683, loss = 1.14669381\n",
      "Iteration 8684, loss = 1.14668504\n",
      "Iteration 8685, loss = 1.14668038\n",
      "Iteration 8686, loss = 1.14667893\n",
      "Iteration 8687, loss = 1.14667232\n",
      "Iteration 8688, loss = 1.14666574\n",
      "Iteration 8689, loss = 1.14665748\n",
      "Iteration 8690, loss = 1.14665452\n",
      "Iteration 8691, loss = 1.14665223\n",
      "Iteration 8692, loss = 1.14664637\n",
      "Iteration 8693, loss = 1.14663893\n",
      "Iteration 8694, loss = 1.14662556\n",
      "Iteration 8695, loss = 1.14663359\n",
      "Iteration 8696, loss = 1.14661249\n",
      "Iteration 8697, loss = 1.14660725\n",
      "Iteration 8698, loss = 1.14660684\n",
      "Iteration 8699, loss = 1.14660448\n",
      "Iteration 8700, loss = 1.14659375\n",
      "Iteration 8701, loss = 1.14658869\n",
      "Iteration 8702, loss = 1.14658599\n",
      "Iteration 8703, loss = 1.14657380\n",
      "Iteration 8704, loss = 1.14656500\n",
      "Iteration 8705, loss = 1.14656625\n",
      "Iteration 8706, loss = 1.14655119\n",
      "Iteration 8707, loss = 1.14655503\n",
      "Iteration 8708, loss = 1.14655083\n",
      "Iteration 8709, loss = 1.14654353\n",
      "Iteration 8710, loss = 1.14653484\n",
      "Iteration 8711, loss = 1.14652895\n",
      "Iteration 8712, loss = 1.14651586\n",
      "Iteration 8713, loss = 1.14650698\n",
      "Iteration 8714, loss = 1.14651200\n",
      "Iteration 8715, loss = 1.14649913\n",
      "Iteration 8716, loss = 1.14649584\n",
      "Iteration 8717, loss = 1.14648924\n",
      "Iteration 8718, loss = 1.14648651\n",
      "Iteration 8719, loss = 1.14648273\n",
      "Iteration 8720, loss = 1.14646605\n",
      "Iteration 8721, loss = 1.14646103\n",
      "Iteration 8722, loss = 1.14646385\n",
      "Iteration 8723, loss = 1.14645072\n",
      "Iteration 8724, loss = 1.14644767\n",
      "Iteration 8725, loss = 1.14644099\n",
      "Iteration 8726, loss = 1.14642995\n",
      "Iteration 8727, loss = 1.14643045\n",
      "Iteration 8728, loss = 1.14641547\n",
      "Iteration 8729, loss = 1.14641078\n",
      "Iteration 8730, loss = 1.14640469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8731, loss = 1.14639644\n",
      "Iteration 8732, loss = 1.14639338\n",
      "Iteration 8733, loss = 1.14639376\n",
      "Iteration 8734, loss = 1.14638428\n",
      "Iteration 8735, loss = 1.14638107\n",
      "Iteration 8736, loss = 1.14637708\n",
      "Iteration 8737, loss = 1.14635923\n",
      "Iteration 8738, loss = 1.14635319\n",
      "Iteration 8739, loss = 1.14635978\n",
      "Iteration 8740, loss = 1.14635522\n",
      "Iteration 8741, loss = 1.14633597\n",
      "Iteration 8742, loss = 1.14633244\n",
      "Iteration 8743, loss = 1.14633749\n",
      "Iteration 8744, loss = 1.14632074\n",
      "Iteration 8745, loss = 1.14631030\n",
      "Iteration 8746, loss = 1.14630952\n",
      "Iteration 8747, loss = 1.14630296\n",
      "Iteration 8748, loss = 1.14630702\n",
      "Iteration 8749, loss = 1.14628850\n",
      "Iteration 8750, loss = 1.14629015\n",
      "Iteration 8751, loss = 1.14627687\n",
      "Iteration 8752, loss = 1.14627638\n",
      "Iteration 8753, loss = 1.14626567\n",
      "Iteration 8754, loss = 1.14625531\n",
      "Iteration 8755, loss = 1.14624890\n",
      "Iteration 8756, loss = 1.14625499\n",
      "Iteration 8757, loss = 1.14623708\n",
      "Iteration 8758, loss = 1.14622843\n",
      "Iteration 8759, loss = 1.14622944\n",
      "Iteration 8760, loss = 1.14621631\n",
      "Iteration 8761, loss = 1.14622725\n",
      "Iteration 8762, loss = 1.14620825\n",
      "Iteration 8763, loss = 1.14619736\n",
      "Iteration 8764, loss = 1.14619631\n",
      "Iteration 8765, loss = 1.14618883\n",
      "Iteration 8766, loss = 1.14619001\n",
      "Iteration 8767, loss = 1.14617431\n",
      "Iteration 8768, loss = 1.14616909\n",
      "Iteration 8769, loss = 1.14617830\n",
      "Iteration 8770, loss = 1.14616190\n",
      "Iteration 8771, loss = 1.14615557\n",
      "Iteration 8772, loss = 1.14614501\n",
      "Iteration 8773, loss = 1.14613821\n",
      "Iteration 8774, loss = 1.14615350\n",
      "Iteration 8775, loss = 1.14612856\n",
      "Iteration 8776, loss = 1.14612072\n",
      "Iteration 8777, loss = 1.14611862\n",
      "Iteration 8778, loss = 1.14613781\n",
      "Iteration 8779, loss = 1.14610549\n",
      "Iteration 8780, loss = 1.14609931\n",
      "Iteration 8781, loss = 1.14609233\n",
      "Iteration 8782, loss = 1.14608782\n",
      "Iteration 8783, loss = 1.14607774\n",
      "Iteration 8784, loss = 1.14607259\n",
      "Iteration 8785, loss = 1.14606558\n",
      "Iteration 8786, loss = 1.14607018\n",
      "Iteration 8787, loss = 1.14605041\n",
      "Iteration 8788, loss = 1.14604803\n",
      "Iteration 8789, loss = 1.14605325\n",
      "Iteration 8790, loss = 1.14603218\n",
      "Iteration 8791, loss = 1.14602685\n",
      "Iteration 8792, loss = 1.14603938\n",
      "Iteration 8793, loss = 1.14601651\n",
      "Iteration 8794, loss = 1.14601577\n",
      "Iteration 8795, loss = 1.14600365\n",
      "Iteration 8796, loss = 1.14599828\n",
      "Iteration 8797, loss = 1.14599400\n",
      "Iteration 8798, loss = 1.14598444\n",
      "Iteration 8799, loss = 1.14599856\n",
      "Iteration 8800, loss = 1.14597773\n",
      "Iteration 8801, loss = 1.14596536\n",
      "Iteration 8802, loss = 1.14596681\n",
      "Iteration 8803, loss = 1.14595443\n",
      "Iteration 8804, loss = 1.14595798\n",
      "Iteration 8805, loss = 1.14594190\n",
      "Iteration 8806, loss = 1.14593649\n",
      "Iteration 8807, loss = 1.14592761\n",
      "Iteration 8808, loss = 1.14593154\n",
      "Iteration 8809, loss = 1.14592048\n",
      "Iteration 8810, loss = 1.14590978\n",
      "Iteration 8811, loss = 1.14590363\n",
      "Iteration 8812, loss = 1.14589581\n",
      "Iteration 8813, loss = 1.14589198\n",
      "Iteration 8814, loss = 1.14590558\n",
      "Iteration 8815, loss = 1.14588031\n",
      "Iteration 8816, loss = 1.14588229\n",
      "Iteration 8817, loss = 1.14587391\n",
      "Iteration 8818, loss = 1.14586447\n",
      "Iteration 8819, loss = 1.14585776\n",
      "Iteration 8820, loss = 1.14585648\n",
      "Iteration 8821, loss = 1.14584273\n",
      "Iteration 8822, loss = 1.14583807\n",
      "Iteration 8823, loss = 1.14583320\n",
      "Iteration 8824, loss = 1.14582560\n",
      "Iteration 8825, loss = 1.14582002\n",
      "Iteration 8826, loss = 1.14581574\n",
      "Iteration 8827, loss = 1.14581914\n",
      "Iteration 8828, loss = 1.14579881\n",
      "Iteration 8829, loss = 1.14580832\n",
      "Iteration 8830, loss = 1.14578418\n",
      "Iteration 8831, loss = 1.14578622\n",
      "Iteration 8832, loss = 1.14577924\n",
      "Iteration 8833, loss = 1.14577764\n",
      "Iteration 8834, loss = 1.14576515\n",
      "Iteration 8835, loss = 1.14578095\n",
      "Iteration 8836, loss = 1.14575250\n",
      "Iteration 8837, loss = 1.14575043\n",
      "Iteration 8838, loss = 1.14574713\n",
      "Iteration 8839, loss = 1.14573078\n",
      "Iteration 8840, loss = 1.14572897\n",
      "Iteration 8841, loss = 1.14572075\n",
      "Iteration 8842, loss = 1.14571245\n",
      "Iteration 8843, loss = 1.14570837\n",
      "Iteration 8844, loss = 1.14570207\n",
      "Iteration 8845, loss = 1.14570187\n",
      "Iteration 8846, loss = 1.14569700\n",
      "Iteration 8847, loss = 1.14568602\n",
      "Iteration 8848, loss = 1.14568122\n",
      "Iteration 8849, loss = 1.14568454\n",
      "Iteration 8850, loss = 1.14566279\n",
      "Iteration 8851, loss = 1.14566426\n",
      "Iteration 8852, loss = 1.14565377\n",
      "Iteration 8853, loss = 1.14564615\n",
      "Iteration 8854, loss = 1.14564517\n",
      "Iteration 8855, loss = 1.14563482\n",
      "Iteration 8856, loss = 1.14562745\n",
      "Iteration 8857, loss = 1.14562022\n",
      "Iteration 8858, loss = 1.14562513\n",
      "Iteration 8859, loss = 1.14561208\n",
      "Iteration 8860, loss = 1.14560706\n",
      "Iteration 8861, loss = 1.14559982\n",
      "Iteration 8862, loss = 1.14560272\n",
      "Iteration 8863, loss = 1.14559106\n",
      "Iteration 8864, loss = 1.14558119\n",
      "Iteration 8865, loss = 1.14558320\n",
      "Iteration 8866, loss = 1.14557796\n",
      "Iteration 8867, loss = 1.14556433\n",
      "Iteration 8868, loss = 1.14555695\n",
      "Iteration 8869, loss = 1.14555075\n",
      "Iteration 8870, loss = 1.14554094\n",
      "Iteration 8871, loss = 1.14553340\n",
      "Iteration 8872, loss = 1.14552601\n",
      "Iteration 8873, loss = 1.14552354\n",
      "Iteration 8874, loss = 1.14551488\n",
      "Iteration 8875, loss = 1.14551689\n",
      "Iteration 8876, loss = 1.14550459\n",
      "Iteration 8877, loss = 1.14550032\n",
      "Iteration 8878, loss = 1.14549787\n",
      "Iteration 8879, loss = 1.14549241\n",
      "Iteration 8880, loss = 1.14548347\n",
      "Iteration 8881, loss = 1.14547372\n",
      "Iteration 8882, loss = 1.14547834\n",
      "Iteration 8883, loss = 1.14546358\n",
      "Iteration 8884, loss = 1.14545933\n",
      "Iteration 8885, loss = 1.14545620\n",
      "Iteration 8886, loss = 1.14545848\n",
      "Iteration 8887, loss = 1.14546022\n",
      "Iteration 8888, loss = 1.14543064\n",
      "Iteration 8889, loss = 1.14544405\n",
      "Iteration 8890, loss = 1.14542906\n",
      "Iteration 8891, loss = 1.14541843\n",
      "Iteration 8892, loss = 1.14541322\n",
      "Iteration 8893, loss = 1.14539854\n",
      "Iteration 8894, loss = 1.14539214\n",
      "Iteration 8895, loss = 1.14539090\n",
      "Iteration 8896, loss = 1.14538182\n",
      "Iteration 8897, loss = 1.14538368\n",
      "Iteration 8898, loss = 1.14536928\n",
      "Iteration 8899, loss = 1.14537144\n",
      "Iteration 8900, loss = 1.14535549\n",
      "Iteration 8901, loss = 1.14535376\n",
      "Iteration 8902, loss = 1.14534761\n",
      "Iteration 8903, loss = 1.14533847\n",
      "Iteration 8904, loss = 1.14534373\n",
      "Iteration 8905, loss = 1.14532961\n",
      "Iteration 8906, loss = 1.14532317\n",
      "Iteration 8907, loss = 1.14531356\n",
      "Iteration 8908, loss = 1.14530725\n",
      "Iteration 8909, loss = 1.14529996\n",
      "Iteration 8910, loss = 1.14529920\n",
      "Iteration 8911, loss = 1.14530696\n",
      "Iteration 8912, loss = 1.14528582\n",
      "Iteration 8913, loss = 1.14528663\n",
      "Iteration 8914, loss = 1.14527576\n",
      "Iteration 8915, loss = 1.14527678\n",
      "Iteration 8916, loss = 1.14527169\n",
      "Iteration 8917, loss = 1.14525245\n",
      "Iteration 8918, loss = 1.14524928\n",
      "Iteration 8919, loss = 1.14524062\n",
      "Iteration 8920, loss = 1.14523271\n",
      "Iteration 8921, loss = 1.14523873\n",
      "Iteration 8922, loss = 1.14522425\n",
      "Iteration 8923, loss = 1.14521614\n",
      "Iteration 8924, loss = 1.14523501\n",
      "Iteration 8925, loss = 1.14520380\n",
      "Iteration 8926, loss = 1.14520789\n",
      "Iteration 8927, loss = 1.14519828\n",
      "Iteration 8928, loss = 1.14519965\n",
      "Iteration 8929, loss = 1.14517760\n",
      "Iteration 8930, loss = 1.14517091\n",
      "Iteration 8931, loss = 1.14516704\n",
      "Iteration 8932, loss = 1.14517634\n",
      "Iteration 8933, loss = 1.14516401\n",
      "Iteration 8934, loss = 1.14515584\n",
      "Iteration 8935, loss = 1.14514105\n",
      "Iteration 8936, loss = 1.14513577\n",
      "Iteration 8937, loss = 1.14513201\n",
      "Iteration 8938, loss = 1.14512280\n",
      "Iteration 8939, loss = 1.14511473\n",
      "Iteration 8940, loss = 1.14512494\n",
      "Iteration 8941, loss = 1.14510568\n",
      "Iteration 8942, loss = 1.14511180\n",
      "Iteration 8943, loss = 1.14510080\n",
      "Iteration 8944, loss = 1.14508810\n",
      "Iteration 8945, loss = 1.14508442\n",
      "Iteration 8946, loss = 1.14507450\n",
      "Iteration 8947, loss = 1.14508445\n",
      "Iteration 8948, loss = 1.14506740\n",
      "Iteration 8949, loss = 1.14506242\n",
      "Iteration 8950, loss = 1.14505543\n",
      "Iteration 8951, loss = 1.14504640\n",
      "Iteration 8952, loss = 1.14504312\n",
      "Iteration 8953, loss = 1.14503366\n",
      "Iteration 8954, loss = 1.14503566\n",
      "Iteration 8955, loss = 1.14502527\n",
      "Iteration 8956, loss = 1.14501168\n",
      "Iteration 8957, loss = 1.14501124\n",
      "Iteration 8958, loss = 1.14500799\n",
      "Iteration 8959, loss = 1.14499506\n",
      "Iteration 8960, loss = 1.14499881\n",
      "Iteration 8961, loss = 1.14498681\n",
      "Iteration 8962, loss = 1.14499789\n",
      "Iteration 8963, loss = 1.14497764\n",
      "Iteration 8964, loss = 1.14496814\n",
      "Iteration 8965, loss = 1.14497960\n",
      "Iteration 8966, loss = 1.14495299\n",
      "Iteration 8967, loss = 1.14495948\n",
      "Iteration 8968, loss = 1.14494752\n",
      "Iteration 8969, loss = 1.14494246\n",
      "Iteration 8970, loss = 1.14492649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8971, loss = 1.14493181\n",
      "Iteration 8972, loss = 1.14491674\n",
      "Iteration 8973, loss = 1.14493235\n",
      "Iteration 8974, loss = 1.14492037\n",
      "Iteration 8975, loss = 1.14491090\n",
      "Iteration 8976, loss = 1.14490304\n",
      "Iteration 8977, loss = 1.14488392\n",
      "Iteration 8978, loss = 1.14487829\n",
      "Iteration 8979, loss = 1.14487451\n",
      "Iteration 8980, loss = 1.14488737\n",
      "Iteration 8981, loss = 1.14487984\n",
      "Iteration 8982, loss = 1.14485457\n",
      "Iteration 8983, loss = 1.14485639\n",
      "Iteration 8984, loss = 1.14484948\n",
      "Iteration 8985, loss = 1.14483485\n",
      "Iteration 8986, loss = 1.14483915\n",
      "Iteration 8987, loss = 1.14482805\n",
      "Iteration 8988, loss = 1.14481880\n",
      "Iteration 8989, loss = 1.14481515\n",
      "Iteration 8990, loss = 1.14480470\n",
      "Iteration 8991, loss = 1.14481094\n",
      "Iteration 8992, loss = 1.14479392\n",
      "Iteration 8993, loss = 1.14479194\n",
      "Iteration 8994, loss = 1.14479374\n",
      "Iteration 8995, loss = 1.14478496\n",
      "Iteration 8996, loss = 1.14476710\n",
      "Iteration 8997, loss = 1.14476642\n",
      "Iteration 8998, loss = 1.14475813\n",
      "Iteration 8999, loss = 1.14476120\n",
      "Iteration 9000, loss = 1.14475597\n",
      "Iteration 9001, loss = 1.14474488\n",
      "Iteration 9002, loss = 1.14473095\n",
      "Iteration 9003, loss = 1.14472638\n",
      "Iteration 9004, loss = 1.14472135\n",
      "Iteration 9005, loss = 1.14471756\n",
      "Iteration 9006, loss = 1.14470668\n",
      "Iteration 9007, loss = 1.14470592\n",
      "Iteration 9008, loss = 1.14470013\n",
      "Iteration 9009, loss = 1.14468705\n",
      "Iteration 9010, loss = 1.14468649\n",
      "Iteration 9011, loss = 1.14467518\n",
      "Iteration 9012, loss = 1.14467159\n",
      "Iteration 9013, loss = 1.14466971\n",
      "Iteration 9014, loss = 1.14466633\n",
      "Iteration 9015, loss = 1.14465017\n",
      "Iteration 9016, loss = 1.14465500\n",
      "Iteration 9017, loss = 1.14464310\n",
      "Iteration 9018, loss = 1.14463333\n",
      "Iteration 9019, loss = 1.14463042\n",
      "Iteration 9020, loss = 1.14462327\n",
      "Iteration 9021, loss = 1.14461419\n",
      "Iteration 9022, loss = 1.14460797\n",
      "Iteration 9023, loss = 1.14460323\n",
      "Iteration 9024, loss = 1.14460136\n",
      "Iteration 9025, loss = 1.14459542\n",
      "Iteration 9026, loss = 1.14458657\n",
      "Iteration 9027, loss = 1.14457550\n",
      "Iteration 9028, loss = 1.14457555\n",
      "Iteration 9029, loss = 1.14456581\n",
      "Iteration 9030, loss = 1.14455841\n",
      "Iteration 9031, loss = 1.14455715\n",
      "Iteration 9032, loss = 1.14454758\n",
      "Iteration 9033, loss = 1.14454095\n",
      "Iteration 9034, loss = 1.14453994\n",
      "Iteration 9035, loss = 1.14453315\n",
      "Iteration 9036, loss = 1.14452461\n",
      "Iteration 9037, loss = 1.14452007\n",
      "Iteration 9038, loss = 1.14451670\n",
      "Iteration 9039, loss = 1.14451418\n",
      "Iteration 9040, loss = 1.14450429\n",
      "Iteration 9041, loss = 1.14450272\n",
      "Iteration 9042, loss = 1.14448814\n",
      "Iteration 9043, loss = 1.14448117\n",
      "Iteration 9044, loss = 1.14447953\n",
      "Iteration 9045, loss = 1.14447408\n",
      "Iteration 9046, loss = 1.14446271\n",
      "Iteration 9047, loss = 1.14446072\n",
      "Iteration 9048, loss = 1.14446975\n",
      "Iteration 9049, loss = 1.14444465\n",
      "Iteration 9050, loss = 1.14443951\n",
      "Iteration 9051, loss = 1.14443193\n",
      "Iteration 9052, loss = 1.14443738\n",
      "Iteration 9053, loss = 1.14443127\n",
      "Iteration 9054, loss = 1.14441475\n",
      "Iteration 9055, loss = 1.14441068\n",
      "Iteration 9056, loss = 1.14441121\n",
      "Iteration 9057, loss = 1.14439523\n",
      "Iteration 9058, loss = 1.14440532\n",
      "Iteration 9059, loss = 1.14441643\n",
      "Iteration 9060, loss = 1.14437368\n",
      "Iteration 9061, loss = 1.14438053\n",
      "Iteration 9062, loss = 1.14438828\n",
      "Iteration 9063, loss = 1.14436643\n",
      "Iteration 9064, loss = 1.14435315\n",
      "Iteration 9065, loss = 1.14434948\n",
      "Iteration 9066, loss = 1.14435864\n",
      "Iteration 9067, loss = 1.14433525\n",
      "Iteration 9068, loss = 1.14432793\n",
      "Iteration 9069, loss = 1.14434934\n",
      "Iteration 9070, loss = 1.14432439\n",
      "Iteration 9071, loss = 1.14431223\n",
      "Iteration 9072, loss = 1.14431590\n",
      "Iteration 9073, loss = 1.14429690\n",
      "Iteration 9074, loss = 1.14429050\n",
      "Iteration 9075, loss = 1.14428778\n",
      "Iteration 9076, loss = 1.14428366\n",
      "Iteration 9077, loss = 1.14428024\n",
      "Iteration 9078, loss = 1.14426997\n",
      "Iteration 9079, loss = 1.14427530\n",
      "Iteration 9080, loss = 1.14425765\n",
      "Iteration 9081, loss = 1.14425341\n",
      "Iteration 9082, loss = 1.14426618\n",
      "Iteration 9083, loss = 1.14424716\n",
      "Iteration 9084, loss = 1.14423663\n",
      "Iteration 9085, loss = 1.14423030\n",
      "Iteration 9086, loss = 1.14422088\n",
      "Iteration 9087, loss = 1.14421092\n",
      "Iteration 9088, loss = 1.14420321\n",
      "Iteration 9089, loss = 1.14420671\n",
      "Iteration 9090, loss = 1.14419572\n",
      "Iteration 9091, loss = 1.14419146\n",
      "Iteration 9092, loss = 1.14418270\n",
      "Iteration 9093, loss = 1.14417334\n",
      "Iteration 9094, loss = 1.14416944\n",
      "Iteration 9095, loss = 1.14416357\n",
      "Iteration 9096, loss = 1.14415411\n",
      "Iteration 9097, loss = 1.14415023\n",
      "Iteration 9098, loss = 1.14414479\n",
      "Iteration 9099, loss = 1.14413416\n",
      "Iteration 9100, loss = 1.14414841\n",
      "Iteration 9101, loss = 1.14415225\n",
      "Iteration 9102, loss = 1.14413762\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 9103, loss = 1.14411423\n",
      "Iteration 9104, loss = 1.14412151\n",
      "Iteration 9105, loss = 1.14410705\n",
      "Iteration 9106, loss = 1.14410564\n",
      "Iteration 9107, loss = 1.14410471\n",
      "Iteration 9108, loss = 1.14410030\n",
      "Iteration 9109, loss = 1.14409556\n",
      "Iteration 9110, loss = 1.14409826\n",
      "Iteration 9111, loss = 1.14408931\n",
      "Iteration 9112, loss = 1.14408370\n",
      "Iteration 9113, loss = 1.14408303\n",
      "Iteration 9114, loss = 1.14407817\n",
      "Iteration 9115, loss = 1.14407785\n",
      "Iteration 9116, loss = 1.14408243\n",
      "Iteration 9117, loss = 1.14407608\n",
      "Iteration 9118, loss = 1.14407164\n",
      "Iteration 9119, loss = 1.14407224\n",
      "Iteration 9120, loss = 1.14407047\n",
      "Iteration 9121, loss = 1.14406905\n",
      "Iteration 9122, loss = 1.14406640\n",
      "Iteration 9123, loss = 1.14406607\n",
      "Iteration 9124, loss = 1.14406473\n",
      "Iteration 9125, loss = 1.14406281\n",
      "Iteration 9126, loss = 1.14406210\n",
      "Iteration 9127, loss = 1.14406116\n",
      "Iteration 9128, loss = 1.14405888\n",
      "Iteration 9129, loss = 1.14406038\n",
      "Iteration 9130, loss = 1.14405661\n",
      "Iteration 9131, loss = 1.14405790\n",
      "Iteration 9132, loss = 1.14405461\n",
      "Iteration 9133, loss = 1.14405313\n",
      "Iteration 9134, loss = 1.14405335\n",
      "Iteration 9135, loss = 1.14405169\n",
      "Iteration 9136, loss = 1.14405125\n",
      "Iteration 9137, loss = 1.14405019\n",
      "Iteration 9138, loss = 1.14404693\n",
      "Iteration 9139, loss = 1.14404492\n",
      "Iteration 9140, loss = 1.14404446\n",
      "Iteration 9141, loss = 1.14404339\n",
      "Iteration 9142, loss = 1.14404179\n",
      "Iteration 9143, loss = 1.14404373\n",
      "Iteration 9144, loss = 1.14404073\n",
      "Iteration 9145, loss = 1.14404277\n",
      "Iteration 9146, loss = 1.14403645\n",
      "Iteration 9147, loss = 1.14403544\n",
      "Iteration 9148, loss = 1.14403374\n",
      "Iteration 9149, loss = 1.14403359\n",
      "Iteration 9150, loss = 1.14403284\n",
      "Iteration 9151, loss = 1.14403794\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 9152, loss = 1.14402970\n",
      "Iteration 9153, loss = 1.14402834\n",
      "Iteration 9154, loss = 1.14402772\n",
      "Iteration 9155, loss = 1.14402848\n",
      "Iteration 9156, loss = 1.14402709\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 9157, loss = 1.14402610\n",
      "Iteration 9158, loss = 1.14402544\n",
      "Iteration 9159, loss = 1.14402500\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 9160, loss = 1.14402460\n",
      "Iteration 9161, loss = 1.14402429\n",
      "Iteration 9162, loss = 1.14402409\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 9163, loss = 1.14402388\n",
      "Iteration 9164, loss = 1.14402370\n",
      "Iteration 9165, loss = 1.14402355\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size=15000, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 10, 10), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=1e-06, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3624"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47168"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cnf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
