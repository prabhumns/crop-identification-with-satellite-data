{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import svm\n",
    "from statistics import mean\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as shf\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('E:/Time Series/pra.pkl', 'rb')\n",
    "data2 = pickle.load(file_object)\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOYABEENS 204639\n",
      "CORN 151407\n",
      "DEVELOPED 12214\n",
      "FOREST 14646\n",
      "WATER BODY 6501\n",
      "ALFALFA 781\n",
      "OATS 25\n"
     ]
    }
   ],
   "source": [
    "for key, value in data2.items():\n",
    "    print (key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {key:value[:6000] for key, value in data.items() if len(value) >= 6000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(45):\n",
    "    lin = [l[i] for value in data.values() for l in value]\n",
    "    mins = min(lin)\n",
    "    maxs = max(lin)\n",
    "    means = mean(lin)\n",
    "    for key, value in data.items():\n",
    "        for example in value:\n",
    "            example[i] = np.float64((example[i] - means)/(maxs-mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : SOYABEENS\n",
      "1 : CORN\n",
      "2 : DEVELOPED\n",
      "3 : FOREST\n",
      "4 : WATER BODY\n"
     ]
    }
   ],
   "source": [
    "training_examples = []\n",
    "training_labels = []\n",
    "test_examples = []\n",
    "test_labels = []\n",
    "cv_examples = []\n",
    "cv_labels = []\n",
    "i = 0;\n",
    "for key, value in data.items():\n",
    "    training_examples = training_examples + value[:5000]\n",
    "    cv_examples = cv_examples + value[5000:5500]\n",
    "    test_examples = test_examples + value[5500:]\n",
    "    training_labels = training_labels + [i for v in range(5000)]\n",
    "    cv_labels = cv_labels + [i for v in range(500)]\n",
    "    test_labels = test_labels + [i for v in range(500)]\n",
    "    print(i,':', key)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples, training_labels  = shf(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=3, \n",
    "              kernel='rbf', \n",
    "              degree= 3,\n",
    "              gamma='auto',\n",
    "              coef0=0.0, \n",
    "              shrinking=True, \n",
    "              probability=True, \n",
    "              tol=0.0000001, \n",
    "              cache_size=200, \n",
    "              class_weight=None, \n",
    "              verbose=True, \n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=3, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=1e-07, verbose=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77356"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = MLPClassifier(hidden_layer_sizes=(30,10),\n",
    "                    activation='tanh', \n",
    "                    solver='sgd',\n",
    "                    alpha=0.0001, \n",
    "                    batch_size='auto', \n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=10000, \n",
    "                    shuffle=True, \n",
    "                    random_state=None, \n",
    "                    tol=0.00001, \n",
    "                    verbose=True, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False,\n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.58827652\n",
      "Iteration 2, loss = 1.47122507\n",
      "Iteration 3, loss = 1.41451627\n",
      "Iteration 4, loss = 1.37690440\n",
      "Iteration 5, loss = 1.35073457\n",
      "Iteration 6, loss = 1.33213269\n",
      "Iteration 7, loss = 1.31728763\n",
      "Iteration 8, loss = 1.30565731\n",
      "Iteration 9, loss = 1.29611248\n",
      "Iteration 10, loss = 1.28838506\n",
      "Iteration 11, loss = 1.28194127\n",
      "Iteration 12, loss = 1.27640179\n",
      "Iteration 13, loss = 1.27161419\n",
      "Iteration 14, loss = 1.26737024\n",
      "Iteration 15, loss = 1.26373373\n",
      "Iteration 16, loss = 1.26029774\n",
      "Iteration 17, loss = 1.25716726\n",
      "Iteration 18, loss = 1.25423389\n",
      "Iteration 19, loss = 1.25162079\n",
      "Iteration 20, loss = 1.24915416\n",
      "Iteration 21, loss = 1.24660401\n",
      "Iteration 22, loss = 1.24450996\n",
      "Iteration 23, loss = 1.24240085\n",
      "Iteration 24, loss = 1.24032678\n",
      "Iteration 25, loss = 1.23851923\n",
      "Iteration 26, loss = 1.23659956\n",
      "Iteration 27, loss = 1.23474500\n",
      "Iteration 28, loss = 1.23308837\n",
      "Iteration 29, loss = 1.23126806\n",
      "Iteration 30, loss = 1.22974763\n",
      "Iteration 31, loss = 1.22810576\n",
      "Iteration 32, loss = 1.22668199\n",
      "Iteration 33, loss = 1.22501841\n",
      "Iteration 34, loss = 1.22367711\n",
      "Iteration 35, loss = 1.22232912\n",
      "Iteration 36, loss = 1.22101780\n",
      "Iteration 37, loss = 1.21972786\n",
      "Iteration 38, loss = 1.21838629\n",
      "Iteration 39, loss = 1.21718136\n",
      "Iteration 40, loss = 1.21599130\n",
      "Iteration 41, loss = 1.21490184\n",
      "Iteration 42, loss = 1.21364100\n",
      "Iteration 43, loss = 1.21269963\n",
      "Iteration 44, loss = 1.21161056\n",
      "Iteration 45, loss = 1.21056517\n",
      "Iteration 46, loss = 1.20951517\n",
      "Iteration 47, loss = 1.20843014\n",
      "Iteration 48, loss = 1.20743496\n",
      "Iteration 49, loss = 1.20659868\n",
      "Iteration 50, loss = 1.20556784\n",
      "Iteration 51, loss = 1.20484782\n",
      "Iteration 52, loss = 1.20375795\n",
      "Iteration 53, loss = 1.20291965\n",
      "Iteration 54, loss = 1.20211069\n",
      "Iteration 55, loss = 1.20122411\n",
      "Iteration 56, loss = 1.20024369\n",
      "Iteration 57, loss = 1.19971091\n",
      "Iteration 58, loss = 1.19875979\n",
      "Iteration 59, loss = 1.19806388\n",
      "Iteration 60, loss = 1.19725587\n",
      "Iteration 61, loss = 1.19641012\n",
      "Iteration 62, loss = 1.19573913\n",
      "Iteration 63, loss = 1.19503583\n",
      "Iteration 64, loss = 1.19418199\n",
      "Iteration 65, loss = 1.19353243\n",
      "Iteration 66, loss = 1.19302634\n",
      "Iteration 67, loss = 1.19222920\n",
      "Iteration 68, loss = 1.19160390\n",
      "Iteration 69, loss = 1.19088140\n",
      "Iteration 70, loss = 1.19036191\n",
      "Iteration 71, loss = 1.18955643\n",
      "Iteration 72, loss = 1.18908917\n",
      "Iteration 73, loss = 1.18822595\n",
      "Iteration 74, loss = 1.18775397\n",
      "Iteration 75, loss = 1.18712205\n",
      "Iteration 76, loss = 1.18656346\n",
      "Iteration 77, loss = 1.18572309\n",
      "Iteration 78, loss = 1.18537103\n",
      "Iteration 79, loss = 1.18459895\n",
      "Iteration 80, loss = 1.18411911\n",
      "Iteration 81, loss = 1.18356540\n",
      "Iteration 82, loss = 1.18293326\n",
      "Iteration 83, loss = 1.18235476\n",
      "Iteration 84, loss = 1.18181691\n",
      "Iteration 85, loss = 1.18133314\n",
      "Iteration 86, loss = 1.18071260\n",
      "Iteration 87, loss = 1.18023387\n",
      "Iteration 88, loss = 1.17991035\n",
      "Iteration 89, loss = 1.17927534\n",
      "Iteration 90, loss = 1.17869242\n",
      "Iteration 91, loss = 1.17832838\n",
      "Iteration 92, loss = 1.17753783\n",
      "Iteration 93, loss = 1.17731221\n",
      "Iteration 94, loss = 1.17665430\n",
      "Iteration 95, loss = 1.17642917\n",
      "Iteration 96, loss = 1.17609863\n",
      "Iteration 97, loss = 1.17549728\n",
      "Iteration 98, loss = 1.17480915\n",
      "Iteration 99, loss = 1.17457471\n",
      "Iteration 100, loss = 1.17399835\n",
      "Iteration 101, loss = 1.17357742\n",
      "Iteration 102, loss = 1.17315478\n",
      "Iteration 103, loss = 1.17264125\n",
      "Iteration 104, loss = 1.17212012\n",
      "Iteration 105, loss = 1.17181377\n",
      "Iteration 106, loss = 1.17127227\n",
      "Iteration 107, loss = 1.17091898\n",
      "Iteration 108, loss = 1.17040539\n",
      "Iteration 109, loss = 1.16989024\n",
      "Iteration 110, loss = 1.16944120\n",
      "Iteration 111, loss = 1.16895506\n",
      "Iteration 112, loss = 1.16866544\n",
      "Iteration 113, loss = 1.16847817\n",
      "Iteration 114, loss = 1.16763159\n",
      "Iteration 115, loss = 1.16729411\n",
      "Iteration 116, loss = 1.16701975\n",
      "Iteration 117, loss = 1.16638028\n",
      "Iteration 118, loss = 1.16601712\n",
      "Iteration 119, loss = 1.16546947\n",
      "Iteration 120, loss = 1.16510108\n",
      "Iteration 121, loss = 1.16473980\n",
      "Iteration 122, loss = 1.16423284\n",
      "Iteration 123, loss = 1.16390207\n",
      "Iteration 124, loss = 1.16341864\n",
      "Iteration 125, loss = 1.16308664\n",
      "Iteration 126, loss = 1.16262079\n",
      "Iteration 127, loss = 1.16235309\n",
      "Iteration 128, loss = 1.16191673\n",
      "Iteration 129, loss = 1.16152819\n",
      "Iteration 130, loss = 1.16091688\n",
      "Iteration 131, loss = 1.16084274\n",
      "Iteration 132, loss = 1.16022435\n",
      "Iteration 133, loss = 1.15976650\n",
      "Iteration 134, loss = 1.15928569\n",
      "Iteration 135, loss = 1.15882728\n",
      "Iteration 136, loss = 1.15860148\n",
      "Iteration 137, loss = 1.15821752\n",
      "Iteration 138, loss = 1.15766689\n",
      "Iteration 139, loss = 1.15739695\n",
      "Iteration 140, loss = 1.15681231\n",
      "Iteration 141, loss = 1.15652195\n",
      "Iteration 142, loss = 1.15600697\n",
      "Iteration 143, loss = 1.15567669\n",
      "Iteration 144, loss = 1.15515613\n",
      "Iteration 145, loss = 1.15493299\n",
      "Iteration 146, loss = 1.15450769\n",
      "Iteration 147, loss = 1.15412746\n",
      "Iteration 148, loss = 1.15354964\n",
      "Iteration 149, loss = 1.15318927\n",
      "Iteration 150, loss = 1.15280705\n",
      "Iteration 151, loss = 1.15239581\n",
      "Iteration 152, loss = 1.15197850\n",
      "Iteration 153, loss = 1.15175057\n",
      "Iteration 154, loss = 1.15135944\n",
      "Iteration 155, loss = 1.15092100\n",
      "Iteration 156, loss = 1.15037885\n",
      "Iteration 157, loss = 1.15005485\n",
      "Iteration 158, loss = 1.14959602\n",
      "Iteration 159, loss = 1.14917685\n",
      "Iteration 160, loss = 1.14857195\n",
      "Iteration 161, loss = 1.14830055\n",
      "Iteration 162, loss = 1.14777991\n",
      "Iteration 163, loss = 1.14764691\n",
      "Iteration 164, loss = 1.14715572\n",
      "Iteration 165, loss = 1.14680215\n",
      "Iteration 166, loss = 1.14636936\n",
      "Iteration 167, loss = 1.14571341\n",
      "Iteration 168, loss = 1.14561428\n",
      "Iteration 169, loss = 1.14507882\n",
      "Iteration 170, loss = 1.14445250\n",
      "Iteration 171, loss = 1.14419509\n",
      "Iteration 172, loss = 1.14368101\n",
      "Iteration 173, loss = 1.14327551\n",
      "Iteration 174, loss = 1.14276299\n",
      "Iteration 175, loss = 1.14282079\n",
      "Iteration 176, loss = 1.14208534\n",
      "Iteration 177, loss = 1.14186000\n",
      "Iteration 178, loss = 1.14107670\n",
      "Iteration 179, loss = 1.14088354\n",
      "Iteration 180, loss = 1.14026255\n",
      "Iteration 181, loss = 1.13968262\n",
      "Iteration 182, loss = 1.13924843\n",
      "Iteration 183, loss = 1.13890189\n",
      "Iteration 184, loss = 1.13831943\n",
      "Iteration 185, loss = 1.13794748\n",
      "Iteration 186, loss = 1.13766806\n",
      "Iteration 187, loss = 1.13708211\n",
      "Iteration 188, loss = 1.13707738\n",
      "Iteration 189, loss = 1.13632089\n",
      "Iteration 190, loss = 1.13590490\n",
      "Iteration 191, loss = 1.13527379\n",
      "Iteration 192, loss = 1.13491183\n",
      "Iteration 193, loss = 1.13454226\n",
      "Iteration 194, loss = 1.13394465\n",
      "Iteration 195, loss = 1.13377483\n",
      "Iteration 196, loss = 1.13327578\n",
      "Iteration 197, loss = 1.13261325\n",
      "Iteration 198, loss = 1.13217295\n",
      "Iteration 199, loss = 1.13186222\n",
      "Iteration 200, loss = 1.13166593\n",
      "Iteration 201, loss = 1.13101707\n",
      "Iteration 202, loss = 1.13024698\n",
      "Iteration 203, loss = 1.13016956\n",
      "Iteration 204, loss = 1.12938993\n",
      "Iteration 205, loss = 1.12931622\n",
      "Iteration 206, loss = 1.12849647\n",
      "Iteration 207, loss = 1.12840345\n",
      "Iteration 208, loss = 1.12778932\n",
      "Iteration 209, loss = 1.12718621\n",
      "Iteration 210, loss = 1.12694527\n",
      "Iteration 211, loss = 1.12643338\n",
      "Iteration 212, loss = 1.12631549\n",
      "Iteration 213, loss = 1.12547184\n",
      "Iteration 214, loss = 1.12524171\n",
      "Iteration 215, loss = 1.12443928\n",
      "Iteration 216, loss = 1.12460701\n",
      "Iteration 217, loss = 1.12368274\n",
      "Iteration 218, loss = 1.12327421\n",
      "Iteration 219, loss = 1.12265123\n",
      "Iteration 220, loss = 1.12233190\n",
      "Iteration 221, loss = 1.12213182\n",
      "Iteration 222, loss = 1.12154156\n",
      "Iteration 223, loss = 1.12094920\n",
      "Iteration 224, loss = 1.12067025\n",
      "Iteration 225, loss = 1.11979683\n",
      "Iteration 226, loss = 1.11936104\n",
      "Iteration 227, loss = 1.11890915\n",
      "Iteration 228, loss = 1.11856436\n",
      "Iteration 229, loss = 1.11842353\n",
      "Iteration 230, loss = 1.11788449\n",
      "Iteration 231, loss = 1.11735928\n",
      "Iteration 232, loss = 1.11701509\n",
      "Iteration 233, loss = 1.11686218\n",
      "Iteration 234, loss = 1.11572128\n",
      "Iteration 235, loss = 1.11545619\n",
      "Iteration 236, loss = 1.11485856\n",
      "Iteration 237, loss = 1.11445897\n",
      "Iteration 238, loss = 1.11400666\n",
      "Iteration 239, loss = 1.11349839\n",
      "Iteration 240, loss = 1.11315174\n",
      "Iteration 241, loss = 1.11255458\n",
      "Iteration 242, loss = 1.11164844\n",
      "Iteration 243, loss = 1.11155632\n",
      "Iteration 244, loss = 1.11065412\n",
      "Iteration 245, loss = 1.11064140\n",
      "Iteration 246, loss = 1.11030264\n",
      "Iteration 247, loss = 1.10975796\n",
      "Iteration 248, loss = 1.10882853\n",
      "Iteration 249, loss = 1.10878413\n",
      "Iteration 250, loss = 1.10821799\n",
      "Iteration 251, loss = 1.10749861\n",
      "Iteration 252, loss = 1.10741465\n",
      "Iteration 253, loss = 1.10673658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 1.10646585\n",
      "Iteration 255, loss = 1.10555474\n",
      "Iteration 256, loss = 1.10528299\n",
      "Iteration 257, loss = 1.10474546\n",
      "Iteration 258, loss = 1.10428149\n",
      "Iteration 259, loss = 1.10417897\n",
      "Iteration 260, loss = 1.10305339\n",
      "Iteration 261, loss = 1.10286245\n",
      "Iteration 262, loss = 1.10251139\n",
      "Iteration 263, loss = 1.10174021\n",
      "Iteration 264, loss = 1.10133762\n",
      "Iteration 265, loss = 1.10031279\n",
      "Iteration 266, loss = 1.09995673\n",
      "Iteration 267, loss = 1.09973100\n",
      "Iteration 268, loss = 1.09893343\n",
      "Iteration 269, loss = 1.09853959\n",
      "Iteration 270, loss = 1.09842445\n",
      "Iteration 271, loss = 1.09723322\n",
      "Iteration 272, loss = 1.09724375\n",
      "Iteration 273, loss = 1.09641273\n",
      "Iteration 274, loss = 1.09662552\n",
      "Iteration 275, loss = 1.09541405\n",
      "Iteration 276, loss = 1.09529802\n",
      "Iteration 277, loss = 1.09393483\n",
      "Iteration 278, loss = 1.09324795\n",
      "Iteration 279, loss = 1.09256265\n",
      "Iteration 280, loss = 1.09259516\n",
      "Iteration 281, loss = 1.09147429\n",
      "Iteration 282, loss = 1.09096312\n",
      "Iteration 283, loss = 1.09076743\n",
      "Iteration 284, loss = 1.08961648\n",
      "Iteration 285, loss = 1.08969038\n",
      "Iteration 286, loss = 1.08905053\n",
      "Iteration 287, loss = 1.08798921\n",
      "Iteration 288, loss = 1.08693492\n",
      "Iteration 289, loss = 1.08718565\n",
      "Iteration 290, loss = 1.08626259\n",
      "Iteration 291, loss = 1.08573458\n",
      "Iteration 292, loss = 1.08528715\n",
      "Iteration 293, loss = 1.08457308\n",
      "Iteration 294, loss = 1.08375453\n",
      "Iteration 295, loss = 1.08280877\n",
      "Iteration 296, loss = 1.08227259\n",
      "Iteration 297, loss = 1.08229220\n",
      "Iteration 298, loss = 1.08097703\n",
      "Iteration 299, loss = 1.08061154\n",
      "Iteration 300, loss = 1.08064316\n",
      "Iteration 301, loss = 1.07912793\n",
      "Iteration 302, loss = 1.07796060\n",
      "Iteration 303, loss = 1.07751137\n",
      "Iteration 304, loss = 1.07700851\n",
      "Iteration 305, loss = 1.07578187\n",
      "Iteration 306, loss = 1.07543413\n",
      "Iteration 307, loss = 1.07490412\n",
      "Iteration 308, loss = 1.07402410\n",
      "Iteration 309, loss = 1.07345875\n",
      "Iteration 310, loss = 1.07266431\n",
      "Iteration 311, loss = 1.07204306\n",
      "Iteration 312, loss = 1.07195044\n",
      "Iteration 313, loss = 1.06985027\n",
      "Iteration 314, loss = 1.06955313\n",
      "Iteration 315, loss = 1.06894084\n",
      "Iteration 316, loss = 1.06795986\n",
      "Iteration 317, loss = 1.06642859\n",
      "Iteration 318, loss = 1.06652889\n",
      "Iteration 319, loss = 1.06532234\n",
      "Iteration 320, loss = 1.06493206\n",
      "Iteration 321, loss = 1.06417192\n",
      "Iteration 322, loss = 1.06337368\n",
      "Iteration 323, loss = 1.06266567\n",
      "Iteration 324, loss = 1.06102566\n",
      "Iteration 325, loss = 1.06108375\n",
      "Iteration 326, loss = 1.06024140\n",
      "Iteration 327, loss = 1.05772384\n",
      "Iteration 328, loss = 1.05934719\n",
      "Iteration 329, loss = 1.05666285\n",
      "Iteration 330, loss = 1.05642885\n",
      "Iteration 331, loss = 1.05501086\n",
      "Iteration 332, loss = 1.05406745\n",
      "Iteration 333, loss = 1.05373823\n",
      "Iteration 334, loss = 1.05316347\n",
      "Iteration 335, loss = 1.05232605\n",
      "Iteration 336, loss = 1.05079678\n",
      "Iteration 337, loss = 1.04994275\n",
      "Iteration 338, loss = 1.04895656\n",
      "Iteration 339, loss = 1.04815286\n",
      "Iteration 340, loss = 1.04581317\n",
      "Iteration 341, loss = 1.04649447\n",
      "Iteration 342, loss = 1.04560158\n",
      "Iteration 343, loss = 1.04562767\n",
      "Iteration 344, loss = 1.04342102\n",
      "Iteration 345, loss = 1.04172625\n",
      "Iteration 346, loss = 1.04173644\n",
      "Iteration 347, loss = 1.04305628\n",
      "Iteration 348, loss = 1.04168259\n",
      "Iteration 349, loss = 1.03966067\n",
      "Iteration 350, loss = 1.03849383\n",
      "Iteration 351, loss = 1.03781451\n",
      "Iteration 352, loss = 1.03659044\n",
      "Iteration 353, loss = 1.03656263\n",
      "Iteration 354, loss = 1.03548439\n",
      "Iteration 355, loss = 1.03389927\n",
      "Iteration 356, loss = 1.03087582\n",
      "Iteration 357, loss = 1.03660157\n",
      "Iteration 358, loss = 1.02894711\n",
      "Iteration 359, loss = 1.03012826\n",
      "Iteration 360, loss = 1.02940454\n",
      "Iteration 361, loss = 1.03070879\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 362, loss = 1.01761298\n",
      "Iteration 363, loss = 1.01695326\n",
      "Iteration 364, loss = 1.01657642\n",
      "Iteration 365, loss = 1.01622262\n",
      "Iteration 366, loss = 1.01641808\n",
      "Iteration 367, loss = 1.01581725\n",
      "Iteration 368, loss = 1.01508902\n",
      "Iteration 369, loss = 1.01509272\n",
      "Iteration 370, loss = 1.01489609\n",
      "Iteration 371, loss = 1.01433326\n",
      "Iteration 372, loss = 1.01383072\n",
      "Iteration 373, loss = 1.01389072\n",
      "Iteration 374, loss = 1.01319553\n",
      "Iteration 375, loss = 1.01317027\n",
      "Iteration 376, loss = 1.01279043\n",
      "Iteration 377, loss = 1.01236432\n",
      "Iteration 378, loss = 1.01191782\n",
      "Iteration 379, loss = 1.01152536\n",
      "Iteration 380, loss = 1.01132126\n",
      "Iteration 381, loss = 1.01117739\n",
      "Iteration 382, loss = 1.01052306\n",
      "Iteration 383, loss = 1.01060813\n",
      "Iteration 384, loss = 1.01026854\n",
      "Iteration 385, loss = 1.00993362\n",
      "Iteration 386, loss = 1.00951486\n",
      "Iteration 387, loss = 1.00883776\n",
      "Iteration 388, loss = 1.00870953\n",
      "Iteration 389, loss = 1.00857586\n",
      "Iteration 390, loss = 1.00799754\n",
      "Iteration 391, loss = 1.00752919\n",
      "Iteration 392, loss = 1.00756152\n",
      "Iteration 393, loss = 1.00701707\n",
      "Iteration 394, loss = 1.00640883\n",
      "Iteration 395, loss = 1.00639540\n",
      "Iteration 396, loss = 1.00607080\n",
      "Iteration 397, loss = 1.00616605\n",
      "Iteration 398, loss = 1.00562956\n",
      "Iteration 399, loss = 1.00531983\n",
      "Iteration 400, loss = 1.00497129\n",
      "Iteration 401, loss = 1.00492315\n",
      "Iteration 402, loss = 1.00427379\n",
      "Iteration 403, loss = 1.00409086\n",
      "Iteration 404, loss = 1.00353075\n",
      "Iteration 405, loss = 1.00329105\n",
      "Iteration 406, loss = 1.00318224\n",
      "Iteration 407, loss = 1.00262887\n",
      "Iteration 408, loss = 1.00213596\n",
      "Iteration 409, loss = 1.00227845\n",
      "Iteration 410, loss = 1.00155486\n",
      "Iteration 411, loss = 1.00164136\n",
      "Iteration 412, loss = 1.00087783\n",
      "Iteration 413, loss = 1.00107476\n",
      "Iteration 414, loss = 1.00080628\n",
      "Iteration 415, loss = 1.00029351\n",
      "Iteration 416, loss = 0.99986242\n",
      "Iteration 417, loss = 0.99935927\n",
      "Iteration 418, loss = 0.99925552\n",
      "Iteration 419, loss = 0.99906484\n",
      "Iteration 420, loss = 0.99849883\n",
      "Iteration 421, loss = 0.99794786\n",
      "Iteration 422, loss = 0.99794724\n",
      "Iteration 423, loss = 0.99758618\n",
      "Iteration 424, loss = 0.99722276\n",
      "Iteration 425, loss = 0.99666564\n",
      "Iteration 426, loss = 0.99651573\n",
      "Iteration 427, loss = 0.99608858\n",
      "Iteration 428, loss = 0.99584320\n",
      "Iteration 429, loss = 0.99555844\n",
      "Iteration 430, loss = 0.99551726\n",
      "Iteration 431, loss = 0.99528392\n",
      "Iteration 432, loss = 0.99467654\n",
      "Iteration 433, loss = 0.99459894\n",
      "Iteration 434, loss = 0.99400345\n",
      "Iteration 435, loss = 0.99365663\n",
      "Iteration 436, loss = 0.99354668\n",
      "Iteration 437, loss = 0.99310770\n",
      "Iteration 438, loss = 0.99303119\n",
      "Iteration 439, loss = 0.99256311\n",
      "Iteration 440, loss = 0.99241993\n",
      "Iteration 441, loss = 0.99188030\n",
      "Iteration 442, loss = 0.99157878\n",
      "Iteration 443, loss = 0.99131831\n",
      "Iteration 444, loss = 0.99093712\n",
      "Iteration 445, loss = 0.99050772\n",
      "Iteration 446, loss = 0.99058293\n",
      "Iteration 447, loss = 0.98969541\n",
      "Iteration 448, loss = 0.98987166\n",
      "Iteration 449, loss = 0.98925188\n",
      "Iteration 450, loss = 0.98871329\n",
      "Iteration 451, loss = 0.98863033\n",
      "Iteration 452, loss = 0.98790743\n",
      "Iteration 453, loss = 0.98776895\n",
      "Iteration 454, loss = 0.98743031\n",
      "Iteration 455, loss = 0.98730236\n",
      "Iteration 456, loss = 0.98691914\n",
      "Iteration 457, loss = 0.98638346\n",
      "Iteration 458, loss = 0.98593320\n",
      "Iteration 459, loss = 0.98563614\n",
      "Iteration 460, loss = 0.98561051\n",
      "Iteration 461, loss = 0.98511404\n",
      "Iteration 462, loss = 0.98561527\n",
      "Iteration 463, loss = 0.98477428\n",
      "Iteration 464, loss = 0.98434542\n",
      "Iteration 465, loss = 0.98390526\n",
      "Iteration 466, loss = 0.98394709\n",
      "Iteration 467, loss = 0.98391100\n",
      "Iteration 468, loss = 0.98311665\n",
      "Iteration 469, loss = 0.98268162\n",
      "Iteration 470, loss = 0.98268552\n",
      "Iteration 471, loss = 0.98241696\n",
      "Iteration 472, loss = 0.98214844\n",
      "Iteration 473, loss = 0.98149997\n",
      "Iteration 474, loss = 0.98129717\n",
      "Iteration 475, loss = 0.98080087\n",
      "Iteration 476, loss = 0.98088644\n",
      "Iteration 477, loss = 0.98039615\n",
      "Iteration 478, loss = 0.97987148\n",
      "Iteration 479, loss = 0.98001884\n",
      "Iteration 480, loss = 0.97929861\n",
      "Iteration 481, loss = 0.97903698\n",
      "Iteration 482, loss = 0.97902856\n",
      "Iteration 483, loss = 0.97834861\n",
      "Iteration 484, loss = 0.97855628\n",
      "Iteration 485, loss = 0.97814491\n",
      "Iteration 486, loss = 0.97746366\n",
      "Iteration 487, loss = 0.97751933\n",
      "Iteration 488, loss = 0.97718754\n",
      "Iteration 489, loss = 0.97704890\n",
      "Iteration 490, loss = 0.97621744\n",
      "Iteration 491, loss = 0.97624228\n",
      "Iteration 492, loss = 0.97633777\n",
      "Iteration 493, loss = 0.97617363\n",
      "Iteration 494, loss = 0.97542275\n",
      "Iteration 495, loss = 0.97506750\n",
      "Iteration 496, loss = 0.97486003\n",
      "Iteration 497, loss = 0.97513359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 498, loss = 0.97423965\n",
      "Iteration 499, loss = 0.97378594\n",
      "Iteration 500, loss = 0.97332683\n",
      "Iteration 501, loss = 0.97324632\n",
      "Iteration 502, loss = 0.97286308\n",
      "Iteration 503, loss = 0.97292791\n",
      "Iteration 504, loss = 0.97373144\n",
      "Iteration 505, loss = 0.97228786\n",
      "Iteration 506, loss = 0.97216289\n",
      "Iteration 507, loss = 0.97166630\n",
      "Iteration 508, loss = 0.97112111\n",
      "Iteration 509, loss = 0.97079561\n",
      "Iteration 510, loss = 0.97097225\n",
      "Iteration 511, loss = 0.97022077\n",
      "Iteration 512, loss = 0.97002179\n",
      "Iteration 513, loss = 0.97001334\n",
      "Iteration 514, loss = 0.96949378\n",
      "Iteration 515, loss = 0.96909846\n",
      "Iteration 516, loss = 0.96891188\n",
      "Iteration 517, loss = 0.96889090\n",
      "Iteration 518, loss = 0.96873940\n",
      "Iteration 519, loss = 0.96812749\n",
      "Iteration 520, loss = 0.96821524\n",
      "Iteration 521, loss = 0.96742073\n",
      "Iteration 522, loss = 0.96810073\n",
      "Iteration 523, loss = 0.96735685\n",
      "Iteration 524, loss = 0.96717226\n",
      "Iteration 525, loss = 0.96692111\n",
      "Iteration 526, loss = 0.96709105\n",
      "Iteration 527, loss = 0.96619654\n",
      "Iteration 528, loss = 0.96540216\n",
      "Iteration 529, loss = 0.96559803\n",
      "Iteration 530, loss = 0.96578163\n",
      "Iteration 531, loss = 0.96474838\n",
      "Iteration 532, loss = 0.96520010\n",
      "Iteration 533, loss = 0.96434408\n",
      "Iteration 534, loss = 0.96367724\n",
      "Iteration 535, loss = 0.96354478\n",
      "Iteration 536, loss = 0.96316374\n",
      "Iteration 537, loss = 0.96325084\n",
      "Iteration 538, loss = 0.96319611\n",
      "Iteration 539, loss = 0.96295095\n",
      "Iteration 540, loss = 0.96314421\n",
      "Iteration 541, loss = 0.96233793\n",
      "Iteration 542, loss = 0.96190685\n",
      "Iteration 543, loss = 0.96184777\n",
      "Iteration 544, loss = 0.96131297\n",
      "Iteration 545, loss = 0.96134622\n",
      "Iteration 546, loss = 0.96114322\n",
      "Iteration 547, loss = 0.96074385\n",
      "Iteration 548, loss = 0.96052149\n",
      "Iteration 549, loss = 0.96005739\n",
      "Iteration 550, loss = 0.96013092\n",
      "Iteration 551, loss = 0.96043770\n",
      "Iteration 552, loss = 0.95956896\n",
      "Iteration 553, loss = 0.95950418\n",
      "Iteration 554, loss = 0.95863482\n",
      "Iteration 555, loss = 0.95860105\n",
      "Iteration 556, loss = 0.95866458\n",
      "Iteration 557, loss = 0.95839465\n",
      "Iteration 558, loss = 0.95794196\n",
      "Iteration 559, loss = 0.95793278\n",
      "Iteration 560, loss = 0.95730984\n",
      "Iteration 561, loss = 0.95725640\n",
      "Iteration 562, loss = 0.95720196\n",
      "Iteration 563, loss = 0.95706399\n",
      "Iteration 564, loss = 0.95587334\n",
      "Iteration 565, loss = 0.95623402\n",
      "Iteration 566, loss = 0.95561707\n",
      "Iteration 567, loss = 0.95534740\n",
      "Iteration 568, loss = 0.95526894\n",
      "Iteration 569, loss = 0.95507946\n",
      "Iteration 570, loss = 0.95477010\n",
      "Iteration 571, loss = 0.95489511\n",
      "Iteration 572, loss = 0.95434967\n",
      "Iteration 573, loss = 0.95397212\n",
      "Iteration 574, loss = 0.95335086\n",
      "Iteration 575, loss = 0.95390908\n",
      "Iteration 576, loss = 0.95367543\n",
      "Iteration 577, loss = 0.95252517\n",
      "Iteration 578, loss = 0.95339555\n",
      "Iteration 579, loss = 0.95250146\n",
      "Iteration 580, loss = 0.95185654\n",
      "Iteration 581, loss = 0.95178767\n",
      "Iteration 582, loss = 0.95196170\n",
      "Iteration 583, loss = 0.95197151\n",
      "Iteration 584, loss = 0.95109173\n",
      "Iteration 585, loss = 0.95128271\n",
      "Iteration 586, loss = 0.95057436\n",
      "Iteration 587, loss = 0.95070459\n",
      "Iteration 588, loss = 0.95074127\n",
      "Iteration 589, loss = 0.94958472\n",
      "Iteration 590, loss = 0.95023131\n",
      "Iteration 591, loss = 0.94937974\n",
      "Iteration 592, loss = 0.94915295\n",
      "Iteration 593, loss = 0.94928357\n",
      "Iteration 594, loss = 0.94867772\n",
      "Iteration 595, loss = 0.94868490\n",
      "Iteration 596, loss = 0.94800366\n",
      "Iteration 597, loss = 0.94789677\n",
      "Iteration 598, loss = 0.94785851\n",
      "Iteration 599, loss = 0.94713431\n",
      "Iteration 600, loss = 0.94820239\n",
      "Iteration 601, loss = 0.94727565\n",
      "Iteration 602, loss = 0.94739168\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 603, loss = 0.94463224\n",
      "Iteration 604, loss = 0.94467177\n",
      "Iteration 605, loss = 0.94449229\n",
      "Iteration 606, loss = 0.94446485\n",
      "Iteration 607, loss = 0.94446450\n",
      "Iteration 608, loss = 0.94443933\n",
      "Iteration 609, loss = 0.94435553\n",
      "Iteration 610, loss = 0.94425318\n",
      "Iteration 611, loss = 0.94415099\n",
      "Iteration 612, loss = 0.94403509\n",
      "Iteration 613, loss = 0.94412177\n",
      "Iteration 614, loss = 0.94404225\n",
      "Iteration 615, loss = 0.94417157\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 616, loss = 0.94373931\n",
      "Iteration 617, loss = 0.94362506\n",
      "Iteration 618, loss = 0.94359773\n",
      "Iteration 619, loss = 0.94356155\n",
      "Iteration 620, loss = 0.94350592\n",
      "Iteration 621, loss = 0.94354577\n",
      "Iteration 622, loss = 0.94348586\n",
      "Iteration 623, loss = 0.94353797\n",
      "Iteration 624, loss = 0.94356680\n",
      "Iteration 625, loss = 0.94351848\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 626, loss = 0.94340532\n",
      "Iteration 627, loss = 0.94337236\n",
      "Iteration 628, loss = 0.94338557\n",
      "Iteration 629, loss = 0.94337974\n",
      "Iteration 630, loss = 0.94338187\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 631, loss = 0.94335464\n",
      "Iteration 632, loss = 0.94334967\n",
      "Iteration 633, loss = 0.94335498\n",
      "Iteration 634, loss = 0.94334776\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 10), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=1e-05, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6028"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60376"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cnf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
