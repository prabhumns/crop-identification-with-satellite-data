{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import svm\n",
    "from statistics import mean\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as shf\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('E:/Time Series/pra.pkl', 'rb')\n",
    "data2 = pickle.load(file_object)\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOYABEENS 204639\n",
      "CORN 151407\n",
      "DEVELOPED 12214\n",
      "FOREST 14646\n",
      "WATER BODY 6501\n",
      "ALFALFA 781\n",
      "OATS 25\n"
     ]
    }
   ],
   "source": [
    "for key, value in data2.items():\n",
    "    print (key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {key:value[:6000] for key, value in data.items() if len(value) >= 6000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(45):\n",
    "    lin = [l[i] for value in data.values() for l in value]\n",
    "    mins = min(lin)\n",
    "    maxs = max(lin)\n",
    "    means = mean(lin)\n",
    "    for key, value in data.items():\n",
    "        for example in value:\n",
    "            example[i] = np.float64((example[i] - means)/(maxs-mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : SOYABEENS\n",
      "1 : CORN\n",
      "2 : DEVELOPED\n",
      "3 : FOREST\n",
      "4 : WATER BODY\n"
     ]
    }
   ],
   "source": [
    "training_examples = []\n",
    "training_labels = []\n",
    "test_examples = []\n",
    "test_labels = []\n",
    "cv_examples = []\n",
    "cv_labels = []\n",
    "i = 0;\n",
    "for key, value in data.items():\n",
    "    training_examples = training_examples + value[:5000]\n",
    "    cv_examples = cv_examples + value[5000:5500]\n",
    "    test_examples = test_examples + value[5500:]\n",
    "    training_labels = training_labels + [i for v in range(5000)]\n",
    "    cv_labels = cv_labels + [i for v in range(500)]\n",
    "    test_labels = test_labels + [i for v in range(500)]\n",
    "    print(i,':', key)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples, training_labels  = shf(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=0.3, \n",
    "              kernel='rbf', \n",
    "              degree= 3,\n",
    "              gamma='auto',\n",
    "              coef0=0.0, \n",
    "              shrinking=True, \n",
    "              probability=True, \n",
    "              tol=0.00001, \n",
    "              cache_size=200, \n",
    "              class_weight=None, \n",
    "              verbose=True, \n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovr', \n",
    "              random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.3, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5376"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = MLPClassifier(hidden_layer_sizes=(90),\n",
    "                    activation='tanh', \n",
    "                    solver='sgd',\n",
    "                    alpha=0.0001, \n",
    "                    batch_size='auto', \n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=1000, \n",
    "                    shuffle=True, \n",
    "                    random_state=None, \n",
    "                    tol=0.000001, \n",
    "                    verbose=True, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False, \n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.53735851\n",
      "Iteration 2, loss = 1.36602341\n",
      "Iteration 3, loss = 1.31967138\n",
      "Iteration 4, loss = 1.29236692\n",
      "Iteration 5, loss = 1.27342080\n",
      "Iteration 6, loss = 1.25934705\n",
      "Iteration 7, loss = 1.24778784\n",
      "Iteration 8, loss = 1.23918840\n",
      "Iteration 9, loss = 1.23151249\n",
      "Iteration 10, loss = 1.22525020\n",
      "Iteration 11, loss = 1.22005800\n",
      "Iteration 12, loss = 1.21495480\n",
      "Iteration 13, loss = 1.21087828\n",
      "Iteration 14, loss = 1.20715369\n",
      "Iteration 15, loss = 1.20376793\n",
      "Iteration 16, loss = 1.20081851\n",
      "Iteration 17, loss = 1.19799391\n",
      "Iteration 18, loss = 1.19545371\n",
      "Iteration 19, loss = 1.19276547\n",
      "Iteration 20, loss = 1.19043944\n",
      "Iteration 21, loss = 1.18852709\n",
      "Iteration 22, loss = 1.18679793\n",
      "Iteration 23, loss = 1.18466392\n",
      "Iteration 24, loss = 1.18283611\n",
      "Iteration 25, loss = 1.18148382\n",
      "Iteration 26, loss = 1.17966111\n",
      "Iteration 27, loss = 1.17791474\n",
      "Iteration 28, loss = 1.17678921\n",
      "Iteration 29, loss = 1.17548769\n",
      "Iteration 30, loss = 1.17385700\n",
      "Iteration 31, loss = 1.17281589\n",
      "Iteration 32, loss = 1.17156006\n",
      "Iteration 33, loss = 1.17016004\n",
      "Iteration 34, loss = 1.16916510\n",
      "Iteration 35, loss = 1.16785309\n",
      "Iteration 36, loss = 1.16715760\n",
      "Iteration 37, loss = 1.16581952\n",
      "Iteration 38, loss = 1.16506104\n",
      "Iteration 39, loss = 1.16395501\n",
      "Iteration 40, loss = 1.16301387\n",
      "Iteration 41, loss = 1.16216905\n",
      "Iteration 42, loss = 1.16054877\n",
      "Iteration 43, loss = 1.16043221\n",
      "Iteration 44, loss = 1.15953796\n",
      "Iteration 45, loss = 1.15862946\n",
      "Iteration 46, loss = 1.15752553\n",
      "Iteration 47, loss = 1.15712609\n",
      "Iteration 48, loss = 1.15617172\n",
      "Iteration 49, loss = 1.15515158\n",
      "Iteration 50, loss = 1.15481148\n",
      "Iteration 51, loss = 1.15377494\n",
      "Iteration 52, loss = 1.15289263\n",
      "Iteration 53, loss = 1.15224718\n",
      "Iteration 54, loss = 1.15195045\n",
      "Iteration 55, loss = 1.15083918\n",
      "Iteration 56, loss = 1.15021975\n",
      "Iteration 57, loss = 1.14926697\n",
      "Iteration 58, loss = 1.14869728\n",
      "Iteration 59, loss = 1.14814867\n",
      "Iteration 60, loss = 1.14735574\n",
      "Iteration 61, loss = 1.14660100\n",
      "Iteration 62, loss = 1.14589123\n",
      "Iteration 63, loss = 1.14541108\n",
      "Iteration 64, loss = 1.14476239\n",
      "Iteration 65, loss = 1.14402183\n",
      "Iteration 66, loss = 1.14370611\n",
      "Iteration 67, loss = 1.14278005\n",
      "Iteration 68, loss = 1.14286822\n",
      "Iteration 69, loss = 1.14125363\n",
      "Iteration 70, loss = 1.14075182\n",
      "Iteration 71, loss = 1.14062126\n",
      "Iteration 72, loss = 1.13979385\n",
      "Iteration 73, loss = 1.13942368\n",
      "Iteration 74, loss = 1.13859486\n",
      "Iteration 75, loss = 1.13824305\n",
      "Iteration 76, loss = 1.13715851\n",
      "Iteration 77, loss = 1.13654993\n",
      "Iteration 78, loss = 1.13649802\n",
      "Iteration 79, loss = 1.13581086\n",
      "Iteration 80, loss = 1.13511043\n",
      "Iteration 81, loss = 1.13478865\n",
      "Iteration 82, loss = 1.13368599\n",
      "Iteration 83, loss = 1.13354332\n",
      "Iteration 84, loss = 1.13287379\n",
      "Iteration 85, loss = 1.13246675\n",
      "Iteration 86, loss = 1.13186138\n",
      "Iteration 87, loss = 1.13106535\n",
      "Iteration 88, loss = 1.13082522\n",
      "Iteration 89, loss = 1.13011419\n",
      "Iteration 90, loss = 1.12957338\n",
      "Iteration 91, loss = 1.12943045\n",
      "Iteration 92, loss = 1.12847704\n",
      "Iteration 93, loss = 1.12836952\n",
      "Iteration 94, loss = 1.12770202\n",
      "Iteration 95, loss = 1.12709776\n",
      "Iteration 96, loss = 1.12684048\n",
      "Iteration 97, loss = 1.12621054\n",
      "Iteration 98, loss = 1.12558163\n",
      "Iteration 99, loss = 1.12516446\n",
      "Iteration 100, loss = 1.12443592\n",
      "Iteration 101, loss = 1.12413978\n",
      "Iteration 102, loss = 1.12386277\n",
      "Iteration 103, loss = 1.12317038\n",
      "Iteration 104, loss = 1.12302349\n",
      "Iteration 105, loss = 1.12206767\n",
      "Iteration 106, loss = 1.12175644\n",
      "Iteration 107, loss = 1.12091347\n",
      "Iteration 108, loss = 1.12080619\n",
      "Iteration 109, loss = 1.12017502\n",
      "Iteration 110, loss = 1.11951835\n",
      "Iteration 111, loss = 1.11937064\n",
      "Iteration 112, loss = 1.11847573\n",
      "Iteration 113, loss = 1.11829047\n",
      "Iteration 114, loss = 1.11791874\n",
      "Iteration 115, loss = 1.11709035\n",
      "Iteration 116, loss = 1.11655514\n",
      "Iteration 117, loss = 1.11591607\n",
      "Iteration 118, loss = 1.11557977\n",
      "Iteration 119, loss = 1.11493287\n",
      "Iteration 120, loss = 1.11483064\n",
      "Iteration 121, loss = 1.11454336\n",
      "Iteration 122, loss = 1.11390087\n",
      "Iteration 123, loss = 1.11325574\n",
      "Iteration 124, loss = 1.11308857\n",
      "Iteration 125, loss = 1.11255970\n",
      "Iteration 126, loss = 1.11192781\n",
      "Iteration 127, loss = 1.11099328\n",
      "Iteration 128, loss = 1.11110541\n",
      "Iteration 129, loss = 1.11071400\n",
      "Iteration 130, loss = 1.10988210\n",
      "Iteration 131, loss = 1.10942475\n",
      "Iteration 132, loss = 1.10876710\n",
      "Iteration 133, loss = 1.10885608\n",
      "Iteration 134, loss = 1.10827252\n",
      "Iteration 135, loss = 1.10752435\n",
      "Iteration 136, loss = 1.10730316\n",
      "Iteration 137, loss = 1.10657179\n",
      "Iteration 138, loss = 1.10675110\n",
      "Iteration 139, loss = 1.10622902\n",
      "Iteration 140, loss = 1.10538783\n",
      "Iteration 141, loss = 1.10513473\n",
      "Iteration 142, loss = 1.10405789\n",
      "Iteration 143, loss = 1.10392636\n",
      "Iteration 144, loss = 1.10330189\n",
      "Iteration 145, loss = 1.10281111\n",
      "Iteration 146, loss = 1.10291371\n",
      "Iteration 147, loss = 1.10212227\n",
      "Iteration 148, loss = 1.10183955\n",
      "Iteration 149, loss = 1.10144133\n",
      "Iteration 150, loss = 1.10134843\n",
      "Iteration 151, loss = 1.10021374\n",
      "Iteration 152, loss = 1.09966275\n",
      "Iteration 153, loss = 1.09919235\n",
      "Iteration 154, loss = 1.09918206\n",
      "Iteration 155, loss = 1.09884588\n",
      "Iteration 156, loss = 1.09830506\n",
      "Iteration 157, loss = 1.09793883\n",
      "Iteration 158, loss = 1.09720914\n",
      "Iteration 159, loss = 1.09664934\n",
      "Iteration 160, loss = 1.09584663\n",
      "Iteration 161, loss = 1.09577373\n",
      "Iteration 162, loss = 1.09477679\n",
      "Iteration 163, loss = 1.09439668\n",
      "Iteration 164, loss = 1.09438143\n",
      "Iteration 165, loss = 1.09404503\n",
      "Iteration 166, loss = 1.09320923\n",
      "Iteration 167, loss = 1.09330832\n",
      "Iteration 168, loss = 1.09263110\n",
      "Iteration 169, loss = 1.09219206\n",
      "Iteration 170, loss = 1.09163556\n",
      "Iteration 171, loss = 1.09087807\n",
      "Iteration 172, loss = 1.09047488\n",
      "Iteration 173, loss = 1.09049809\n",
      "Iteration 174, loss = 1.08951587\n",
      "Iteration 175, loss = 1.08937899\n",
      "Iteration 176, loss = 1.08947068\n",
      "Iteration 177, loss = 1.08849039\n",
      "Iteration 178, loss = 1.08763726\n",
      "Iteration 179, loss = 1.08685079\n",
      "Iteration 180, loss = 1.08705216\n",
      "Iteration 181, loss = 1.08673880\n",
      "Iteration 182, loss = 1.08637320\n",
      "Iteration 183, loss = 1.08576421\n",
      "Iteration 184, loss = 1.08507668\n",
      "Iteration 185, loss = 1.08415745\n",
      "Iteration 186, loss = 1.08444664\n",
      "Iteration 187, loss = 1.08358025\n",
      "Iteration 188, loss = 1.08350031\n",
      "Iteration 189, loss = 1.08335008\n",
      "Iteration 190, loss = 1.08250174\n",
      "Iteration 191, loss = 1.08191834\n",
      "Iteration 192, loss = 1.08120726\n",
      "Iteration 193, loss = 1.08081209\n",
      "Iteration 194, loss = 1.07992842\n",
      "Iteration 195, loss = 1.08074541\n",
      "Iteration 196, loss = 1.07918386\n",
      "Iteration 197, loss = 1.07836204\n",
      "Iteration 198, loss = 1.07858247\n",
      "Iteration 199, loss = 1.07756611\n",
      "Iteration 200, loss = 1.07744584\n",
      "Iteration 201, loss = 1.07699027\n",
      "Iteration 202, loss = 1.07575931\n",
      "Iteration 203, loss = 1.07577171\n",
      "Iteration 204, loss = 1.07520912\n",
      "Iteration 205, loss = 1.07487428\n",
      "Iteration 206, loss = 1.07438352\n",
      "Iteration 207, loss = 1.07397726\n",
      "Iteration 208, loss = 1.07336745\n",
      "Iteration 209, loss = 1.07306406\n",
      "Iteration 210, loss = 1.07213779\n",
      "Iteration 211, loss = 1.07143078\n",
      "Iteration 212, loss = 1.07062893\n",
      "Iteration 213, loss = 1.07075964\n",
      "Iteration 214, loss = 1.07007882\n",
      "Iteration 215, loss = 1.06943334\n",
      "Iteration 216, loss = 1.06892245\n",
      "Iteration 217, loss = 1.06827083\n",
      "Iteration 218, loss = 1.06776974\n",
      "Iteration 219, loss = 1.06684551\n",
      "Iteration 220, loss = 1.06674104\n",
      "Iteration 221, loss = 1.06582132\n",
      "Iteration 222, loss = 1.06571710\n",
      "Iteration 223, loss = 1.06515862\n",
      "Iteration 224, loss = 1.06406071\n",
      "Iteration 225, loss = 1.06378316\n",
      "Iteration 226, loss = 1.06336214\n",
      "Iteration 227, loss = 1.06224934\n",
      "Iteration 228, loss = 1.06178212\n",
      "Iteration 229, loss = 1.06191713\n",
      "Iteration 230, loss = 1.06071187\n",
      "Iteration 231, loss = 1.06039563\n",
      "Iteration 232, loss = 1.05955868\n",
      "Iteration 233, loss = 1.05932097\n",
      "Iteration 234, loss = 1.05865266\n",
      "Iteration 235, loss = 1.05849896\n",
      "Iteration 236, loss = 1.05734120\n",
      "Iteration 237, loss = 1.05687315\n",
      "Iteration 238, loss = 1.05636923\n",
      "Iteration 239, loss = 1.05630639\n",
      "Iteration 240, loss = 1.05552732\n",
      "Iteration 241, loss = 1.05422168\n",
      "Iteration 242, loss = 1.05416348\n",
      "Iteration 243, loss = 1.05248122\n",
      "Iteration 244, loss = 1.05283152\n",
      "Iteration 245, loss = 1.05246894\n",
      "Iteration 246, loss = 1.05131085\n",
      "Iteration 247, loss = 1.05055089\n",
      "Iteration 248, loss = 1.05009224\n",
      "Iteration 249, loss = 1.04930099\n",
      "Iteration 250, loss = 1.04846629\n",
      "Iteration 251, loss = 1.04772942\n",
      "Iteration 252, loss = 1.04703060\n",
      "Iteration 253, loss = 1.04673993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 1.04634986\n",
      "Iteration 255, loss = 1.04500901\n",
      "Iteration 256, loss = 1.04460320\n",
      "Iteration 257, loss = 1.04366320\n",
      "Iteration 258, loss = 1.04348749\n",
      "Iteration 259, loss = 1.04242084\n",
      "Iteration 260, loss = 1.04138827\n",
      "Iteration 261, loss = 1.04111454\n",
      "Iteration 262, loss = 1.04012763\n",
      "Iteration 263, loss = 1.03954848\n",
      "Iteration 264, loss = 1.03795135\n",
      "Iteration 265, loss = 1.03831410\n",
      "Iteration 266, loss = 1.03705856\n",
      "Iteration 267, loss = 1.03616123\n",
      "Iteration 268, loss = 1.03646538\n",
      "Iteration 269, loss = 1.03480382\n",
      "Iteration 270, loss = 1.03369895\n",
      "Iteration 271, loss = 1.03406879\n",
      "Iteration 272, loss = 1.03247571\n",
      "Iteration 273, loss = 1.03168633\n",
      "Iteration 274, loss = 1.03100789\n",
      "Iteration 275, loss = 1.03119620\n",
      "Iteration 276, loss = 1.02982753\n",
      "Iteration 277, loss = 1.02895553\n",
      "Iteration 278, loss = 1.02834606\n",
      "Iteration 279, loss = 1.02756740\n",
      "Iteration 280, loss = 1.02741836\n",
      "Iteration 281, loss = 1.02578833\n",
      "Iteration 282, loss = 1.02396288\n",
      "Iteration 283, loss = 1.02388988\n",
      "Iteration 284, loss = 1.02441720\n",
      "Iteration 285, loss = 1.02202640\n",
      "Iteration 286, loss = 1.02090787\n",
      "Iteration 287, loss = 1.02086878\n",
      "Iteration 288, loss = 1.02010469\n",
      "Iteration 289, loss = 1.01899637\n",
      "Iteration 290, loss = 1.01784799\n",
      "Iteration 291, loss = 1.01756924\n",
      "Iteration 292, loss = 1.01720005\n",
      "Iteration 293, loss = 1.01553302\n",
      "Iteration 294, loss = 1.01535022\n",
      "Iteration 295, loss = 1.01451750\n",
      "Iteration 296, loss = 1.01354802\n",
      "Iteration 297, loss = 1.01265871\n",
      "Iteration 298, loss = 1.01161128\n",
      "Iteration 299, loss = 1.01158440\n",
      "Iteration 300, loss = 1.01059773\n",
      "Iteration 301, loss = 1.00987744\n",
      "Iteration 302, loss = 1.00796519\n",
      "Iteration 303, loss = 1.00812779\n",
      "Iteration 304, loss = 1.00624273\n",
      "Iteration 305, loss = 1.00545621\n",
      "Iteration 306, loss = 1.00639303\n",
      "Iteration 307, loss = 1.00341205\n",
      "Iteration 308, loss = 1.00235968\n",
      "Iteration 309, loss = 1.00314067\n",
      "Iteration 310, loss = 1.00144614\n",
      "Iteration 311, loss = 1.00065131\n",
      "Iteration 312, loss = 0.99939676\n",
      "Iteration 313, loss = 0.99795151\n",
      "Iteration 314, loss = 0.99594747\n",
      "Iteration 315, loss = 0.99771186\n",
      "Iteration 316, loss = 0.99610601\n",
      "Iteration 317, loss = 0.99566352\n",
      "Iteration 318, loss = 0.99557873\n",
      "Iteration 319, loss = 0.99169850\n",
      "Iteration 320, loss = 0.99322008\n",
      "Iteration 321, loss = 0.99150794\n",
      "Iteration 322, loss = 0.99077286\n",
      "Iteration 323, loss = 0.99069772\n",
      "Iteration 324, loss = 0.98844603\n",
      "Iteration 325, loss = 0.98722929\n",
      "Iteration 326, loss = 0.98653196\n",
      "Iteration 327, loss = 0.98627729\n",
      "Iteration 328, loss = 0.98563213\n",
      "Iteration 329, loss = 0.98383136\n",
      "Iteration 330, loss = 0.98287634\n",
      "Iteration 331, loss = 0.98459504\n",
      "Iteration 332, loss = 0.98095796\n",
      "Iteration 333, loss = 0.97995632\n",
      "Iteration 334, loss = 0.97869261\n",
      "Iteration 335, loss = 0.97816027\n",
      "Iteration 336, loss = 0.97867547\n",
      "Iteration 337, loss = 0.97873107\n",
      "Iteration 338, loss = 0.97671459\n",
      "Iteration 339, loss = 0.98015469\n",
      "Iteration 340, loss = 0.97539465\n",
      "Iteration 341, loss = 0.97407887\n",
      "Iteration 342, loss = 0.97301466\n",
      "Iteration 343, loss = 0.97277224\n",
      "Iteration 344, loss = 0.97269888\n",
      "Iteration 345, loss = 0.97213194\n",
      "Iteration 346, loss = 0.97031917\n",
      "Iteration 347, loss = 0.97007378\n",
      "Iteration 348, loss = 0.96952766\n",
      "Iteration 349, loss = 0.96636818\n",
      "Iteration 350, loss = 0.96810404\n",
      "Iteration 351, loss = 0.96465404\n",
      "Iteration 352, loss = 0.96687210\n",
      "Iteration 353, loss = 0.96562804\n",
      "Iteration 354, loss = 0.96420393\n",
      "Iteration 355, loss = 0.96245038\n",
      "Iteration 356, loss = 0.96112284\n",
      "Iteration 357, loss = 0.96613465\n",
      "Iteration 358, loss = 0.96339866\n",
      "Iteration 359, loss = 0.96324056\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 360, loss = 0.95157080\n",
      "Iteration 361, loss = 0.94764627\n",
      "Iteration 362, loss = 0.94777485\n",
      "Iteration 363, loss = 0.94737052\n",
      "Iteration 364, loss = 0.94729068\n",
      "Iteration 365, loss = 0.94669739\n",
      "Iteration 366, loss = 0.94634163\n",
      "Iteration 367, loss = 0.94672902\n",
      "Iteration 368, loss = 0.94611676\n",
      "Iteration 369, loss = 0.94573762\n",
      "Iteration 370, loss = 0.94601677\n",
      "Iteration 371, loss = 0.94576230\n",
      "Iteration 372, loss = 0.94524083\n",
      "Iteration 373, loss = 0.94496546\n",
      "Iteration 374, loss = 0.94502548\n",
      "Iteration 375, loss = 0.94478741\n",
      "Iteration 376, loss = 0.94441786\n",
      "Iteration 377, loss = 0.94434569\n",
      "Iteration 378, loss = 0.94414386\n",
      "Iteration 379, loss = 0.94385669\n",
      "Iteration 380, loss = 0.94383308\n",
      "Iteration 381, loss = 0.94359788\n",
      "Iteration 382, loss = 0.94303691\n",
      "Iteration 383, loss = 0.94299841\n",
      "Iteration 384, loss = 0.94291463\n",
      "Iteration 385, loss = 0.94255145\n",
      "Iteration 386, loss = 0.94256373\n",
      "Iteration 387, loss = 0.94200625\n",
      "Iteration 388, loss = 0.94207079\n",
      "Iteration 389, loss = 0.94190857\n",
      "Iteration 390, loss = 0.94196363\n",
      "Iteration 391, loss = 0.94131184\n",
      "Iteration 392, loss = 0.94156053\n",
      "Iteration 393, loss = 0.94111665\n",
      "Iteration 394, loss = 0.94102436\n",
      "Iteration 395, loss = 0.94074696\n",
      "Iteration 396, loss = 0.94050817\n",
      "Iteration 397, loss = 0.94065005\n",
      "Iteration 398, loss = 0.94009537\n",
      "Iteration 399, loss = 0.93966363\n",
      "Iteration 400, loss = 0.93970985\n",
      "Iteration 401, loss = 0.93981580\n",
      "Iteration 402, loss = 0.93928703\n",
      "Iteration 403, loss = 0.93927923\n",
      "Iteration 404, loss = 0.93906100\n",
      "Iteration 405, loss = 0.93856967\n",
      "Iteration 406, loss = 0.93834429\n",
      "Iteration 407, loss = 0.93798736\n",
      "Iteration 408, loss = 0.93853301\n",
      "Iteration 409, loss = 0.93800370\n",
      "Iteration 410, loss = 0.93777389\n",
      "Iteration 411, loss = 0.93742899\n",
      "Iteration 412, loss = 0.93780045\n",
      "Iteration 413, loss = 0.93690996\n",
      "Iteration 414, loss = 0.93711989\n",
      "Iteration 415, loss = 0.93687705\n",
      "Iteration 416, loss = 0.93655279\n",
      "Iteration 417, loss = 0.93670797\n",
      "Iteration 418, loss = 0.93651174\n",
      "Iteration 419, loss = 0.93559125\n",
      "Iteration 420, loss = 0.93558479\n",
      "Iteration 421, loss = 0.93543720\n",
      "Iteration 422, loss = 0.93543977\n",
      "Iteration 423, loss = 0.93519997\n",
      "Iteration 424, loss = 0.93490839\n",
      "Iteration 425, loss = 0.93443047\n",
      "Iteration 426, loss = 0.93448106\n",
      "Iteration 427, loss = 0.93461979\n",
      "Iteration 428, loss = 0.93415121\n",
      "Iteration 429, loss = 0.93379162\n",
      "Iteration 430, loss = 0.93337473\n",
      "Iteration 431, loss = 0.93368503\n",
      "Iteration 432, loss = 0.93326082\n",
      "Iteration 433, loss = 0.93306278\n",
      "Iteration 434, loss = 0.93302941\n",
      "Iteration 435, loss = 0.93262332\n",
      "Iteration 436, loss = 0.93256066\n",
      "Iteration 437, loss = 0.93254195\n",
      "Iteration 438, loss = 0.93215036\n",
      "Iteration 439, loss = 0.93204178\n",
      "Iteration 440, loss = 0.93169390\n",
      "Iteration 441, loss = 0.93130830\n",
      "Iteration 442, loss = 0.93099330\n",
      "Iteration 443, loss = 0.93122820\n",
      "Iteration 444, loss = 0.93099309\n",
      "Iteration 445, loss = 0.93057817\n",
      "Iteration 446, loss = 0.93049428\n",
      "Iteration 447, loss = 0.93036297\n",
      "Iteration 448, loss = 0.93045822\n",
      "Iteration 449, loss = 0.93012977\n",
      "Iteration 450, loss = 0.92973025\n",
      "Iteration 451, loss = 0.92940175\n",
      "Iteration 452, loss = 0.92949375\n",
      "Iteration 453, loss = 0.92948569\n",
      "Iteration 454, loss = 0.92899766\n",
      "Iteration 455, loss = 0.92878122\n",
      "Iteration 456, loss = 0.92877067\n",
      "Iteration 457, loss = 0.92859490\n",
      "Iteration 458, loss = 0.92793195\n",
      "Iteration 459, loss = 0.92782214\n",
      "Iteration 460, loss = 0.92814220\n",
      "Iteration 461, loss = 0.92757851\n",
      "Iteration 462, loss = 0.92776440\n",
      "Iteration 463, loss = 0.92740390\n",
      "Iteration 464, loss = 0.92704923\n",
      "Iteration 465, loss = 0.92711172\n",
      "Iteration 466, loss = 0.92663856\n",
      "Iteration 467, loss = 0.92668819\n",
      "Iteration 468, loss = 0.92630095\n",
      "Iteration 469, loss = 0.92607691\n",
      "Iteration 470, loss = 0.92615270\n",
      "Iteration 471, loss = 0.92551496\n",
      "Iteration 472, loss = 0.92562138\n",
      "Iteration 473, loss = 0.92558059\n",
      "Iteration 474, loss = 0.92542723\n",
      "Iteration 475, loss = 0.92519116\n",
      "Iteration 476, loss = 0.92498827\n",
      "Iteration 477, loss = 0.92451396\n",
      "Iteration 478, loss = 0.92465433\n",
      "Iteration 479, loss = 0.92456795\n",
      "Iteration 480, loss = 0.92422108\n",
      "Iteration 481, loss = 0.92394318\n",
      "Iteration 482, loss = 0.92356401\n",
      "Iteration 483, loss = 0.92352634\n",
      "Iteration 484, loss = 0.92336741\n",
      "Iteration 485, loss = 0.92330696\n",
      "Iteration 486, loss = 0.92309588\n",
      "Iteration 487, loss = 0.92279355\n",
      "Iteration 488, loss = 0.92268078\n",
      "Iteration 489, loss = 0.92225747\n",
      "Iteration 490, loss = 0.92189589\n",
      "Iteration 491, loss = 0.92180445\n",
      "Iteration 492, loss = 0.92192437\n",
      "Iteration 493, loss = 0.92136366\n",
      "Iteration 494, loss = 0.92142413\n",
      "Iteration 495, loss = 0.92132078\n",
      "Iteration 496, loss = 0.92142157\n",
      "Iteration 497, loss = 0.92101022\n",
      "Iteration 498, loss = 0.92075356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 499, loss = 0.92079416\n",
      "Iteration 500, loss = 0.92009210\n",
      "Iteration 501, loss = 0.92045961\n",
      "Iteration 502, loss = 0.92043267\n",
      "Iteration 503, loss = 0.91979026\n",
      "Iteration 504, loss = 0.91973459\n",
      "Iteration 505, loss = 0.91913972\n",
      "Iteration 506, loss = 0.91925347\n",
      "Iteration 507, loss = 0.91897338\n",
      "Iteration 508, loss = 0.91918562\n",
      "Iteration 509, loss = 0.91860571\n",
      "Iteration 510, loss = 0.91854286\n",
      "Iteration 511, loss = 0.91868118\n",
      "Iteration 512, loss = 0.91822491\n",
      "Iteration 513, loss = 0.91823400\n",
      "Iteration 514, loss = 0.91787706\n",
      "Iteration 515, loss = 0.91764421\n",
      "Iteration 516, loss = 0.91776425\n",
      "Iteration 517, loss = 0.91723414\n",
      "Iteration 518, loss = 0.91690180\n",
      "Iteration 519, loss = 0.91688410\n",
      "Iteration 520, loss = 0.91669778\n",
      "Iteration 521, loss = 0.91653899\n",
      "Iteration 522, loss = 0.91633939\n",
      "Iteration 523, loss = 0.91647621\n",
      "Iteration 524, loss = 0.91643515\n",
      "Iteration 525, loss = 0.91582800\n",
      "Iteration 526, loss = 0.91551167\n",
      "Iteration 527, loss = 0.91559775\n",
      "Iteration 528, loss = 0.91503853\n",
      "Iteration 529, loss = 0.91521729\n",
      "Iteration 530, loss = 0.91490544\n",
      "Iteration 531, loss = 0.91481529\n",
      "Iteration 532, loss = 0.91426914\n",
      "Iteration 533, loss = 0.91406742\n",
      "Iteration 534, loss = 0.91475063\n",
      "Iteration 535, loss = 0.91392876\n",
      "Iteration 536, loss = 0.91355585\n",
      "Iteration 537, loss = 0.91348510\n",
      "Iteration 538, loss = 0.91320857\n",
      "Iteration 539, loss = 0.91303402\n",
      "Iteration 540, loss = 0.91303159\n",
      "Iteration 541, loss = 0.91320953\n",
      "Iteration 542, loss = 0.91263010\n",
      "Iteration 543, loss = 0.91251207\n",
      "Iteration 544, loss = 0.91233551\n",
      "Iteration 545, loss = 0.91236858\n",
      "Iteration 546, loss = 0.91232658\n",
      "Iteration 547, loss = 0.91171682\n",
      "Iteration 548, loss = 0.91193186\n",
      "Iteration 549, loss = 0.91177433\n",
      "Iteration 550, loss = 0.91130269\n",
      "Iteration 551, loss = 0.91131065\n",
      "Iteration 552, loss = 0.91116779\n",
      "Iteration 553, loss = 0.91082069\n",
      "Iteration 554, loss = 0.91042104\n",
      "Iteration 555, loss = 0.91051580\n",
      "Iteration 556, loss = 0.90999724\n",
      "Iteration 557, loss = 0.91012579\n",
      "Iteration 558, loss = 0.90976872\n",
      "Iteration 559, loss = 0.90995620\n",
      "Iteration 560, loss = 0.90975379\n",
      "Iteration 561, loss = 0.90960096\n",
      "Iteration 562, loss = 0.90890645\n",
      "Iteration 563, loss = 0.90929381\n",
      "Iteration 564, loss = 0.90899056\n",
      "Iteration 565, loss = 0.90880090\n",
      "Iteration 566, loss = 0.90837449\n",
      "Iteration 567, loss = 0.90833320\n",
      "Iteration 568, loss = 0.90858641\n",
      "Iteration 569, loss = 0.90823134\n",
      "Iteration 570, loss = 0.90773200\n",
      "Iteration 571, loss = 0.90811848\n",
      "Iteration 572, loss = 0.90744690\n",
      "Iteration 573, loss = 0.90719633\n",
      "Iteration 574, loss = 0.90712650\n",
      "Iteration 575, loss = 0.90698684\n",
      "Iteration 576, loss = 0.90665135\n",
      "Iteration 577, loss = 0.90678921\n",
      "Iteration 578, loss = 0.90649122\n",
      "Iteration 579, loss = 0.90615648\n",
      "Iteration 580, loss = 0.90616583\n",
      "Iteration 581, loss = 0.90583960\n",
      "Iteration 582, loss = 0.90604941\n",
      "Iteration 583, loss = 0.90583305\n",
      "Iteration 584, loss = 0.90575715\n",
      "Iteration 585, loss = 0.90508839\n",
      "Iteration 586, loss = 0.90507561\n",
      "Iteration 587, loss = 0.90507313\n",
      "Iteration 588, loss = 0.90477746\n",
      "Iteration 589, loss = 0.90471931\n",
      "Iteration 590, loss = 0.90427801\n",
      "Iteration 591, loss = 0.90415712\n",
      "Iteration 592, loss = 0.90394426\n",
      "Iteration 593, loss = 0.90398056\n",
      "Iteration 594, loss = 0.90362003\n",
      "Iteration 595, loss = 0.90346144\n",
      "Iteration 596, loss = 0.90350671\n",
      "Iteration 597, loss = 0.90320718\n",
      "Iteration 598, loss = 0.90300891\n",
      "Iteration 599, loss = 0.90284133\n",
      "Iteration 600, loss = 0.90288995\n",
      "Iteration 601, loss = 0.90255157\n",
      "Iteration 602, loss = 0.90264092\n",
      "Iteration 603, loss = 0.90217327\n",
      "Iteration 604, loss = 0.90206837\n",
      "Iteration 605, loss = 0.90243063\n",
      "Iteration 606, loss = 0.90193867\n",
      "Iteration 607, loss = 0.90149105\n",
      "Iteration 608, loss = 0.90157448\n",
      "Iteration 609, loss = 0.90158521\n",
      "Iteration 610, loss = 0.90127110\n",
      "Iteration 611, loss = 0.90086035\n",
      "Iteration 612, loss = 0.90101709\n",
      "Iteration 613, loss = 0.90072507\n",
      "Iteration 614, loss = 0.90042205\n",
      "Iteration 615, loss = 0.90008754\n",
      "Iteration 616, loss = 0.89984294\n",
      "Iteration 617, loss = 0.89998970\n",
      "Iteration 618, loss = 0.89953586\n",
      "Iteration 619, loss = 0.89970425\n",
      "Iteration 620, loss = 0.89974973\n",
      "Iteration 621, loss = 0.89935304\n",
      "Iteration 622, loss = 0.89923974\n",
      "Iteration 623, loss = 0.89913514\n",
      "Iteration 624, loss = 0.89917361\n",
      "Iteration 625, loss = 0.89854682\n",
      "Iteration 626, loss = 0.89864244\n",
      "Iteration 627, loss = 0.89845657\n",
      "Iteration 628, loss = 0.89810471\n",
      "Iteration 629, loss = 0.89779954\n",
      "Iteration 630, loss = 0.89802024\n",
      "Iteration 631, loss = 0.89767403\n",
      "Iteration 632, loss = 0.89749335\n",
      "Iteration 633, loss = 0.89721510\n",
      "Iteration 634, loss = 0.89690674\n",
      "Iteration 635, loss = 0.89711075\n",
      "Iteration 636, loss = 0.89702145\n",
      "Iteration 637, loss = 0.89683598\n",
      "Iteration 638, loss = 0.89662702\n",
      "Iteration 639, loss = 0.89647133\n",
      "Iteration 640, loss = 0.89608593\n",
      "Iteration 641, loss = 0.89637590\n",
      "Iteration 642, loss = 0.89609884\n",
      "Iteration 643, loss = 0.89603962\n",
      "Iteration 644, loss = 0.89553409\n",
      "Iteration 645, loss = 0.89560838\n",
      "Iteration 646, loss = 0.89569485\n",
      "Iteration 647, loss = 0.89568105\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 648, loss = 0.89356714\n",
      "Iteration 649, loss = 0.89332101\n",
      "Iteration 650, loss = 0.89319240\n",
      "Iteration 651, loss = 0.89327155\n",
      "Iteration 652, loss = 0.89322925\n",
      "Iteration 653, loss = 0.89328170\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 654, loss = 0.89292178\n",
      "Iteration 655, loss = 0.89268402\n",
      "Iteration 656, loss = 0.89272546\n",
      "Iteration 657, loss = 0.89271641\n",
      "Iteration 658, loss = 0.89268013\n",
      "Iteration 659, loss = 0.89264366\n",
      "Iteration 660, loss = 0.89265782\n",
      "Iteration 661, loss = 0.89264554\n",
      "Iteration 662, loss = 0.89270202\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 663, loss = 0.89253928\n",
      "Iteration 664, loss = 0.89252639\n",
      "Iteration 665, loss = 0.89255159\n",
      "Iteration 666, loss = 0.89253018\n",
      "Iteration 667, loss = 0.89252588\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 668, loss = 0.89253434\n",
      "Iteration 669, loss = 0.89251139\n",
      "Iteration 670, loss = 0.89250499\n",
      "Iteration 671, loss = 0.89250248\n",
      "Iteration 672, loss = 0.89250349\n",
      "Iteration 673, loss = 0.89250381\n",
      "Iteration 674, loss = 0.89250484\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=90, learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=1e-06, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6746"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cnf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
