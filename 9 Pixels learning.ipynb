{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import svm\n",
    "from statistics import mean\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as shf\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('E:/Time Series/pra.pkl', 'rb')\n",
    "data10 = pickle.load(file_object)\n",
    "file_object.close()\n",
    "data2 = data10['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOYABEENS 206935\n",
      "CORN 153000\n",
      "DEVELOPED 12364\n",
      "FOREST 14825\n",
      "WATER BODY 6575\n",
      "ALFALFA 781\n",
      "OATS 25\n"
     ]
    }
   ],
   "source": [
    "for key, value in data2.items():\n",
    "    print (key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {key:value[:6000] for key, value in data.items() if len(value) >= 6000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    for n in range(len(data[key])):\n",
    "        example = data[key][n]\n",
    "        for i in range(45):\n",
    "            example.append(example[i]*example[i])\n",
    "        data[key][n] = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prabh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in ushort_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mins = {}; maxs = {}; means = {}\n",
    "for i in range(45):\n",
    "    lin = [l[i] for value in data.values() for l in value]\n",
    "    mins[i] = min(lin)\n",
    "    maxs[i] = max(lin)\n",
    "    means[i] = mean(lin)\n",
    "    \n",
    "data = {key: [[np.float64((example[i]-means[i])/(maxs[i] - mins[i])) for i in range(45)] for example in value] for key, value in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data #basic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data #with quadratic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : SOYABEENS\n",
      "1 : CORN\n",
      "2 : DEVELOPED\n",
      "3 : FOREST\n",
      "4 : WATER BODY\n"
     ]
    }
   ],
   "source": [
    "training_examples = []\n",
    "training_labels = []\n",
    "test_examples = []\n",
    "test_labels = []\n",
    "cv_examples = []\n",
    "cv_labels = []\n",
    "i = 0;\n",
    "for key, value in data.items():\n",
    "    training_examples = training_examples + value[:5000]\n",
    "    cv_examples = cv_examples + value[5000:5500]\n",
    "    test_examples = test_examples + value[5500:]\n",
    "    training_labels = training_labels + [i for v in range(5000)]\n",
    "    cv_labels = cv_labels + [i for v in range(500)]\n",
    "    test_labels = test_labels + [i for v in range(500)]\n",
    "    print(i,':', key)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples, training_labels  = shf(training_examples+cv_examples, training_labels+cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 ----- 0.048535356521442405\n",
      "21 ----- 0.03985315767818529\n",
      "27 ----- 0.03809236078657449\n",
      "29 ----- 0.03695677184875257\n",
      "24 ----- 0.03598134065371748\n",
      "44 ----- 0.03560175168918066\n",
      "41 ----- 0.03543894943264025\n",
      "7 ----- 0.03285241429999842\n",
      "19 ----- 0.029724885870531882\n",
      "22 ----- 0.028816484016258036\n",
      "43 ----- 0.028506369875081324\n",
      "34 ----- 0.027073119813907774\n",
      "9 ----- 0.02652774145505211\n",
      "5 ----- 0.025772140474956507\n",
      "14 ----- 0.02534716252883547\n",
      "28 ----- 0.022629984521518952\n",
      "10 ----- 0.02231873177849384\n",
      "40 ----- 0.02186460592092842\n",
      "37 ----- 0.021390400755072154\n",
      "12 ----- 0.020852251192170157\n",
      "35 ----- 0.02078187419612108\n",
      "0 ----- 0.02057729293288883\n",
      "42 ----- 0.019231765464939342\n",
      "32 ----- 0.019099175750229007\n",
      "4 ----- 0.018617460813781673\n",
      "20 ----- 0.01859798504538928\n",
      "11 ----- 0.018456147060565635\n",
      "6 ----- 0.01787120269551689\n",
      "17 ----- 0.01689948224687546\n",
      "25 ----- 0.016306363678589397\n",
      "36 ----- 0.015882872811241375\n",
      "2 ----- 0.015625669151704928\n",
      "1 ----- 0.015338253388536218\n",
      "8 ----- 0.014999561355965715\n",
      "16 ----- 0.014235792943392075\n",
      "33 ----- 0.01408556810489563\n",
      "26 ----- 0.013884893968893095\n",
      "13 ----- 0.013833023277744383\n",
      "30 ----- 0.013767921117380084\n",
      "31 ----- 0.013271573346721684\n",
      "38 ----- 0.01324007666312962\n",
      "3 ----- 0.013188146715385384\n",
      "23 ----- 0.012935129445437444\n",
      "18 ----- 0.012687185301667594\n",
      "15 ----- 0.012449601409709988\n"
     ]
    }
   ],
   "source": [
    "s = model.feature_importances_\n",
    "L = np.array(s)\n",
    "t = sorted(range(len(L)), key=lambda i:L[i])\n",
    "z = []; r = 0\n",
    "for j in t[::-1]:\n",
    "    print( j, '-----', s[j])\n",
    "    if s[j]>0.015: z.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "data = {key:[[example[i] for i in z]for example in value] for key, value in data.items()}\n",
    "print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : SOYABEENS\n",
      "1 : CORN\n",
      "2 : DEVELOPED\n",
      "3 : FOREST\n",
      "4 : WATER BODY\n"
     ]
    }
   ],
   "source": [
    "training_examples = []\n",
    "training_labels = []\n",
    "test_examples = []\n",
    "test_labels = []\n",
    "cv_examples = []\n",
    "cv_labels = []\n",
    "i = 0;\n",
    "for key, value in data.items():\n",
    "    training_examples = training_examples + value[:5000]\n",
    "    cv_examples = cv_examples + value[5000:5500]\n",
    "    test_examples = test_examples + value[5500:]\n",
    "    training_labels = training_labels + [i for v in range(5000)]\n",
    "    cv_labels = cv_labels + [i for v in range(500)]\n",
    "    test_labels = test_labels + [i for v in range(500)]\n",
    "    print(i,':', key)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples, training_labels  = shf(training_examples+test_examples, training_labels+test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=10000, \n",
    "              kernel='rbf', \n",
    "              degree= 3,\n",
    "              gamma= 0.001,\n",
    "              coef0=0.0, \n",
    "              shrinking=True, \n",
    "              probability= False, \n",
    "              tol=0.0001, \n",
    "              cache_size=200, \n",
    "              class_weight=None, \n",
    "              verbose=True, \n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovr', \n",
    "              random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6588"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502181818181819"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('clf.pkl', 'wb')\n",
    "pickle.dump(clf, file_object)\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = MLPClassifier(hidden_layer_sizes=(10,10,10),\n",
    "                    activation='tanh', \n",
    "                    solver='sgd',\n",
    "                    alpha=0.03, \n",
    "                    batch_size= 1000, \n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=10000, \n",
    "                    shuffle=True, \n",
    "                    random_state=None, \n",
    "                    tol=0.000001, \n",
    "                    verbose=True, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False,\n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65419895\n",
      "Iteration 2, loss = 1.60103402\n",
      "Iteration 3, loss = 1.56564135\n",
      "Iteration 4, loss = 1.54158117\n",
      "Iteration 5, loss = 1.52304790\n",
      "Iteration 6, loss = 1.50755363\n",
      "Iteration 7, loss = 1.49330778\n",
      "Iteration 8, loss = 1.47994232\n",
      "Iteration 9, loss = 1.46883371\n",
      "Iteration 10, loss = 1.45892151\n",
      "Iteration 11, loss = 1.44987235\n",
      "Iteration 12, loss = 1.44156576\n",
      "Iteration 13, loss = 1.43387979\n",
      "Iteration 14, loss = 1.42670027\n",
      "Iteration 15, loss = 1.42006366\n",
      "Iteration 16, loss = 1.41391639\n",
      "Iteration 17, loss = 1.40817380\n",
      "Iteration 18, loss = 1.40283374\n",
      "Iteration 19, loss = 1.39790178\n",
      "Iteration 20, loss = 1.39327337\n",
      "Iteration 21, loss = 1.38896524\n",
      "Iteration 22, loss = 1.38495121\n",
      "Iteration 23, loss = 1.38115487\n",
      "Iteration 24, loss = 1.37765617\n",
      "Iteration 25, loss = 1.37431880\n",
      "Iteration 26, loss = 1.37121666\n",
      "Iteration 27, loss = 1.36827763\n",
      "Iteration 28, loss = 1.36553175\n",
      "Iteration 29, loss = 1.36289854\n",
      "Iteration 30, loss = 1.36044027\n",
      "Iteration 31, loss = 1.35808826\n",
      "Iteration 32, loss = 1.35587568\n",
      "Iteration 33, loss = 1.35377395\n",
      "Iteration 34, loss = 1.35175622\n",
      "Iteration 35, loss = 1.34984881\n",
      "Iteration 36, loss = 1.34801178\n",
      "Iteration 37, loss = 1.34627894\n",
      "Iteration 38, loss = 1.34462964\n",
      "Iteration 39, loss = 1.34302565\n",
      "Iteration 40, loss = 1.34149719\n",
      "Iteration 41, loss = 1.34004457\n",
      "Iteration 42, loss = 1.33863216\n",
      "Iteration 43, loss = 1.33728179\n",
      "Iteration 44, loss = 1.33598860\n",
      "Iteration 45, loss = 1.33470299\n",
      "Iteration 46, loss = 1.33352536\n",
      "Iteration 47, loss = 1.33234274\n",
      "Iteration 48, loss = 1.33121690\n",
      "Iteration 49, loss = 1.33013474\n",
      "Iteration 50, loss = 1.32906173\n",
      "Iteration 51, loss = 1.32806384\n",
      "Iteration 52, loss = 1.32706919\n",
      "Iteration 53, loss = 1.32610033\n",
      "Iteration 54, loss = 1.32516825\n",
      "Iteration 55, loss = 1.32426339\n",
      "Iteration 56, loss = 1.32339695\n",
      "Iteration 57, loss = 1.32252225\n",
      "Iteration 58, loss = 1.32169335\n",
      "Iteration 59, loss = 1.32088770\n",
      "Iteration 60, loss = 1.32009782\n",
      "Iteration 61, loss = 1.31933026\n",
      "Iteration 62, loss = 1.31857387\n",
      "Iteration 63, loss = 1.31785806\n",
      "Iteration 64, loss = 1.31713019\n",
      "Iteration 65, loss = 1.31642778\n",
      "Iteration 66, loss = 1.31574826\n",
      "Iteration 67, loss = 1.31508173\n",
      "Iteration 68, loss = 1.31441983\n",
      "Iteration 69, loss = 1.31378301\n",
      "Iteration 70, loss = 1.31316040\n",
      "Iteration 71, loss = 1.31255683\n",
      "Iteration 72, loss = 1.31195583\n",
      "Iteration 73, loss = 1.31136797\n",
      "Iteration 74, loss = 1.31078747\n",
      "Iteration 75, loss = 1.31026043\n",
      "Iteration 76, loss = 1.30969602\n",
      "Iteration 77, loss = 1.30914524\n",
      "Iteration 78, loss = 1.30861717\n",
      "Iteration 79, loss = 1.30809108\n",
      "Iteration 80, loss = 1.30762209\n",
      "Iteration 81, loss = 1.30709369\n",
      "Iteration 82, loss = 1.30661174\n",
      "Iteration 83, loss = 1.30612035\n",
      "Iteration 84, loss = 1.30565518\n",
      "Iteration 85, loss = 1.30517037\n",
      "Iteration 86, loss = 1.30472720\n",
      "Iteration 87, loss = 1.30426611\n",
      "Iteration 88, loss = 1.30383150\n",
      "Iteration 89, loss = 1.30341832\n",
      "Iteration 90, loss = 1.30296217\n",
      "Iteration 91, loss = 1.30255652\n",
      "Iteration 92, loss = 1.30213024\n",
      "Iteration 93, loss = 1.30171253\n",
      "Iteration 94, loss = 1.30132463\n",
      "Iteration 95, loss = 1.30092863\n",
      "Iteration 96, loss = 1.30053939\n",
      "Iteration 97, loss = 1.30017046\n",
      "Iteration 98, loss = 1.29976104\n",
      "Iteration 99, loss = 1.29938518\n",
      "Iteration 100, loss = 1.29904708\n",
      "Iteration 101, loss = 1.29867669\n",
      "Iteration 102, loss = 1.29829610\n",
      "Iteration 103, loss = 1.29792799\n",
      "Iteration 104, loss = 1.29759910\n",
      "Iteration 105, loss = 1.29722451\n",
      "Iteration 106, loss = 1.29689735\n",
      "Iteration 107, loss = 1.29656674\n",
      "Iteration 108, loss = 1.29621413\n",
      "Iteration 109, loss = 1.29587393\n",
      "Iteration 110, loss = 1.29555191\n",
      "Iteration 111, loss = 1.29521407\n",
      "Iteration 112, loss = 1.29490923\n",
      "Iteration 113, loss = 1.29457215\n",
      "Iteration 114, loss = 1.29425814\n",
      "Iteration 115, loss = 1.29395377\n",
      "Iteration 116, loss = 1.29363555\n",
      "Iteration 117, loss = 1.29332331\n",
      "Iteration 118, loss = 1.29303128\n",
      "Iteration 119, loss = 1.29271462\n",
      "Iteration 120, loss = 1.29241487\n",
      "Iteration 121, loss = 1.29213297\n",
      "Iteration 122, loss = 1.29182115\n",
      "Iteration 123, loss = 1.29152765\n",
      "Iteration 124, loss = 1.29124371\n",
      "Iteration 125, loss = 1.29096753\n",
      "Iteration 126, loss = 1.29069932\n",
      "Iteration 127, loss = 1.29036718\n",
      "Iteration 128, loss = 1.29010286\n",
      "Iteration 129, loss = 1.28982818\n",
      "Iteration 130, loss = 1.28958591\n",
      "Iteration 131, loss = 1.28929532\n",
      "Iteration 132, loss = 1.28900969\n",
      "Iteration 133, loss = 1.28876815\n",
      "Iteration 134, loss = 1.28850612\n",
      "Iteration 135, loss = 1.28823706\n",
      "Iteration 136, loss = 1.28798625\n",
      "Iteration 137, loss = 1.28775601\n",
      "Iteration 138, loss = 1.28748512\n",
      "Iteration 139, loss = 1.28724202\n",
      "Iteration 140, loss = 1.28697467\n",
      "Iteration 141, loss = 1.28673012\n",
      "Iteration 142, loss = 1.28648244\n",
      "Iteration 143, loss = 1.28625638\n",
      "Iteration 144, loss = 1.28601082\n",
      "Iteration 145, loss = 1.28577902\n",
      "Iteration 146, loss = 1.28554581\n",
      "Iteration 147, loss = 1.28531359\n",
      "Iteration 148, loss = 1.28508574\n",
      "Iteration 149, loss = 1.28484143\n",
      "Iteration 150, loss = 1.28463598\n",
      "Iteration 151, loss = 1.28440211\n",
      "Iteration 152, loss = 1.28421213\n",
      "Iteration 153, loss = 1.28396654\n",
      "Iteration 154, loss = 1.28374652\n",
      "Iteration 155, loss = 1.28353549\n",
      "Iteration 156, loss = 1.28331451\n",
      "Iteration 157, loss = 1.28311332\n",
      "Iteration 158, loss = 1.28289181\n",
      "Iteration 159, loss = 1.28268204\n",
      "Iteration 160, loss = 1.28249134\n",
      "Iteration 161, loss = 1.28227451\n",
      "Iteration 162, loss = 1.28207781\n",
      "Iteration 163, loss = 1.28186604\n",
      "Iteration 164, loss = 1.28165794\n",
      "Iteration 165, loss = 1.28145912\n",
      "Iteration 166, loss = 1.28125289\n",
      "Iteration 167, loss = 1.28106869\n",
      "Iteration 168, loss = 1.28088246\n",
      "Iteration 169, loss = 1.28069358\n",
      "Iteration 170, loss = 1.28049777\n",
      "Iteration 171, loss = 1.28030502\n",
      "Iteration 172, loss = 1.28010050\n",
      "Iteration 173, loss = 1.27992215\n",
      "Iteration 174, loss = 1.27975689\n",
      "Iteration 175, loss = 1.27954812\n",
      "Iteration 176, loss = 1.27937699\n",
      "Iteration 177, loss = 1.27918277\n",
      "Iteration 178, loss = 1.27900723\n",
      "Iteration 179, loss = 1.27882723\n",
      "Iteration 180, loss = 1.27864769\n",
      "Iteration 181, loss = 1.27847854\n",
      "Iteration 182, loss = 1.27830503\n",
      "Iteration 183, loss = 1.27812635\n",
      "Iteration 184, loss = 1.27795295\n",
      "Iteration 185, loss = 1.27778835\n",
      "Iteration 186, loss = 1.27760547\n",
      "Iteration 187, loss = 1.27743529\n",
      "Iteration 188, loss = 1.27727750\n",
      "Iteration 189, loss = 1.27710286\n",
      "Iteration 190, loss = 1.27694081\n",
      "Iteration 191, loss = 1.27679603\n",
      "Iteration 192, loss = 1.27660792\n",
      "Iteration 193, loss = 1.27644135\n",
      "Iteration 194, loss = 1.27629730\n",
      "Iteration 195, loss = 1.27611550\n",
      "Iteration 196, loss = 1.27596676\n",
      "Iteration 197, loss = 1.27584889\n",
      "Iteration 198, loss = 1.27562559\n",
      "Iteration 199, loss = 1.27547818\n",
      "Iteration 200, loss = 1.27531992\n",
      "Iteration 201, loss = 1.27516017\n",
      "Iteration 202, loss = 1.27498897\n",
      "Iteration 203, loss = 1.27484690\n",
      "Iteration 204, loss = 1.27468492\n",
      "Iteration 205, loss = 1.27451773\n",
      "Iteration 206, loss = 1.27438302\n",
      "Iteration 207, loss = 1.27421442\n",
      "Iteration 208, loss = 1.27407509\n",
      "Iteration 209, loss = 1.27392785\n",
      "Iteration 210, loss = 1.27376758\n",
      "Iteration 211, loss = 1.27361230\n",
      "Iteration 212, loss = 1.27345908\n",
      "Iteration 213, loss = 1.27331378\n",
      "Iteration 214, loss = 1.27316795\n",
      "Iteration 215, loss = 1.27301316\n",
      "Iteration 216, loss = 1.27289601\n",
      "Iteration 217, loss = 1.27270496\n",
      "Iteration 218, loss = 1.27256895\n",
      "Iteration 219, loss = 1.27241067\n",
      "Iteration 220, loss = 1.27229140\n",
      "Iteration 221, loss = 1.27212436\n",
      "Iteration 222, loss = 1.27195640\n",
      "Iteration 223, loss = 1.27180931\n",
      "Iteration 224, loss = 1.27166454\n",
      "Iteration 225, loss = 1.27154270\n",
      "Iteration 226, loss = 1.27140022\n",
      "Iteration 227, loss = 1.27125050\n",
      "Iteration 228, loss = 1.27110299\n",
      "Iteration 229, loss = 1.27096789\n",
      "Iteration 230, loss = 1.27079077\n",
      "Iteration 231, loss = 1.27065677\n",
      "Iteration 232, loss = 1.27051237\n",
      "Iteration 233, loss = 1.27038949\n",
      "Iteration 234, loss = 1.27020162\n",
      "Iteration 235, loss = 1.27010073\n",
      "Iteration 236, loss = 1.26994723\n",
      "Iteration 237, loss = 1.26978345\n",
      "Iteration 238, loss = 1.26964191\n",
      "Iteration 239, loss = 1.26949174\n",
      "Iteration 240, loss = 1.26937631\n",
      "Iteration 241, loss = 1.26921950\n",
      "Iteration 242, loss = 1.26907160\n",
      "Iteration 243, loss = 1.26891573\n",
      "Iteration 244, loss = 1.26877871\n",
      "Iteration 245, loss = 1.26865971\n",
      "Iteration 246, loss = 1.26850142\n",
      "Iteration 247, loss = 1.26835120\n",
      "Iteration 248, loss = 1.26821355\n",
      "Iteration 249, loss = 1.26806626\n",
      "Iteration 250, loss = 1.26792740\n",
      "Iteration 251, loss = 1.26778615\n",
      "Iteration 252, loss = 1.26764023\n",
      "Iteration 253, loss = 1.26749904\n",
      "Iteration 254, loss = 1.26734088\n",
      "Iteration 255, loss = 1.26720905\n",
      "Iteration 256, loss = 1.26707641\n",
      "Iteration 257, loss = 1.26692915\n",
      "Iteration 258, loss = 1.26681672\n",
      "Iteration 259, loss = 1.26665596\n",
      "Iteration 260, loss = 1.26651505\n",
      "Iteration 261, loss = 1.26637997\n",
      "Iteration 262, loss = 1.26623147\n",
      "Iteration 263, loss = 1.26609596\n",
      "Iteration 264, loss = 1.26599111\n",
      "Iteration 265, loss = 1.26581208\n",
      "Iteration 266, loss = 1.26567314\n",
      "Iteration 267, loss = 1.26553756\n",
      "Iteration 268, loss = 1.26541379\n",
      "Iteration 269, loss = 1.26527507\n",
      "Iteration 270, loss = 1.26513674\n",
      "Iteration 271, loss = 1.26500274\n",
      "Iteration 272, loss = 1.26487851\n",
      "Iteration 273, loss = 1.26475693\n",
      "Iteration 274, loss = 1.26461067\n",
      "Iteration 275, loss = 1.26447700\n",
      "Iteration 276, loss = 1.26431737\n",
      "Iteration 277, loss = 1.26420516\n",
      "Iteration 278, loss = 1.26406462\n",
      "Iteration 279, loss = 1.26392324\n",
      "Iteration 280, loss = 1.26382592\n",
      "Iteration 281, loss = 1.26367053\n",
      "Iteration 282, loss = 1.26354678\n",
      "Iteration 283, loss = 1.26341679\n",
      "Iteration 284, loss = 1.26328432\n",
      "Iteration 285, loss = 1.26315202\n",
      "Iteration 286, loss = 1.26302938\n",
      "Iteration 287, loss = 1.26291209\n",
      "Iteration 288, loss = 1.26279352\n",
      "Iteration 289, loss = 1.26267047\n",
      "Iteration 290, loss = 1.26252095\n",
      "Iteration 291, loss = 1.26240795\n",
      "Iteration 292, loss = 1.26228346\n",
      "Iteration 293, loss = 1.26215501\n",
      "Iteration 294, loss = 1.26202605\n",
      "Iteration 295, loss = 1.26191364\n",
      "Iteration 296, loss = 1.26182003\n",
      "Iteration 297, loss = 1.26166392\n",
      "Iteration 298, loss = 1.26155325\n",
      "Iteration 299, loss = 1.26141633\n",
      "Iteration 300, loss = 1.26131297\n",
      "Iteration 301, loss = 1.26121137\n",
      "Iteration 302, loss = 1.26109592\n",
      "Iteration 303, loss = 1.26096995\n",
      "Iteration 304, loss = 1.26083596\n",
      "Iteration 305, loss = 1.26071221\n",
      "Iteration 306, loss = 1.26061127\n",
      "Iteration 307, loss = 1.26050058\n",
      "Iteration 308, loss = 1.26035670\n",
      "Iteration 309, loss = 1.26024052\n",
      "Iteration 310, loss = 1.26013183\n",
      "Iteration 311, loss = 1.26001961\n",
      "Iteration 312, loss = 1.25992463\n",
      "Iteration 313, loss = 1.25979157\n",
      "Iteration 314, loss = 1.25968026\n",
      "Iteration 315, loss = 1.25959117\n",
      "Iteration 316, loss = 1.25943866\n",
      "Iteration 317, loss = 1.25935180\n",
      "Iteration 318, loss = 1.25921878\n",
      "Iteration 319, loss = 1.25911518\n",
      "Iteration 320, loss = 1.25899281\n",
      "Iteration 321, loss = 1.25888050\n",
      "Iteration 322, loss = 1.25875965\n",
      "Iteration 323, loss = 1.25867214\n",
      "Iteration 324, loss = 1.25854162\n",
      "Iteration 325, loss = 1.25842539\n",
      "Iteration 326, loss = 1.25830991\n",
      "Iteration 327, loss = 1.25821432\n",
      "Iteration 328, loss = 1.25808824\n",
      "Iteration 329, loss = 1.25800506\n",
      "Iteration 330, loss = 1.25787954\n",
      "Iteration 331, loss = 1.25776124\n",
      "Iteration 332, loss = 1.25764915\n",
      "Iteration 333, loss = 1.25752065\n",
      "Iteration 334, loss = 1.25741758\n",
      "Iteration 335, loss = 1.25732919\n",
      "Iteration 336, loss = 1.25719760\n",
      "Iteration 337, loss = 1.25709772\n",
      "Iteration 338, loss = 1.25698221\n",
      "Iteration 339, loss = 1.25688100\n",
      "Iteration 340, loss = 1.25674942\n",
      "Iteration 341, loss = 1.25664218\n",
      "Iteration 342, loss = 1.25652423\n",
      "Iteration 343, loss = 1.25642906\n",
      "Iteration 344, loss = 1.25632003\n",
      "Iteration 345, loss = 1.25621513\n",
      "Iteration 346, loss = 1.25613665\n",
      "Iteration 347, loss = 1.25602695\n",
      "Iteration 348, loss = 1.25586065\n",
      "Iteration 349, loss = 1.25579034\n",
      "Iteration 350, loss = 1.25566781\n",
      "Iteration 351, loss = 1.25553746\n",
      "Iteration 352, loss = 1.25543630\n",
      "Iteration 353, loss = 1.25534248\n",
      "Iteration 354, loss = 1.25523383\n",
      "Iteration 355, loss = 1.25514374\n",
      "Iteration 356, loss = 1.25500978\n",
      "Iteration 357, loss = 1.25491803\n",
      "Iteration 358, loss = 1.25483857\n",
      "Iteration 359, loss = 1.25469867\n",
      "Iteration 360, loss = 1.25462472\n",
      "Iteration 361, loss = 1.25451572\n",
      "Iteration 362, loss = 1.25441082\n",
      "Iteration 363, loss = 1.25430134\n",
      "Iteration 364, loss = 1.25417322\n",
      "Iteration 365, loss = 1.25407613\n",
      "Iteration 366, loss = 1.25401315\n",
      "Iteration 367, loss = 1.25387366\n",
      "Iteration 368, loss = 1.25378541\n",
      "Iteration 369, loss = 1.25365377\n",
      "Iteration 370, loss = 1.25357467\n",
      "Iteration 371, loss = 1.25345714\n",
      "Iteration 372, loss = 1.25337167\n",
      "Iteration 373, loss = 1.25326526\n",
      "Iteration 374, loss = 1.25319243\n",
      "Iteration 375, loss = 1.25306008\n",
      "Iteration 376, loss = 1.25298262\n",
      "Iteration 377, loss = 1.25284910\n",
      "Iteration 378, loss = 1.25275993\n",
      "Iteration 379, loss = 1.25269092\n",
      "Iteration 380, loss = 1.25254313\n",
      "Iteration 381, loss = 1.25245190\n",
      "Iteration 382, loss = 1.25233748\n",
      "Iteration 383, loss = 1.25223815\n",
      "Iteration 384, loss = 1.25215401\n",
      "Iteration 385, loss = 1.25204614\n",
      "Iteration 386, loss = 1.25192653\n",
      "Iteration 387, loss = 1.25183494\n",
      "Iteration 388, loss = 1.25172483\n",
      "Iteration 389, loss = 1.25163542\n",
      "Iteration 390, loss = 1.25151848\n",
      "Iteration 391, loss = 1.25142519\n",
      "Iteration 392, loss = 1.25130690\n",
      "Iteration 393, loss = 1.25121508\n",
      "Iteration 394, loss = 1.25110287\n",
      "Iteration 395, loss = 1.25099807\n",
      "Iteration 396, loss = 1.25091827\n",
      "Iteration 397, loss = 1.25081090\n",
      "Iteration 398, loss = 1.25070834\n",
      "Iteration 399, loss = 1.25060652\n",
      "Iteration 400, loss = 1.25049741\n",
      "Iteration 401, loss = 1.25040089\n",
      "Iteration 402, loss = 1.25031812\n",
      "Iteration 403, loss = 1.25021417\n",
      "Iteration 404, loss = 1.25010617\n",
      "Iteration 405, loss = 1.25001448\n",
      "Iteration 406, loss = 1.24991343\n",
      "Iteration 407, loss = 1.24981797\n",
      "Iteration 408, loss = 1.24970474\n",
      "Iteration 409, loss = 1.24960686\n",
      "Iteration 410, loss = 1.24950871\n",
      "Iteration 411, loss = 1.24943123\n",
      "Iteration 412, loss = 1.24933053\n",
      "Iteration 413, loss = 1.24922841\n",
      "Iteration 414, loss = 1.24913827\n",
      "Iteration 415, loss = 1.24901738\n",
      "Iteration 416, loss = 1.24894453\n",
      "Iteration 417, loss = 1.24882747\n",
      "Iteration 418, loss = 1.24872887\n",
      "Iteration 419, loss = 1.24863600\n",
      "Iteration 420, loss = 1.24853245\n",
      "Iteration 421, loss = 1.24845134\n",
      "Iteration 422, loss = 1.24832622\n",
      "Iteration 423, loss = 1.24824748\n",
      "Iteration 424, loss = 1.24815456\n",
      "Iteration 425, loss = 1.24804974\n",
      "Iteration 426, loss = 1.24793655\n",
      "Iteration 427, loss = 1.24786013\n",
      "Iteration 428, loss = 1.24776244\n",
      "Iteration 429, loss = 1.24764942\n",
      "Iteration 430, loss = 1.24756678\n",
      "Iteration 431, loss = 1.24746535\n",
      "Iteration 432, loss = 1.24736277\n",
      "Iteration 433, loss = 1.24726859\n",
      "Iteration 434, loss = 1.24719559\n",
      "Iteration 435, loss = 1.24708348\n",
      "Iteration 436, loss = 1.24697910\n",
      "Iteration 437, loss = 1.24688933\n",
      "Iteration 438, loss = 1.24677054\n",
      "Iteration 439, loss = 1.24667590\n",
      "Iteration 440, loss = 1.24661389\n",
      "Iteration 441, loss = 1.24650703\n",
      "Iteration 442, loss = 1.24642521\n",
      "Iteration 443, loss = 1.24631091\n",
      "Iteration 444, loss = 1.24618623\n",
      "Iteration 445, loss = 1.24610792\n",
      "Iteration 446, loss = 1.24599616\n",
      "Iteration 447, loss = 1.24593237\n",
      "Iteration 448, loss = 1.24584124\n",
      "Iteration 449, loss = 1.24573949\n",
      "Iteration 450, loss = 1.24563191\n",
      "Iteration 451, loss = 1.24553612\n",
      "Iteration 452, loss = 1.24543841\n",
      "Iteration 453, loss = 1.24533319\n",
      "Iteration 454, loss = 1.24528643\n",
      "Iteration 455, loss = 1.24515322\n",
      "Iteration 456, loss = 1.24507021\n",
      "Iteration 457, loss = 1.24497724\n",
      "Iteration 458, loss = 1.24488631\n",
      "Iteration 459, loss = 1.24480447\n",
      "Iteration 460, loss = 1.24475757\n",
      "Iteration 461, loss = 1.24468082\n",
      "Iteration 462, loss = 1.24452388\n",
      "Iteration 463, loss = 1.24445009\n",
      "Iteration 464, loss = 1.24434011\n",
      "Iteration 465, loss = 1.24428140\n",
      "Iteration 466, loss = 1.24417797\n",
      "Iteration 467, loss = 1.24409495\n",
      "Iteration 468, loss = 1.24404826\n",
      "Iteration 469, loss = 1.24394889\n",
      "Iteration 470, loss = 1.24385383\n",
      "Iteration 471, loss = 1.24376018\n",
      "Iteration 472, loss = 1.24370553\n",
      "Iteration 473, loss = 1.24361497\n",
      "Iteration 474, loss = 1.24348767\n",
      "Iteration 475, loss = 1.24348373\n",
      "Iteration 476, loss = 1.24334225\n",
      "Iteration 477, loss = 1.24327837\n",
      "Iteration 478, loss = 1.24318695\n",
      "Iteration 479, loss = 1.24311483\n",
      "Iteration 480, loss = 1.24306977\n",
      "Iteration 481, loss = 1.24293413\n",
      "Iteration 482, loss = 1.24287059\n",
      "Iteration 483, loss = 1.24278079\n",
      "Iteration 484, loss = 1.24271795\n",
      "Iteration 485, loss = 1.24264227\n",
      "Iteration 486, loss = 1.24254408\n",
      "Iteration 487, loss = 1.24249019\n",
      "Iteration 488, loss = 1.24238787\n",
      "Iteration 489, loss = 1.24237601\n",
      "Iteration 490, loss = 1.24222225\n",
      "Iteration 491, loss = 1.24215766\n",
      "Iteration 492, loss = 1.24209363\n",
      "Iteration 493, loss = 1.24201741\n",
      "Iteration 494, loss = 1.24195434\n",
      "Iteration 495, loss = 1.24186992\n",
      "Iteration 496, loss = 1.24179885\n",
      "Iteration 497, loss = 1.24172120\n",
      "Iteration 498, loss = 1.24164499\n",
      "Iteration 499, loss = 1.24155859\n",
      "Iteration 500, loss = 1.24150588\n",
      "Iteration 501, loss = 1.24141602\n",
      "Iteration 502, loss = 1.24131488\n",
      "Iteration 503, loss = 1.24128354\n",
      "Iteration 504, loss = 1.24118803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 505, loss = 1.24114501\n",
      "Iteration 506, loss = 1.24104728\n",
      "Iteration 507, loss = 1.24097834\n",
      "Iteration 508, loss = 1.24094728\n",
      "Iteration 509, loss = 1.24086002\n",
      "Iteration 510, loss = 1.24072478\n",
      "Iteration 511, loss = 1.24067938\n",
      "Iteration 512, loss = 1.24065272\n",
      "Iteration 513, loss = 1.24053823\n",
      "Iteration 514, loss = 1.24049457\n",
      "Iteration 515, loss = 1.24043771\n",
      "Iteration 516, loss = 1.24031814\n",
      "Iteration 517, loss = 1.24026868\n",
      "Iteration 518, loss = 1.24022435\n",
      "Iteration 519, loss = 1.24011254\n",
      "Iteration 520, loss = 1.24006021\n",
      "Iteration 521, loss = 1.23998701\n",
      "Iteration 522, loss = 1.23989648\n",
      "Iteration 523, loss = 1.23984390\n",
      "Iteration 524, loss = 1.23976062\n",
      "Iteration 525, loss = 1.23970834\n",
      "Iteration 526, loss = 1.23961152\n",
      "Iteration 527, loss = 1.23956736\n",
      "Iteration 528, loss = 1.23950242\n",
      "Iteration 529, loss = 1.23944781\n",
      "Iteration 530, loss = 1.23933611\n",
      "Iteration 531, loss = 1.23928199\n",
      "Iteration 532, loss = 1.23920566\n",
      "Iteration 533, loss = 1.23912329\n",
      "Iteration 534, loss = 1.23906611\n",
      "Iteration 535, loss = 1.23899556\n",
      "Iteration 536, loss = 1.23893820\n",
      "Iteration 537, loss = 1.23887800\n",
      "Iteration 538, loss = 1.23880705\n",
      "Iteration 539, loss = 1.23873359\n",
      "Iteration 540, loss = 1.23866740\n",
      "Iteration 541, loss = 1.23858372\n",
      "Iteration 542, loss = 1.23854745\n",
      "Iteration 543, loss = 1.23847221\n",
      "Iteration 544, loss = 1.23841750\n",
      "Iteration 545, loss = 1.23831895\n",
      "Iteration 546, loss = 1.23826452\n",
      "Iteration 547, loss = 1.23820859\n",
      "Iteration 548, loss = 1.23813412\n",
      "Iteration 549, loss = 1.23803301\n",
      "Iteration 550, loss = 1.23799302\n",
      "Iteration 551, loss = 1.23792090\n",
      "Iteration 552, loss = 1.23785450\n",
      "Iteration 553, loss = 1.23779177\n",
      "Iteration 554, loss = 1.23772534\n",
      "Iteration 555, loss = 1.23771759\n",
      "Iteration 556, loss = 1.23758688\n",
      "Iteration 557, loss = 1.23753119\n",
      "Iteration 558, loss = 1.23746189\n",
      "Iteration 559, loss = 1.23741328\n",
      "Iteration 560, loss = 1.23738389\n",
      "Iteration 561, loss = 1.23726241\n",
      "Iteration 562, loss = 1.23723746\n",
      "Iteration 563, loss = 1.23712198\n",
      "Iteration 564, loss = 1.23706759\n",
      "Iteration 565, loss = 1.23697771\n",
      "Iteration 566, loss = 1.23695930\n",
      "Iteration 567, loss = 1.23689985\n",
      "Iteration 568, loss = 1.23679300\n",
      "Iteration 569, loss = 1.23672671\n",
      "Iteration 570, loss = 1.23667681\n",
      "Iteration 571, loss = 1.23663192\n",
      "Iteration 572, loss = 1.23654616\n",
      "Iteration 573, loss = 1.23648333\n",
      "Iteration 574, loss = 1.23643211\n",
      "Iteration 575, loss = 1.23635311\n",
      "Iteration 576, loss = 1.23628107\n",
      "Iteration 577, loss = 1.23621754\n",
      "Iteration 578, loss = 1.23613683\n",
      "Iteration 579, loss = 1.23607754\n",
      "Iteration 580, loss = 1.23604881\n",
      "Iteration 581, loss = 1.23596862\n",
      "Iteration 582, loss = 1.23592794\n",
      "Iteration 583, loss = 1.23582621\n",
      "Iteration 584, loss = 1.23576181\n",
      "Iteration 585, loss = 1.23570059\n",
      "Iteration 586, loss = 1.23563426\n",
      "Iteration 587, loss = 1.23557896\n",
      "Iteration 588, loss = 1.23555761\n",
      "Iteration 589, loss = 1.23544111\n",
      "Iteration 590, loss = 1.23536992\n",
      "Iteration 591, loss = 1.23530305\n",
      "Iteration 592, loss = 1.23524824\n",
      "Iteration 593, loss = 1.23520775\n",
      "Iteration 594, loss = 1.23510328\n",
      "Iteration 595, loss = 1.23505959\n",
      "Iteration 596, loss = 1.23497819\n",
      "Iteration 597, loss = 1.23493747\n",
      "Iteration 598, loss = 1.23487584\n",
      "Iteration 599, loss = 1.23480182\n",
      "Iteration 600, loss = 1.23474268\n",
      "Iteration 601, loss = 1.23469452\n",
      "Iteration 602, loss = 1.23462192\n",
      "Iteration 603, loss = 1.23456446\n",
      "Iteration 604, loss = 1.23447642\n",
      "Iteration 605, loss = 1.23441635\n",
      "Iteration 606, loss = 1.23434707\n",
      "Iteration 607, loss = 1.23430187\n",
      "Iteration 608, loss = 1.23421725\n",
      "Iteration 609, loss = 1.23420657\n",
      "Iteration 610, loss = 1.23410591\n",
      "Iteration 611, loss = 1.23403716\n",
      "Iteration 612, loss = 1.23399710\n",
      "Iteration 613, loss = 1.23391988\n",
      "Iteration 614, loss = 1.23390499\n",
      "Iteration 615, loss = 1.23377096\n",
      "Iteration 616, loss = 1.23375192\n",
      "Iteration 617, loss = 1.23365205\n",
      "Iteration 618, loss = 1.23359888\n",
      "Iteration 619, loss = 1.23351204\n",
      "Iteration 620, loss = 1.23346213\n",
      "Iteration 621, loss = 1.23339439\n",
      "Iteration 622, loss = 1.23332236\n",
      "Iteration 623, loss = 1.23326620\n",
      "Iteration 624, loss = 1.23323134\n",
      "Iteration 625, loss = 1.23315227\n",
      "Iteration 626, loss = 1.23309124\n",
      "Iteration 627, loss = 1.23303306\n",
      "Iteration 628, loss = 1.23293703\n",
      "Iteration 629, loss = 1.23291924\n",
      "Iteration 630, loss = 1.23281082\n",
      "Iteration 631, loss = 1.23277976\n",
      "Iteration 632, loss = 1.23268710\n",
      "Iteration 633, loss = 1.23263166\n",
      "Iteration 634, loss = 1.23258363\n",
      "Iteration 635, loss = 1.23252614\n",
      "Iteration 636, loss = 1.23249963\n",
      "Iteration 637, loss = 1.23236879\n",
      "Iteration 638, loss = 1.23231657\n",
      "Iteration 639, loss = 1.23222886\n",
      "Iteration 640, loss = 1.23217547\n",
      "Iteration 641, loss = 1.23213528\n",
      "Iteration 642, loss = 1.23207244\n",
      "Iteration 643, loss = 1.23198621\n",
      "Iteration 644, loss = 1.23191639\n",
      "Iteration 645, loss = 1.23187075\n",
      "Iteration 646, loss = 1.23181323\n",
      "Iteration 647, loss = 1.23171961\n",
      "Iteration 648, loss = 1.23169442\n",
      "Iteration 649, loss = 1.23160988\n",
      "Iteration 650, loss = 1.23153423\n",
      "Iteration 651, loss = 1.23151159\n",
      "Iteration 652, loss = 1.23146253\n",
      "Iteration 653, loss = 1.23133155\n",
      "Iteration 654, loss = 1.23128363\n",
      "Iteration 655, loss = 1.23121303\n",
      "Iteration 656, loss = 1.23115507\n",
      "Iteration 657, loss = 1.23110040\n",
      "Iteration 658, loss = 1.23104171\n",
      "Iteration 659, loss = 1.23102436\n",
      "Iteration 660, loss = 1.23090648\n",
      "Iteration 661, loss = 1.23082586\n",
      "Iteration 662, loss = 1.23074291\n",
      "Iteration 663, loss = 1.23069285\n",
      "Iteration 664, loss = 1.23064795\n",
      "Iteration 665, loss = 1.23061450\n",
      "Iteration 666, loss = 1.23047517\n",
      "Iteration 667, loss = 1.23042446\n",
      "Iteration 668, loss = 1.23036930\n",
      "Iteration 669, loss = 1.23034370\n",
      "Iteration 670, loss = 1.23022995\n",
      "Iteration 671, loss = 1.23019378\n",
      "Iteration 672, loss = 1.23011849\n",
      "Iteration 673, loss = 1.23004706\n",
      "Iteration 674, loss = 1.22997450\n",
      "Iteration 675, loss = 1.22988829\n",
      "Iteration 676, loss = 1.22997518\n",
      "Iteration 677, loss = 1.22989054\n",
      "Iteration 678, loss = 1.22969444\n",
      "Iteration 679, loss = 1.22964027\n",
      "Iteration 680, loss = 1.22959497\n",
      "Iteration 681, loss = 1.22962081\n",
      "Iteration 682, loss = 1.22946064\n",
      "Iteration 683, loss = 1.22938718\n",
      "Iteration 684, loss = 1.22934959\n",
      "Iteration 685, loss = 1.22925843\n",
      "Iteration 686, loss = 1.22921642\n",
      "Iteration 687, loss = 1.22915863\n",
      "Iteration 688, loss = 1.22903188\n",
      "Iteration 689, loss = 1.22900431\n",
      "Iteration 690, loss = 1.22893654\n",
      "Iteration 691, loss = 1.22887744\n",
      "Iteration 692, loss = 1.22877248\n",
      "Iteration 693, loss = 1.22871435\n",
      "Iteration 694, loss = 1.22865964\n",
      "Iteration 695, loss = 1.22859072\n",
      "Iteration 696, loss = 1.22854232\n",
      "Iteration 697, loss = 1.22846210\n",
      "Iteration 698, loss = 1.22837720\n",
      "Iteration 699, loss = 1.22830091\n",
      "Iteration 700, loss = 1.22825255\n",
      "Iteration 701, loss = 1.22821470\n",
      "Iteration 702, loss = 1.22813840\n",
      "Iteration 703, loss = 1.22801821\n",
      "Iteration 704, loss = 1.22799185\n",
      "Iteration 705, loss = 1.22789311\n",
      "Iteration 706, loss = 1.22787147\n",
      "Iteration 707, loss = 1.22779571\n",
      "Iteration 708, loss = 1.22773265\n",
      "Iteration 709, loss = 1.22763010\n",
      "Iteration 710, loss = 1.22758243\n",
      "Iteration 711, loss = 1.22750152\n",
      "Iteration 712, loss = 1.22741223\n",
      "Iteration 713, loss = 1.22736860\n",
      "Iteration 714, loss = 1.22729324\n",
      "Iteration 715, loss = 1.22726084\n",
      "Iteration 716, loss = 1.22720761\n",
      "Iteration 717, loss = 1.22710528\n",
      "Iteration 718, loss = 1.22701502\n",
      "Iteration 719, loss = 1.22693898\n",
      "Iteration 720, loss = 1.22686339\n",
      "Iteration 721, loss = 1.22682606\n",
      "Iteration 722, loss = 1.22680111\n",
      "Iteration 723, loss = 1.22663939\n",
      "Iteration 724, loss = 1.22664975\n",
      "Iteration 725, loss = 1.22650391\n",
      "Iteration 726, loss = 1.22647056\n",
      "Iteration 727, loss = 1.22642749\n",
      "Iteration 728, loss = 1.22637314\n",
      "Iteration 729, loss = 1.22626195\n",
      "Iteration 730, loss = 1.22621787\n",
      "Iteration 731, loss = 1.22613702\n",
      "Iteration 732, loss = 1.22605860\n",
      "Iteration 733, loss = 1.22597364\n",
      "Iteration 734, loss = 1.22592419\n",
      "Iteration 735, loss = 1.22586773\n",
      "Iteration 736, loss = 1.22577625\n",
      "Iteration 737, loss = 1.22568686\n",
      "Iteration 738, loss = 1.22560917\n",
      "Iteration 739, loss = 1.22553851\n",
      "Iteration 740, loss = 1.22545638\n",
      "Iteration 741, loss = 1.22540535\n",
      "Iteration 742, loss = 1.22527709\n",
      "Iteration 743, loss = 1.22525065\n",
      "Iteration 744, loss = 1.22517354\n",
      "Iteration 745, loss = 1.22508964\n",
      "Iteration 746, loss = 1.22503654\n",
      "Iteration 747, loss = 1.22492925\n",
      "Iteration 748, loss = 1.22490050\n",
      "Iteration 749, loss = 1.22484124\n",
      "Iteration 750, loss = 1.22485813\n",
      "Iteration 751, loss = 1.22472231\n",
      "Iteration 752, loss = 1.22461739\n",
      "Iteration 753, loss = 1.22448382\n",
      "Iteration 754, loss = 1.22441351\n",
      "Iteration 755, loss = 1.22435499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 756, loss = 1.22430666\n",
      "Iteration 757, loss = 1.22421073\n",
      "Iteration 758, loss = 1.22414686\n",
      "Iteration 759, loss = 1.22404678\n",
      "Iteration 760, loss = 1.22396415\n",
      "Iteration 761, loss = 1.22390613\n",
      "Iteration 762, loss = 1.22382318\n",
      "Iteration 763, loss = 1.22374830\n",
      "Iteration 764, loss = 1.22368642\n",
      "Iteration 765, loss = 1.22358568\n",
      "Iteration 766, loss = 1.22353104\n",
      "Iteration 767, loss = 1.22345010\n",
      "Iteration 768, loss = 1.22335988\n",
      "Iteration 769, loss = 1.22329438\n",
      "Iteration 770, loss = 1.22317936\n",
      "Iteration 771, loss = 1.22313078\n",
      "Iteration 772, loss = 1.22306787\n",
      "Iteration 773, loss = 1.22304322\n",
      "Iteration 774, loss = 1.22294328\n",
      "Iteration 775, loss = 1.22283095\n",
      "Iteration 776, loss = 1.22277253\n",
      "Iteration 777, loss = 1.22266537\n",
      "Iteration 778, loss = 1.22259807\n",
      "Iteration 779, loss = 1.22249178\n",
      "Iteration 780, loss = 1.22240993\n",
      "Iteration 781, loss = 1.22234710\n",
      "Iteration 782, loss = 1.22226550\n",
      "Iteration 783, loss = 1.22218972\n",
      "Iteration 784, loss = 1.22211303\n",
      "Iteration 785, loss = 1.22202400\n",
      "Iteration 786, loss = 1.22190867\n",
      "Iteration 787, loss = 1.22189611\n",
      "Iteration 788, loss = 1.22177772\n",
      "Iteration 789, loss = 1.22171371\n",
      "Iteration 790, loss = 1.22158852\n",
      "Iteration 791, loss = 1.22153012\n",
      "Iteration 792, loss = 1.22143175\n",
      "Iteration 793, loss = 1.22135642\n",
      "Iteration 794, loss = 1.22127266\n",
      "Iteration 795, loss = 1.22123079\n",
      "Iteration 796, loss = 1.22114854\n",
      "Iteration 797, loss = 1.22101272\n",
      "Iteration 798, loss = 1.22096468\n",
      "Iteration 799, loss = 1.22087190\n",
      "Iteration 800, loss = 1.22081203\n",
      "Iteration 801, loss = 1.22070603\n",
      "Iteration 802, loss = 1.22059150\n",
      "Iteration 803, loss = 1.22058246\n",
      "Iteration 804, loss = 1.22053273\n",
      "Iteration 805, loss = 1.22038496\n",
      "Iteration 806, loss = 1.22034041\n",
      "Iteration 807, loss = 1.22020090\n",
      "Iteration 808, loss = 1.22015882\n",
      "Iteration 809, loss = 1.22000985\n",
      "Iteration 810, loss = 1.21992742\n",
      "Iteration 811, loss = 1.21987442\n",
      "Iteration 812, loss = 1.21979513\n",
      "Iteration 813, loss = 1.21968544\n",
      "Iteration 814, loss = 1.21956644\n",
      "Iteration 815, loss = 1.21951014\n",
      "Iteration 816, loss = 1.21952408\n",
      "Iteration 817, loss = 1.21933217\n",
      "Iteration 818, loss = 1.21921312\n",
      "Iteration 819, loss = 1.21912754\n",
      "Iteration 820, loss = 1.21905470\n",
      "Iteration 821, loss = 1.21900270\n",
      "Iteration 822, loss = 1.21890531\n",
      "Iteration 823, loss = 1.21878858\n",
      "Iteration 824, loss = 1.21868250\n",
      "Iteration 825, loss = 1.21867017\n",
      "Iteration 826, loss = 1.21848527\n",
      "Iteration 827, loss = 1.21842820\n",
      "Iteration 828, loss = 1.21839408\n",
      "Iteration 829, loss = 1.21832054\n",
      "Iteration 830, loss = 1.21824631\n",
      "Iteration 831, loss = 1.21808844\n",
      "Iteration 832, loss = 1.21798625\n",
      "Iteration 833, loss = 1.21788743\n",
      "Iteration 834, loss = 1.21779884\n",
      "Iteration 835, loss = 1.21767460\n",
      "Iteration 836, loss = 1.21761779\n",
      "Iteration 837, loss = 1.21756967\n",
      "Iteration 838, loss = 1.21748815\n",
      "Iteration 839, loss = 1.21731400\n",
      "Iteration 840, loss = 1.21720459\n",
      "Iteration 841, loss = 1.21722513\n",
      "Iteration 842, loss = 1.21712333\n",
      "Iteration 843, loss = 1.21700743\n",
      "Iteration 844, loss = 1.21684175\n",
      "Iteration 845, loss = 1.21676671\n",
      "Iteration 846, loss = 1.21667547\n",
      "Iteration 847, loss = 1.21655451\n",
      "Iteration 848, loss = 1.21651155\n",
      "Iteration 849, loss = 1.21642205\n",
      "Iteration 850, loss = 1.21626832\n",
      "Iteration 851, loss = 1.21623113\n",
      "Iteration 852, loss = 1.21607423\n",
      "Iteration 853, loss = 1.21601203\n",
      "Iteration 854, loss = 1.21585869\n",
      "Iteration 855, loss = 1.21586811\n",
      "Iteration 856, loss = 1.21565984\n",
      "Iteration 857, loss = 1.21556093\n",
      "Iteration 858, loss = 1.21558841\n",
      "Iteration 859, loss = 1.21541249\n",
      "Iteration 860, loss = 1.21530770\n",
      "Iteration 861, loss = 1.21520483\n",
      "Iteration 862, loss = 1.21514457\n",
      "Iteration 863, loss = 1.21505096\n",
      "Iteration 864, loss = 1.21492562\n",
      "Iteration 865, loss = 1.21489680\n",
      "Iteration 866, loss = 1.21470516\n",
      "Iteration 867, loss = 1.21456430\n",
      "Iteration 868, loss = 1.21441899\n",
      "Iteration 869, loss = 1.21449146\n",
      "Iteration 870, loss = 1.21429024\n",
      "Iteration 871, loss = 1.21418702\n",
      "Iteration 872, loss = 1.21413560\n",
      "Iteration 873, loss = 1.21398413\n",
      "Iteration 874, loss = 1.21382419\n",
      "Iteration 875, loss = 1.21377260\n",
      "Iteration 876, loss = 1.21359977\n",
      "Iteration 877, loss = 1.21350825\n",
      "Iteration 878, loss = 1.21342425\n",
      "Iteration 879, loss = 1.21342479\n",
      "Iteration 880, loss = 1.21321161\n",
      "Iteration 881, loss = 1.21312042\n",
      "Iteration 882, loss = 1.21295103\n",
      "Iteration 883, loss = 1.21295106\n",
      "Iteration 884, loss = 1.21277766\n",
      "Iteration 885, loss = 1.21268448\n",
      "Iteration 886, loss = 1.21254143\n",
      "Iteration 887, loss = 1.21265327\n",
      "Iteration 888, loss = 1.21235475\n",
      "Iteration 889, loss = 1.21225293\n",
      "Iteration 890, loss = 1.21224530\n",
      "Iteration 891, loss = 1.21217340\n",
      "Iteration 892, loss = 1.21194335\n",
      "Iteration 893, loss = 1.21181857\n",
      "Iteration 894, loss = 1.21174796\n",
      "Iteration 895, loss = 1.21166727\n",
      "Iteration 896, loss = 1.21155199\n",
      "Iteration 897, loss = 1.21137324\n",
      "Iteration 898, loss = 1.21126599\n",
      "Iteration 899, loss = 1.21116247\n",
      "Iteration 900, loss = 1.21106271\n",
      "Iteration 901, loss = 1.21094583\n",
      "Iteration 902, loss = 1.21083097\n",
      "Iteration 903, loss = 1.21085146\n",
      "Iteration 904, loss = 1.21057540\n",
      "Iteration 905, loss = 1.21047840\n",
      "Iteration 906, loss = 1.21042137\n",
      "Iteration 907, loss = 1.21034676\n",
      "Iteration 908, loss = 1.21010410\n",
      "Iteration 909, loss = 1.21001080\n",
      "Iteration 910, loss = 1.20991594\n",
      "Iteration 911, loss = 1.20980527\n",
      "Iteration 912, loss = 1.20968589\n",
      "Iteration 913, loss = 1.20959886\n",
      "Iteration 914, loss = 1.20943451\n",
      "Iteration 915, loss = 1.20939605\n",
      "Iteration 916, loss = 1.20919465\n",
      "Iteration 917, loss = 1.20911190\n",
      "Iteration 918, loss = 1.20891572\n",
      "Iteration 919, loss = 1.20885281\n",
      "Iteration 920, loss = 1.20879053\n",
      "Iteration 921, loss = 1.20855771\n",
      "Iteration 922, loss = 1.20852555\n",
      "Iteration 923, loss = 1.20839472\n",
      "Iteration 924, loss = 1.20823284\n",
      "Iteration 925, loss = 1.20810912\n",
      "Iteration 926, loss = 1.20815071\n",
      "Iteration 927, loss = 1.20785423\n",
      "Iteration 928, loss = 1.20774080\n",
      "Iteration 929, loss = 1.20766778\n",
      "Iteration 930, loss = 1.20748228\n",
      "Iteration 931, loss = 1.20733578\n",
      "Iteration 932, loss = 1.20728489\n",
      "Iteration 933, loss = 1.20712731\n",
      "Iteration 934, loss = 1.20700219\n",
      "Iteration 935, loss = 1.20704175\n",
      "Iteration 936, loss = 1.20688209\n",
      "Iteration 937, loss = 1.20667876\n",
      "Iteration 938, loss = 1.20652997\n",
      "Iteration 939, loss = 1.20636039\n",
      "Iteration 940, loss = 1.20625629\n",
      "Iteration 941, loss = 1.20612951\n",
      "Iteration 942, loss = 1.20603885\n",
      "Iteration 943, loss = 1.20595277\n",
      "Iteration 944, loss = 1.20574577\n",
      "Iteration 945, loss = 1.20567684\n",
      "Iteration 946, loss = 1.20550499\n",
      "Iteration 947, loss = 1.20540647\n",
      "Iteration 948, loss = 1.20543080\n",
      "Iteration 949, loss = 1.20512959\n",
      "Iteration 950, loss = 1.20495250\n",
      "Iteration 951, loss = 1.20494186\n",
      "Iteration 952, loss = 1.20470108\n",
      "Iteration 953, loss = 1.20457760\n",
      "Iteration 954, loss = 1.20450463\n",
      "Iteration 955, loss = 1.20437407\n",
      "Iteration 956, loss = 1.20427223\n",
      "Iteration 957, loss = 1.20398574\n",
      "Iteration 958, loss = 1.20404248\n",
      "Iteration 959, loss = 1.20381430\n",
      "Iteration 960, loss = 1.20374624\n",
      "Iteration 961, loss = 1.20349196\n",
      "Iteration 962, loss = 1.20340062\n",
      "Iteration 963, loss = 1.20328513\n",
      "Iteration 964, loss = 1.20316118\n",
      "Iteration 965, loss = 1.20301289\n",
      "Iteration 966, loss = 1.20289461\n",
      "Iteration 967, loss = 1.20282247\n",
      "Iteration 968, loss = 1.20267343\n",
      "Iteration 969, loss = 1.20250932\n",
      "Iteration 970, loss = 1.20226060\n",
      "Iteration 971, loss = 1.20217922\n",
      "Iteration 972, loss = 1.20195306\n",
      "Iteration 973, loss = 1.20172859\n",
      "Iteration 974, loss = 1.20186333\n",
      "Iteration 975, loss = 1.20164115\n",
      "Iteration 976, loss = 1.20134047\n",
      "Iteration 977, loss = 1.20143803\n",
      "Iteration 978, loss = 1.20134223\n",
      "Iteration 979, loss = 1.20105122\n",
      "Iteration 980, loss = 1.20084066\n",
      "Iteration 981, loss = 1.20074093\n",
      "Iteration 982, loss = 1.20054258\n",
      "Iteration 983, loss = 1.20046093\n",
      "Iteration 984, loss = 1.20025149\n",
      "Iteration 985, loss = 1.20022909\n",
      "Iteration 986, loss = 1.19995219\n",
      "Iteration 987, loss = 1.19997276\n",
      "Iteration 988, loss = 1.19967191\n",
      "Iteration 989, loss = 1.19969864\n",
      "Iteration 990, loss = 1.19942932\n",
      "Iteration 991, loss = 1.19925117\n",
      "Iteration 992, loss = 1.19909203\n",
      "Iteration 993, loss = 1.19893293\n",
      "Iteration 994, loss = 1.19873760\n",
      "Iteration 995, loss = 1.19870571\n",
      "Iteration 996, loss = 1.19848544\n",
      "Iteration 997, loss = 1.19835720\n",
      "Iteration 998, loss = 1.19823545\n",
      "Iteration 999, loss = 1.19799303\n",
      "Iteration 1000, loss = 1.19787849\n",
      "Iteration 1001, loss = 1.19771437\n",
      "Iteration 1002, loss = 1.19781872\n",
      "Iteration 1003, loss = 1.19740104\n",
      "Iteration 1004, loss = 1.19727778\n",
      "Iteration 1005, loss = 1.19701078\n",
      "Iteration 1006, loss = 1.19685749\n",
      "Iteration 1007, loss = 1.19690463\n",
      "Iteration 1008, loss = 1.19659223\n",
      "Iteration 1009, loss = 1.19648261\n",
      "Iteration 1010, loss = 1.19628514\n",
      "Iteration 1011, loss = 1.19624264\n",
      "Iteration 1012, loss = 1.19602705\n",
      "Iteration 1013, loss = 1.19581536\n",
      "Iteration 1014, loss = 1.19571257\n",
      "Iteration 1015, loss = 1.19549346\n",
      "Iteration 1016, loss = 1.19534468\n",
      "Iteration 1017, loss = 1.19517316\n",
      "Iteration 1018, loss = 1.19492691\n",
      "Iteration 1019, loss = 1.19497659\n",
      "Iteration 1020, loss = 1.19465638\n",
      "Iteration 1021, loss = 1.19475535\n",
      "Iteration 1022, loss = 1.19442889\n",
      "Iteration 1023, loss = 1.19416842\n",
      "Iteration 1024, loss = 1.19404418\n",
      "Iteration 1025, loss = 1.19379369\n",
      "Iteration 1026, loss = 1.19364487\n",
      "Iteration 1027, loss = 1.19363095\n",
      "Iteration 1028, loss = 1.19345908\n",
      "Iteration 1029, loss = 1.19313579\n",
      "Iteration 1030, loss = 1.19291926\n",
      "Iteration 1031, loss = 1.19278113\n",
      "Iteration 1032, loss = 1.19264166\n",
      "Iteration 1033, loss = 1.19259693\n",
      "Iteration 1034, loss = 1.19231436\n",
      "Iteration 1035, loss = 1.19227595\n",
      "Iteration 1036, loss = 1.19191477\n",
      "Iteration 1037, loss = 1.19185910\n",
      "Iteration 1038, loss = 1.19170813\n",
      "Iteration 1039, loss = 1.19141965\n",
      "Iteration 1040, loss = 1.19160817\n",
      "Iteration 1041, loss = 1.19116003\n",
      "Iteration 1042, loss = 1.19086072\n",
      "Iteration 1043, loss = 1.19092245\n",
      "Iteration 1044, loss = 1.19072699\n",
      "Iteration 1045, loss = 1.19028931\n",
      "Iteration 1046, loss = 1.19019320\n",
      "Iteration 1047, loss = 1.19002748\n",
      "Iteration 1048, loss = 1.18980524\n",
      "Iteration 1049, loss = 1.18972727\n",
      "Iteration 1050, loss = 1.18961198\n",
      "Iteration 1051, loss = 1.18933814\n",
      "Iteration 1052, loss = 1.18907683\n",
      "Iteration 1053, loss = 1.18894769\n",
      "Iteration 1054, loss = 1.18870753\n",
      "Iteration 1055, loss = 1.18861148\n",
      "Iteration 1056, loss = 1.18835104\n",
      "Iteration 1057, loss = 1.18799492\n",
      "Iteration 1058, loss = 1.18808122\n",
      "Iteration 1059, loss = 1.18761850\n",
      "Iteration 1060, loss = 1.18754616\n",
      "Iteration 1061, loss = 1.18760728\n",
      "Iteration 1062, loss = 1.18722638\n",
      "Iteration 1063, loss = 1.18691359\n",
      "Iteration 1064, loss = 1.18690625\n",
      "Iteration 1065, loss = 1.18657966\n",
      "Iteration 1066, loss = 1.18633661\n",
      "Iteration 1067, loss = 1.18639194\n",
      "Iteration 1068, loss = 1.18610864\n",
      "Iteration 1069, loss = 1.18578029\n",
      "Iteration 1070, loss = 1.18580750\n",
      "Iteration 1071, loss = 1.18546185\n",
      "Iteration 1072, loss = 1.18526047\n",
      "Iteration 1073, loss = 1.18491192\n",
      "Iteration 1074, loss = 1.18486807\n",
      "Iteration 1075, loss = 1.18471941\n",
      "Iteration 1076, loss = 1.18458281\n",
      "Iteration 1077, loss = 1.18414633\n",
      "Iteration 1078, loss = 1.18398157\n",
      "Iteration 1079, loss = 1.18381803\n",
      "Iteration 1080, loss = 1.18351740\n",
      "Iteration 1081, loss = 1.18338729\n",
      "Iteration 1082, loss = 1.18333259\n",
      "Iteration 1083, loss = 1.18304918\n",
      "Iteration 1084, loss = 1.18273558\n",
      "Iteration 1085, loss = 1.18246998\n",
      "Iteration 1086, loss = 1.18217547\n",
      "Iteration 1087, loss = 1.18224617\n",
      "Iteration 1088, loss = 1.18177585\n",
      "Iteration 1089, loss = 1.18192371\n",
      "Iteration 1090, loss = 1.18146827\n",
      "Iteration 1091, loss = 1.18119075\n",
      "Iteration 1092, loss = 1.18087872\n",
      "Iteration 1093, loss = 1.18063759\n",
      "Iteration 1094, loss = 1.18060391\n",
      "Iteration 1095, loss = 1.18029186\n",
      "Iteration 1096, loss = 1.18006617\n",
      "Iteration 1097, loss = 1.17986965\n",
      "Iteration 1098, loss = 1.17970077\n",
      "Iteration 1099, loss = 1.17927138\n",
      "Iteration 1100, loss = 1.17914859\n",
      "Iteration 1101, loss = 1.17880791\n",
      "Iteration 1102, loss = 1.17845440\n",
      "Iteration 1103, loss = 1.17835064\n",
      "Iteration 1104, loss = 1.17805288\n",
      "Iteration 1105, loss = 1.17816743\n",
      "Iteration 1106, loss = 1.17754569\n",
      "Iteration 1107, loss = 1.17742289\n",
      "Iteration 1108, loss = 1.17721195\n",
      "Iteration 1109, loss = 1.17695268\n",
      "Iteration 1110, loss = 1.17679751\n",
      "Iteration 1111, loss = 1.17644099\n",
      "Iteration 1112, loss = 1.17634760\n",
      "Iteration 1113, loss = 1.17600095\n",
      "Iteration 1114, loss = 1.17557893\n",
      "Iteration 1115, loss = 1.17522906\n",
      "Iteration 1116, loss = 1.17506989\n",
      "Iteration 1117, loss = 1.17509100\n",
      "Iteration 1118, loss = 1.17458603\n",
      "Iteration 1119, loss = 1.17438326\n",
      "Iteration 1120, loss = 1.17402910\n",
      "Iteration 1121, loss = 1.17364864\n",
      "Iteration 1122, loss = 1.17351100\n",
      "Iteration 1123, loss = 1.17341212\n",
      "Iteration 1124, loss = 1.17311933\n",
      "Iteration 1125, loss = 1.17264909\n",
      "Iteration 1126, loss = 1.17233283\n",
      "Iteration 1127, loss = 1.17243010\n",
      "Iteration 1128, loss = 1.17192770\n",
      "Iteration 1129, loss = 1.17144074\n",
      "Iteration 1130, loss = 1.17111213\n",
      "Iteration 1131, loss = 1.17104569\n",
      "Iteration 1132, loss = 1.17065350\n",
      "Iteration 1133, loss = 1.17021784\n",
      "Iteration 1134, loss = 1.16988255\n",
      "Iteration 1135, loss = 1.16988820\n",
      "Iteration 1136, loss = 1.16945162\n",
      "Iteration 1137, loss = 1.16889217\n",
      "Iteration 1138, loss = 1.16874318\n",
      "Iteration 1139, loss = 1.16855444\n",
      "Iteration 1140, loss = 1.16810340\n",
      "Iteration 1141, loss = 1.16755992\n",
      "Iteration 1142, loss = 1.16736885\n",
      "Iteration 1143, loss = 1.16685212\n",
      "Iteration 1144, loss = 1.16678150\n",
      "Iteration 1145, loss = 1.16639450\n",
      "Iteration 1146, loss = 1.16603488\n",
      "Iteration 1147, loss = 1.16576372\n",
      "Iteration 1148, loss = 1.16559993\n",
      "Iteration 1149, loss = 1.16502319\n",
      "Iteration 1150, loss = 1.16471016\n",
      "Iteration 1151, loss = 1.16437623\n",
      "Iteration 1152, loss = 1.16400189\n",
      "Iteration 1153, loss = 1.16356565\n",
      "Iteration 1154, loss = 1.16290935\n",
      "Iteration 1155, loss = 1.16280184\n",
      "Iteration 1156, loss = 1.16235258\n",
      "Iteration 1157, loss = 1.16201113\n",
      "Iteration 1158, loss = 1.16162938\n",
      "Iteration 1159, loss = 1.16132905\n",
      "Iteration 1160, loss = 1.16100686\n",
      "Iteration 1161, loss = 1.16051701\n",
      "Iteration 1162, loss = 1.16026084\n",
      "Iteration 1163, loss = 1.15996505\n",
      "Iteration 1164, loss = 1.15947063\n",
      "Iteration 1165, loss = 1.15897445\n",
      "Iteration 1166, loss = 1.15838579\n",
      "Iteration 1167, loss = 1.15791160\n",
      "Iteration 1168, loss = 1.15780224\n",
      "Iteration 1169, loss = 1.15698887\n",
      "Iteration 1170, loss = 1.15689709\n",
      "Iteration 1171, loss = 1.15640317\n",
      "Iteration 1172, loss = 1.15539723\n",
      "Iteration 1173, loss = 1.15511468\n",
      "Iteration 1174, loss = 1.15515837\n",
      "Iteration 1175, loss = 1.15468076\n",
      "Iteration 1176, loss = 1.15488296\n",
      "Iteration 1177, loss = 1.15371946\n",
      "Iteration 1178, loss = 1.15339485\n",
      "Iteration 1179, loss = 1.15243142\n",
      "Iteration 1180, loss = 1.15242654\n",
      "Iteration 1181, loss = 1.15306449\n",
      "Iteration 1182, loss = 1.15167947\n",
      "Iteration 1183, loss = 1.15075863\n",
      "Iteration 1184, loss = 1.15021294\n",
      "Iteration 1185, loss = 1.14959081\n",
      "Iteration 1186, loss = 1.14915812\n",
      "Iteration 1187, loss = 1.14832801\n",
      "Iteration 1188, loss = 1.14795241\n",
      "Iteration 1189, loss = 1.14685487\n",
      "Iteration 1190, loss = 1.14667921\n",
      "Iteration 1191, loss = 1.14631661\n",
      "Iteration 1192, loss = 1.14736275\n",
      "Iteration 1193, loss = 1.14629051\n",
      "Iteration 1194, loss = 1.14540127\n",
      "Iteration 1195, loss = 1.14454130\n",
      "Iteration 1196, loss = 1.14469334\n",
      "Iteration 1197, loss = 1.14541190\n",
      "Iteration 1198, loss = 1.14386105\n",
      "Iteration 1199, loss = 1.14331518\n",
      "Iteration 1200, loss = 1.14198232\n",
      "Iteration 1201, loss = 1.14212475\n",
      "Iteration 1202, loss = 1.14333783\n",
      "Iteration 1203, loss = 1.14002085\n",
      "Iteration 1204, loss = 1.13980057\n",
      "Iteration 1205, loss = 1.14089779\n",
      "Iteration 1206, loss = 1.14043012\n",
      "Iteration 1207, loss = 1.13959702\n",
      "Iteration 1208, loss = 1.13761922\n",
      "Iteration 1209, loss = 1.13768921\n",
      "Iteration 1210, loss = 1.13580791\n",
      "Iteration 1211, loss = 1.13851774\n",
      "Iteration 1212, loss = 1.13456180\n",
      "Iteration 1213, loss = 1.13636843\n",
      "Iteration 1214, loss = 1.14147100\n",
      "Iteration 1215, loss = 1.13912009\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 1216, loss = 1.13093367\n",
      "Iteration 1217, loss = 1.13097640\n",
      "Iteration 1218, loss = 1.13062833\n",
      "Iteration 1219, loss = 1.13052442\n",
      "Iteration 1220, loss = 1.13039452\n",
      "Iteration 1221, loss = 1.13019715\n",
      "Iteration 1222, loss = 1.13015004\n",
      "Iteration 1223, loss = 1.12991059\n",
      "Iteration 1224, loss = 1.12982757\n",
      "Iteration 1225, loss = 1.12959915\n",
      "Iteration 1226, loss = 1.12945056\n",
      "Iteration 1227, loss = 1.12923207\n",
      "Iteration 1228, loss = 1.12925915\n",
      "Iteration 1229, loss = 1.12910269\n",
      "Iteration 1230, loss = 1.12885892\n",
      "Iteration 1231, loss = 1.12889165\n",
      "Iteration 1232, loss = 1.12861255\n",
      "Iteration 1233, loss = 1.12848228\n",
      "Iteration 1234, loss = 1.12828344\n",
      "Iteration 1235, loss = 1.12819745\n",
      "Iteration 1236, loss = 1.12805396\n",
      "Iteration 1237, loss = 1.12785788\n",
      "Iteration 1238, loss = 1.12770424\n",
      "Iteration 1239, loss = 1.12755997\n",
      "Iteration 1240, loss = 1.12727060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1241, loss = 1.12747494\n",
      "Iteration 1242, loss = 1.12715764\n",
      "Iteration 1243, loss = 1.12703120\n",
      "Iteration 1244, loss = 1.12696418\n",
      "Iteration 1245, loss = 1.12676497\n",
      "Iteration 1246, loss = 1.12669161\n",
      "Iteration 1247, loss = 1.12651715\n",
      "Iteration 1248, loss = 1.12627732\n",
      "Iteration 1249, loss = 1.12609652\n",
      "Iteration 1250, loss = 1.12588972\n",
      "Iteration 1251, loss = 1.12595424\n",
      "Iteration 1252, loss = 1.12577644\n",
      "Iteration 1253, loss = 1.12541003\n",
      "Iteration 1254, loss = 1.12542874\n",
      "Iteration 1255, loss = 1.12542351\n",
      "Iteration 1256, loss = 1.12500613\n",
      "Iteration 1257, loss = 1.12479917\n",
      "Iteration 1258, loss = 1.12474569\n",
      "Iteration 1259, loss = 1.12460552\n",
      "Iteration 1260, loss = 1.12453703\n",
      "Iteration 1261, loss = 1.12436198\n",
      "Iteration 1262, loss = 1.12432878\n",
      "Iteration 1263, loss = 1.12407237\n",
      "Iteration 1264, loss = 1.12396043\n",
      "Iteration 1265, loss = 1.12371627\n",
      "Iteration 1266, loss = 1.12356020\n",
      "Iteration 1267, loss = 1.12348428\n",
      "Iteration 1268, loss = 1.12309292\n",
      "Iteration 1269, loss = 1.12308871\n",
      "Iteration 1270, loss = 1.12296974\n",
      "Iteration 1271, loss = 1.12277013\n",
      "Iteration 1272, loss = 1.12276219\n",
      "Iteration 1273, loss = 1.12253226\n",
      "Iteration 1274, loss = 1.12232147\n",
      "Iteration 1275, loss = 1.12232544\n",
      "Iteration 1276, loss = 1.12196469\n",
      "Iteration 1277, loss = 1.12178293\n",
      "Iteration 1278, loss = 1.12166574\n",
      "Iteration 1279, loss = 1.12167153\n",
      "Iteration 1280, loss = 1.12164296\n",
      "Iteration 1281, loss = 1.12117717\n",
      "Iteration 1282, loss = 1.12117685\n",
      "Iteration 1283, loss = 1.12112327\n",
      "Iteration 1284, loss = 1.12092719\n",
      "Iteration 1285, loss = 1.12060541\n",
      "Iteration 1286, loss = 1.12077926\n",
      "Iteration 1287, loss = 1.12035154\n",
      "Iteration 1288, loss = 1.12029209\n",
      "Iteration 1289, loss = 1.12000765\n",
      "Iteration 1290, loss = 1.11993498\n",
      "Iteration 1291, loss = 1.11962280\n",
      "Iteration 1292, loss = 1.11948155\n",
      "Iteration 1293, loss = 1.11925516\n",
      "Iteration 1294, loss = 1.11915662\n",
      "Iteration 1295, loss = 1.11938203\n",
      "Iteration 1296, loss = 1.11894004\n",
      "Iteration 1297, loss = 1.11871712\n",
      "Iteration 1298, loss = 1.11863011\n",
      "Iteration 1299, loss = 1.11831113\n",
      "Iteration 1300, loss = 1.11825766\n",
      "Iteration 1301, loss = 1.11810112\n",
      "Iteration 1302, loss = 1.11768629\n",
      "Iteration 1303, loss = 1.11765666\n",
      "Iteration 1304, loss = 1.11760769\n",
      "Iteration 1305, loss = 1.11762372\n",
      "Iteration 1306, loss = 1.11736787\n",
      "Iteration 1307, loss = 1.11721111\n",
      "Iteration 1308, loss = 1.11705288\n",
      "Iteration 1309, loss = 1.11667297\n",
      "Iteration 1310, loss = 1.11656526\n",
      "Iteration 1311, loss = 1.11639112\n",
      "Iteration 1312, loss = 1.11649427\n",
      "Iteration 1313, loss = 1.11613502\n",
      "Iteration 1314, loss = 1.11625069\n",
      "Iteration 1315, loss = 1.11584717\n",
      "Iteration 1316, loss = 1.11563294\n",
      "Iteration 1317, loss = 1.11554487\n",
      "Iteration 1318, loss = 1.11539418\n",
      "Iteration 1319, loss = 1.11523984\n",
      "Iteration 1320, loss = 1.11499420\n",
      "Iteration 1321, loss = 1.11504951\n",
      "Iteration 1322, loss = 1.11476351\n",
      "Iteration 1323, loss = 1.11457686\n",
      "Iteration 1324, loss = 1.11441037\n",
      "Iteration 1325, loss = 1.11428815\n",
      "Iteration 1326, loss = 1.11400515\n",
      "Iteration 1327, loss = 1.11401002\n",
      "Iteration 1328, loss = 1.11367335\n",
      "Iteration 1329, loss = 1.11367180\n",
      "Iteration 1330, loss = 1.11341590\n",
      "Iteration 1331, loss = 1.11323476\n",
      "Iteration 1332, loss = 1.11301395\n",
      "Iteration 1333, loss = 1.11276779\n",
      "Iteration 1334, loss = 1.11266198\n",
      "Iteration 1335, loss = 1.11252016\n",
      "Iteration 1336, loss = 1.11242849\n",
      "Iteration 1337, loss = 1.11226681\n",
      "Iteration 1338, loss = 1.11225054\n",
      "Iteration 1339, loss = 1.11185747\n",
      "Iteration 1340, loss = 1.11170177\n",
      "Iteration 1341, loss = 1.11157827\n",
      "Iteration 1342, loss = 1.11135809\n",
      "Iteration 1343, loss = 1.11130566\n",
      "Iteration 1344, loss = 1.11107811\n",
      "Iteration 1345, loss = 1.11079567\n",
      "Iteration 1346, loss = 1.11052622\n",
      "Iteration 1347, loss = 1.11084512\n",
      "Iteration 1348, loss = 1.11058502\n",
      "Iteration 1349, loss = 1.11030901\n",
      "Iteration 1350, loss = 1.11009465\n",
      "Iteration 1351, loss = 1.10985109\n",
      "Iteration 1352, loss = 1.10978431\n",
      "Iteration 1353, loss = 1.10975613\n",
      "Iteration 1354, loss = 1.10963610\n",
      "Iteration 1355, loss = 1.10936248\n",
      "Iteration 1356, loss = 1.10917682\n",
      "Iteration 1357, loss = 1.10878107\n",
      "Iteration 1358, loss = 1.10888378\n",
      "Iteration 1359, loss = 1.10859860\n",
      "Iteration 1360, loss = 1.10847579\n",
      "Iteration 1361, loss = 1.10818653\n",
      "Iteration 1362, loss = 1.10828644\n",
      "Iteration 1363, loss = 1.10789120\n",
      "Iteration 1364, loss = 1.10803792\n",
      "Iteration 1365, loss = 1.10749024\n",
      "Iteration 1366, loss = 1.10745860\n",
      "Iteration 1367, loss = 1.10711765\n",
      "Iteration 1368, loss = 1.10697544\n",
      "Iteration 1369, loss = 1.10697967\n",
      "Iteration 1370, loss = 1.10671173\n",
      "Iteration 1371, loss = 1.10646912\n",
      "Iteration 1372, loss = 1.10651647\n",
      "Iteration 1373, loss = 1.10625078\n",
      "Iteration 1374, loss = 1.10605150\n",
      "Iteration 1375, loss = 1.10598366\n",
      "Iteration 1376, loss = 1.10563408\n",
      "Iteration 1377, loss = 1.10556430\n",
      "Iteration 1378, loss = 1.10537589\n",
      "Iteration 1379, loss = 1.10533691\n",
      "Iteration 1380, loss = 1.10502867\n",
      "Iteration 1381, loss = 1.10493755\n",
      "Iteration 1382, loss = 1.10470502\n",
      "Iteration 1383, loss = 1.10475275\n",
      "Iteration 1384, loss = 1.10448465\n",
      "Iteration 1385, loss = 1.10422365\n",
      "Iteration 1386, loss = 1.10404920\n",
      "Iteration 1387, loss = 1.10384797\n",
      "Iteration 1388, loss = 1.10382860\n",
      "Iteration 1389, loss = 1.10349102\n",
      "Iteration 1390, loss = 1.10325224\n",
      "Iteration 1391, loss = 1.10309610\n",
      "Iteration 1392, loss = 1.10312758\n",
      "Iteration 1393, loss = 1.10259729\n",
      "Iteration 1394, loss = 1.10260174\n",
      "Iteration 1395, loss = 1.10245374\n",
      "Iteration 1396, loss = 1.10232259\n",
      "Iteration 1397, loss = 1.10197659\n",
      "Iteration 1398, loss = 1.10216537\n",
      "Iteration 1399, loss = 1.10186854\n",
      "Iteration 1400, loss = 1.10160812\n",
      "Iteration 1401, loss = 1.10137513\n",
      "Iteration 1402, loss = 1.10132317\n",
      "Iteration 1403, loss = 1.10101209\n",
      "Iteration 1404, loss = 1.10090857\n",
      "Iteration 1405, loss = 1.10082964\n",
      "Iteration 1406, loss = 1.10052816\n",
      "Iteration 1407, loss = 1.10035957\n",
      "Iteration 1408, loss = 1.10046074\n",
      "Iteration 1409, loss = 1.10021096\n",
      "Iteration 1410, loss = 1.09999338\n",
      "Iteration 1411, loss = 1.09972186\n",
      "Iteration 1412, loss = 1.09950243\n",
      "Iteration 1413, loss = 1.09932192\n",
      "Iteration 1414, loss = 1.09921003\n",
      "Iteration 1415, loss = 1.09906946\n",
      "Iteration 1416, loss = 1.09899701\n",
      "Iteration 1417, loss = 1.09862029\n",
      "Iteration 1418, loss = 1.09854746\n",
      "Iteration 1419, loss = 1.09832445\n",
      "Iteration 1420, loss = 1.09824759\n",
      "Iteration 1421, loss = 1.09805628\n",
      "Iteration 1422, loss = 1.09814827\n",
      "Iteration 1423, loss = 1.09775080\n",
      "Iteration 1424, loss = 1.09750185\n",
      "Iteration 1425, loss = 1.09713597\n",
      "Iteration 1426, loss = 1.09711853\n",
      "Iteration 1427, loss = 1.09688827\n",
      "Iteration 1428, loss = 1.09671647\n",
      "Iteration 1429, loss = 1.09664721\n",
      "Iteration 1430, loss = 1.09646320\n",
      "Iteration 1431, loss = 1.09601254\n",
      "Iteration 1432, loss = 1.09608009\n",
      "Iteration 1433, loss = 1.09600175\n",
      "Iteration 1434, loss = 1.09592140\n",
      "Iteration 1435, loss = 1.09555893\n",
      "Iteration 1436, loss = 1.09543722\n",
      "Iteration 1437, loss = 1.09555092\n",
      "Iteration 1438, loss = 1.09545003\n",
      "Iteration 1439, loss = 1.09482625\n",
      "Iteration 1440, loss = 1.09494019\n",
      "Iteration 1441, loss = 1.09462739\n",
      "Iteration 1442, loss = 1.09449617\n",
      "Iteration 1443, loss = 1.09416415\n",
      "Iteration 1444, loss = 1.09426355\n",
      "Iteration 1445, loss = 1.09383404\n",
      "Iteration 1446, loss = 1.09373771\n",
      "Iteration 1447, loss = 1.09346270\n",
      "Iteration 1448, loss = 1.09345245\n",
      "Iteration 1449, loss = 1.09320397\n",
      "Iteration 1450, loss = 1.09320257\n",
      "Iteration 1451, loss = 1.09304661\n",
      "Iteration 1452, loss = 1.09280825\n",
      "Iteration 1453, loss = 1.09250301\n",
      "Iteration 1454, loss = 1.09233468\n",
      "Iteration 1455, loss = 1.09233669\n",
      "Iteration 1456, loss = 1.09201722\n",
      "Iteration 1457, loss = 1.09171563\n",
      "Iteration 1458, loss = 1.09169601\n",
      "Iteration 1459, loss = 1.09151837\n",
      "Iteration 1460, loss = 1.09141100\n",
      "Iteration 1461, loss = 1.09118949\n",
      "Iteration 1462, loss = 1.09098497\n",
      "Iteration 1463, loss = 1.09067500\n",
      "Iteration 1464, loss = 1.09071952\n",
      "Iteration 1465, loss = 1.09061545\n",
      "Iteration 1466, loss = 1.09028451\n",
      "Iteration 1467, loss = 1.09036635\n",
      "Iteration 1468, loss = 1.09046183\n",
      "Iteration 1469, loss = 1.08979887\n",
      "Iteration 1470, loss = 1.08983198\n",
      "Iteration 1471, loss = 1.08945539\n",
      "Iteration 1472, loss = 1.08931164\n",
      "Iteration 1473, loss = 1.08909984\n",
      "Iteration 1474, loss = 1.08885576\n",
      "Iteration 1475, loss = 1.08879201\n",
      "Iteration 1476, loss = 1.08883029\n",
      "Iteration 1477, loss = 1.08894396\n",
      "Iteration 1478, loss = 1.08832547\n",
      "Iteration 1479, loss = 1.08820869\n",
      "Iteration 1480, loss = 1.08823371\n",
      "Iteration 1481, loss = 1.08808849\n",
      "Iteration 1482, loss = 1.08771920\n",
      "Iteration 1483, loss = 1.08733933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1484, loss = 1.08725941\n",
      "Iteration 1485, loss = 1.08730904\n",
      "Iteration 1486, loss = 1.08707631\n",
      "Iteration 1487, loss = 1.08710465\n",
      "Iteration 1488, loss = 1.08666927\n",
      "Iteration 1489, loss = 1.08666178\n",
      "Iteration 1490, loss = 1.08630825\n",
      "Iteration 1491, loss = 1.08607547\n",
      "Iteration 1492, loss = 1.08610407\n",
      "Iteration 1493, loss = 1.08593001\n",
      "Iteration 1494, loss = 1.08563334\n",
      "Iteration 1495, loss = 1.08535429\n",
      "Iteration 1496, loss = 1.08551086\n",
      "Iteration 1497, loss = 1.08535478\n",
      "Iteration 1498, loss = 1.08501878\n",
      "Iteration 1499, loss = 1.08487774\n",
      "Iteration 1500, loss = 1.08482039\n",
      "Iteration 1501, loss = 1.08468999\n",
      "Iteration 1502, loss = 1.08452024\n",
      "Iteration 1503, loss = 1.08418929\n",
      "Iteration 1504, loss = 1.08419732\n",
      "Iteration 1505, loss = 1.08411790\n",
      "Iteration 1506, loss = 1.08362721\n",
      "Iteration 1507, loss = 1.08363200\n",
      "Iteration 1508, loss = 1.08345700\n",
      "Iteration 1509, loss = 1.08369713\n",
      "Iteration 1510, loss = 1.08310899\n",
      "Iteration 1511, loss = 1.08297815\n",
      "Iteration 1512, loss = 1.08281077\n",
      "Iteration 1513, loss = 1.08244603\n",
      "Iteration 1514, loss = 1.08249191\n",
      "Iteration 1515, loss = 1.08237275\n",
      "Iteration 1516, loss = 1.08212893\n",
      "Iteration 1517, loss = 1.08189930\n",
      "Iteration 1518, loss = 1.08180069\n",
      "Iteration 1519, loss = 1.08157113\n",
      "Iteration 1520, loss = 1.08137705\n",
      "Iteration 1521, loss = 1.08148526\n",
      "Iteration 1522, loss = 1.08126298\n",
      "Iteration 1523, loss = 1.08129544\n",
      "Iteration 1524, loss = 1.08092076\n",
      "Iteration 1525, loss = 1.08056607\n",
      "Iteration 1526, loss = 1.08054762\n",
      "Iteration 1527, loss = 1.08027084\n",
      "Iteration 1528, loss = 1.08034661\n",
      "Iteration 1529, loss = 1.08017468\n",
      "Iteration 1530, loss = 1.08017352\n",
      "Iteration 1531, loss = 1.07986743\n",
      "Iteration 1532, loss = 1.07957291\n",
      "Iteration 1533, loss = 1.07958258\n",
      "Iteration 1534, loss = 1.07930960\n",
      "Iteration 1535, loss = 1.07896589\n",
      "Iteration 1536, loss = 1.07902750\n",
      "Iteration 1537, loss = 1.07914283\n",
      "Iteration 1538, loss = 1.07859882\n",
      "Iteration 1539, loss = 1.07868486\n",
      "Iteration 1540, loss = 1.07853938\n",
      "Iteration 1541, loss = 1.07794101\n",
      "Iteration 1542, loss = 1.07786767\n",
      "Iteration 1543, loss = 1.07790248\n",
      "Iteration 1544, loss = 1.07789581\n",
      "Iteration 1545, loss = 1.07730088\n",
      "Iteration 1546, loss = 1.07734289\n",
      "Iteration 1547, loss = 1.07720158\n",
      "Iteration 1548, loss = 1.07706260\n",
      "Iteration 1549, loss = 1.07705583\n",
      "Iteration 1550, loss = 1.07676148\n",
      "Iteration 1551, loss = 1.07713608\n",
      "Iteration 1552, loss = 1.07651467\n",
      "Iteration 1553, loss = 1.07630995\n",
      "Iteration 1554, loss = 1.07612077\n",
      "Iteration 1555, loss = 1.07589503\n",
      "Iteration 1556, loss = 1.07613756\n",
      "Iteration 1557, loss = 1.07563056\n",
      "Iteration 1558, loss = 1.07545538\n",
      "Iteration 1559, loss = 1.07535846\n",
      "Iteration 1560, loss = 1.07555098\n",
      "Iteration 1561, loss = 1.07478712\n",
      "Iteration 1562, loss = 1.07472557\n",
      "Iteration 1563, loss = 1.07469840\n",
      "Iteration 1564, loss = 1.07449406\n",
      "Iteration 1565, loss = 1.07443649\n",
      "Iteration 1566, loss = 1.07420402\n",
      "Iteration 1567, loss = 1.07388091\n",
      "Iteration 1568, loss = 1.07401440\n",
      "Iteration 1569, loss = 1.07380887\n",
      "Iteration 1570, loss = 1.07365682\n",
      "Iteration 1571, loss = 1.07359020\n",
      "Iteration 1572, loss = 1.07359168\n",
      "Iteration 1573, loss = 1.07319319\n",
      "Iteration 1574, loss = 1.07317243\n",
      "Iteration 1575, loss = 1.07312571\n",
      "Iteration 1576, loss = 1.07286849\n",
      "Iteration 1577, loss = 1.07234711\n",
      "Iteration 1578, loss = 1.07229146\n",
      "Iteration 1579, loss = 1.07264972\n",
      "Iteration 1580, loss = 1.07198437\n",
      "Iteration 1581, loss = 1.07171811\n",
      "Iteration 1582, loss = 1.07152971\n",
      "Iteration 1583, loss = 1.07150936\n",
      "Iteration 1584, loss = 1.07149128\n",
      "Iteration 1585, loss = 1.07121821\n",
      "Iteration 1586, loss = 1.07108863\n",
      "Iteration 1587, loss = 1.07072575\n",
      "Iteration 1588, loss = 1.07077608\n",
      "Iteration 1589, loss = 1.07042847\n",
      "Iteration 1590, loss = 1.07033090\n",
      "Iteration 1591, loss = 1.07028380\n",
      "Iteration 1592, loss = 1.07026685\n",
      "Iteration 1593, loss = 1.07000789\n",
      "Iteration 1594, loss = 1.06995551\n",
      "Iteration 1595, loss = 1.06950850\n",
      "Iteration 1596, loss = 1.06939975\n",
      "Iteration 1597, loss = 1.06881692\n",
      "Iteration 1598, loss = 1.06891539\n",
      "Iteration 1599, loss = 1.06863888\n",
      "Iteration 1600, loss = 1.06843636\n",
      "Iteration 1601, loss = 1.06825849\n",
      "Iteration 1602, loss = 1.06810172\n",
      "Iteration 1603, loss = 1.06785207\n",
      "Iteration 1604, loss = 1.06803160\n",
      "Iteration 1605, loss = 1.06789071\n",
      "Iteration 1606, loss = 1.06726861\n",
      "Iteration 1607, loss = 1.06716350\n",
      "Iteration 1608, loss = 1.06726328\n",
      "Iteration 1609, loss = 1.06700964\n",
      "Iteration 1610, loss = 1.06671816\n",
      "Iteration 1611, loss = 1.06641507\n",
      "Iteration 1612, loss = 1.06645108\n",
      "Iteration 1613, loss = 1.06653532\n",
      "Iteration 1614, loss = 1.06614922\n",
      "Iteration 1615, loss = 1.06588268\n",
      "Iteration 1616, loss = 1.06608058\n",
      "Iteration 1617, loss = 1.06572181\n",
      "Iteration 1618, loss = 1.06538274\n",
      "Iteration 1619, loss = 1.06536900\n",
      "Iteration 1620, loss = 1.06550749\n",
      "Iteration 1621, loss = 1.06553394\n",
      "Iteration 1622, loss = 1.06486577\n",
      "Iteration 1623, loss = 1.06441569\n",
      "Iteration 1624, loss = 1.06418752\n",
      "Iteration 1625, loss = 1.06467276\n",
      "Iteration 1626, loss = 1.06420564\n",
      "Iteration 1627, loss = 1.06412840\n",
      "Iteration 1628, loss = 1.06380238\n",
      "Iteration 1629, loss = 1.06358409\n",
      "Iteration 1630, loss = 1.06368335\n",
      "Iteration 1631, loss = 1.06360779\n",
      "Iteration 1632, loss = 1.06355714\n",
      "Iteration 1633, loss = 1.06297904\n",
      "Iteration 1634, loss = 1.06292300\n",
      "Iteration 1635, loss = 1.06324010\n",
      "Iteration 1636, loss = 1.06317477\n",
      "Iteration 1637, loss = 1.06240757\n",
      "Iteration 1638, loss = 1.06266935\n",
      "Iteration 1639, loss = 1.06199516\n",
      "Iteration 1640, loss = 1.06190226\n",
      "Iteration 1641, loss = 1.06154413\n",
      "Iteration 1642, loss = 1.06195990\n",
      "Iteration 1643, loss = 1.06157613\n",
      "Iteration 1644, loss = 1.06114575\n",
      "Iteration 1645, loss = 1.06157021\n",
      "Iteration 1646, loss = 1.06130166\n",
      "Iteration 1647, loss = 1.06133232\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 1648, loss = 1.06075406\n",
      "Iteration 1649, loss = 1.06038209\n",
      "Iteration 1650, loss = 1.06037915\n",
      "Iteration 1651, loss = 1.06027320\n",
      "Iteration 1652, loss = 1.06023535\n",
      "Iteration 1653, loss = 1.06031389\n",
      "Iteration 1654, loss = 1.06021851\n",
      "Iteration 1655, loss = 1.06018534\n",
      "Iteration 1656, loss = 1.06012587\n",
      "Iteration 1657, loss = 1.06011340\n",
      "Iteration 1658, loss = 1.06012308\n",
      "Iteration 1659, loss = 1.06018724\n",
      "Iteration 1660, loss = 1.06002630\n",
      "Iteration 1661, loss = 1.05996663\n",
      "Iteration 1662, loss = 1.05994551\n",
      "Iteration 1663, loss = 1.06005975\n",
      "Iteration 1664, loss = 1.05991263\n",
      "Iteration 1665, loss = 1.05982276\n",
      "Iteration 1666, loss = 1.05980385\n",
      "Iteration 1667, loss = 1.05976153\n",
      "Iteration 1668, loss = 1.05977372\n",
      "Iteration 1669, loss = 1.05983803\n",
      "Iteration 1670, loss = 1.05982142\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 1671, loss = 1.05981120\n",
      "Iteration 1672, loss = 1.05961256\n",
      "Iteration 1673, loss = 1.05961128\n",
      "Iteration 1674, loss = 1.05959848\n",
      "Iteration 1675, loss = 1.05966629\n",
      "Iteration 1676, loss = 1.05958751\n",
      "Iteration 1677, loss = 1.05959002\n",
      "Iteration 1678, loss = 1.05958252\n",
      "Iteration 1679, loss = 1.05958521\n",
      "Iteration 1680, loss = 1.05959229\n",
      "Iteration 1681, loss = 1.05963728\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 1682, loss = 1.05953635\n",
      "Iteration 1683, loss = 1.05953153\n",
      "Iteration 1684, loss = 1.05953838\n",
      "Iteration 1685, loss = 1.05953543\n",
      "Iteration 1686, loss = 1.05952646\n",
      "Iteration 1687, loss = 1.05953275\n",
      "Iteration 1688, loss = 1.05954219\n",
      "Iteration 1689, loss = 1.05952621\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 1690, loss = 1.05952286\n",
      "Iteration 1691, loss = 1.05951947\n",
      "Iteration 1692, loss = 1.05951961\n",
      "Iteration 1693, loss = 1.05951919\n",
      "Iteration 1694, loss = 1.05951898\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.03, batch_size=1000, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10, 10), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=1e-06, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf.fit(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5796"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(cv_examples) == cv_labels)/len(cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60268"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cnf.predict(training_examples) == training_labels)/len(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cnf.predict(test_examples) == test_labels)/len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
